{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-BasicSR-lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "izxcg28hfc58",
        "PvjF6Mju2tVW",
        "U8H0Y5LF1G8d",
        "R2suCcAi1F2r",
        "gmHVQJ44L6Ch",
        "xqpFnEbQyzCR",
        "fv2IFzTH9W8V",
        "ur82Z5hO3GgZ",
        "V0BPZ2OU03Ae",
        "WzBj6jlz5_Ze",
        "YEj2gEWfvyqM",
        "msE-RbkL0LQ9",
        "ahBsPQZVVRfF",
        "NZc7QSEN2uoR",
        "xREJTZSbsEa8",
        "eodC8LcPOLFe"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsxYKkWLXju5"
      },
      "source": [
        "# Colab-BasicSR (pytorch lightning)\n",
        "\n",
        "[This tutorial](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09), [this issue](https://stackoverflow.com/questions/65387967/misconfigurationerror-no-tpu-devices-were-found-even-when-tpu-is-connected-in)  and [this Colab](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/03-basic-gan.ipynb#scrollTo=3vKszYf6y1Vv) were very helpful. This Colab does support single-GPU, multi-GPU and TPU training.\n",
        "\n",
        "Can use various loss functions and has the context_encoder discriminator as default. Currently there are only various inpainting generators from [my BasicSR fork](https://github.com/styler00dollar/Colab-BasicSR).\n",
        "\n",
        "What is not included inside this Colab, but is included in [my normal BasicSR Colab](https://colab.research.google.com/github/styler00dollar/Colab-BasicSR/blob/master/Colab-BasicSR.ipynb):\n",
        "- [edge-informed-sisr](https://github.com/knazeri/edge-informed-sisr/blob/master/src/models.py)\n",
        "- [USRNet](https://github.com/cszn/KAIR/blob/master/models/network_usrnet.py)\n",
        "- [OFT Dataloader](https://github.com/styler00dollar/Colab-BasicSR/tree/master/codes/data)\n",
        "- Some loss functions, but most are here\n",
        "- DiffAug / Mixup\n",
        "\n",
        "What currently is here but not inside the other Colab:\n",
        "- Custom mask loading\n",
        "- New discriminators (EfficientNet, ResNeSt, Transformer)\n",
        "- [AdamP](https://github.com/clovaai/AdamP)\n",
        "\n",
        "Sidenotes:\n",
        "- Does validation on set validation frequency and epoch end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPB6KmEOPQW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBmBFu3vTlTX",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "# create empty folders\n",
        "!mkdir /content/masks\n",
        "!mkdir /content/validation\n",
        "!mkdir /content/data\n",
        "!mkdir /content/logs/\n",
        "\n",
        "#!pip install pytorch-lightning -U\n",
        "# Hotfix, to avoid pytorch-lightning bug\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWDUI7DqflrY",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "# create empty folders\n",
        "!mkdir /content/masks\n",
        "!mkdir /content/validation\n",
        "!mkdir /content/data\n",
        "!mkdir /content/logs/\n",
        "\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null\n",
        "\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0I03TFfDaqtR"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzLIdqOkNSsT"
      },
      "source": [
        "Paths:\n",
        "```\n",
        "/content/data (rgb data)\n",
        "/content/masks (1 channel masks, black = mask, white = original image)\n",
        "/content/validation (images for validation)\n",
        "/content/validation_output (validation destination, will be created if not present)\n",
        "/content/test (rgb data)\n",
        "/content/test_output (test output, will be created if not present)\n",
        "```\n",
        "By default, random masks will have 50% chance and custom masks will have 50% chance. Current validation does not rely on metrics and will take a green masked LR image as input, but metrics are added and only need a custom dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fSZHn8ayFM",
        "cellView": "form"
      },
      "source": [
        "#@title copy data somehow\n",
        "!mkdir '/content/data'\n",
        "!mkdir '/content/data/images'\n",
        "!cp \"/content/drive/MyDrive/classification_v3.7z\" \"/content/data/images/data.7z\"\n",
        "%cd /content/data/images\n",
        "!7z x \"data.7z\"\n",
        "!rm -rf /content/data/images/data.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izxcg28hfc58"
      },
      "source": [
        "# Optional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhGrqwNy0pan"
      },
      "source": [
        "# EfficientNet\n",
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ihFZWQHfd-1"
      },
      "source": [
        "# AdamP\n",
        "!pip install adamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "UZyQogo2SUyY"
      },
      "source": [
        "#@title transformer.py\n",
        "\"\"\"\n",
        "ViT_8_8.py (17-2-20)\n",
        "https://github.com/VITA-Group/TransGAN/blob/97d4b5b29d237ff4bf1337e2a2cf402a6c8a314c/models/ViT_8_8.py\n",
        "\n",
        "ViT_helper.py (17-2-20)\n",
        "https://github.com/VITA-Group/TransGAN/blob/97d4b5b29d237ff4bf1337e2a2cf402a6c8a314c/models/ViT_helper.py\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import math\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(pl.LightningModule):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "from itertools import repeat\n",
        "from torch._six import container_abcs\n",
        "\n",
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, container_abcs.Iterable):\n",
        "            return x\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "# @Date    : 2019-08-15\n",
        "# @Author  : Xinyu Gong (xy_gong@tamu.edu)\n",
        "# @Link    : None\n",
        "# @Version : 0.0\n",
        "\n",
        "class matmul(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "    \n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class Mlp(pl.LightningModule):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(pl.LightningModule):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(pl.LightningModule):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HybridEmbed(pl.LightningModule):\n",
        "    \"\"\" CNN Feature Map Embedding\n",
        "    Extract feature map from CNN, flatten, project to embedding dim.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        assert isinstance(backbone, pl.LightningModule)\n",
        "        img_size = to_2tuple(img_size)\n",
        "        self.img_size = img_size\n",
        "        self.backbone = backbone\n",
        "        if feature_size is None:\n",
        "            with torch.no_grad():\n",
        "                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
        "                # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
        "                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
        "                training = backbone.training\n",
        "                if training:\n",
        "                    backbone.eval()\n",
        "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
        "                feature_size = o.shape[-2:]\n",
        "                feature_dim = o.shape[1]\n",
        "                backbone.train(training)\n",
        "        else:\n",
        "            feature_size = to_2tuple(feature_size)\n",
        "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
        "        self.num_patches = feature_size[0] * feature_size[1]\n",
        "        self.proj = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)[-1]\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TranformerDiscriminator(pl.LightningModule):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=1, in_chans=3, num_classes=1, embed_dim=64, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = 64  # num_features for consistency with other models\n",
        "        self.depth = depth\n",
        "        self.patch_size = patch_size\n",
        "        self.img_size = img_size\n",
        "\n",
        "        if hybrid_backbone is not None:\n",
        "            self.patch_embed = HybridEmbed(\n",
        "                hybrid_backbone, img_size=self.img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        else:\n",
        "            self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        num_patches = (self.img_size // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(self.depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
        "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
        "        #self.repr_act = nn.Tanh()\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "        print(\"Transformer init complete\")\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        #if \"None\" not in self.args.diff_aug:\n",
        "        #    x = DiffAugment(x, self.args.diff_aug, True)\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x).flatten(2).permute(0,2,1)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=16):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "    return out_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAE57Rv75Ta2",
        "cellView": "form"
      },
      "source": [
        "#@title ResNeSt.py\n",
        "#https://github.com/zhanghang1989/ResNeSt/blob/11eb547225c6b98bdf6cab774fb58dffc53362b1/resnest/torch/splat.py\n",
        "\"\"\"Split-Attention\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "__all__ = ['SplAtConv2d']\n",
        "\n",
        "class SplAtConv2d(Module):\n",
        "    \"\"\"Split-Attention Conv2d\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n",
        "                 dilation=(1, 1), groups=1, bias=True,\n",
        "                 radix=2, reduction_factor=4,\n",
        "                 rectify=False, rectify_avg=False, norm_layer=None,\n",
        "                 dropblock_prob=0.0, **kwargs):\n",
        "        super(SplAtConv2d, self).__init__()\n",
        "        padding = _pair(padding)\n",
        "        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n",
        "        self.rectify_avg = rectify_avg\n",
        "        inter_channels = max(in_channels*radix//reduction_factor, 32)\n",
        "        self.radix = radix\n",
        "        self.cardinality = groups\n",
        "        self.channels = channels\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        if self.rectify:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
        "                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n",
        "        else:\n",
        "            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
        "                               groups=groups*radix, bias=bias, **kwargs)\n",
        "        self.use_bn = norm_layer is not None\n",
        "        if self.use_bn:\n",
        "            self.bn0 = norm_layer(channels*radix)\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n",
        "        if self.use_bn:\n",
        "            self.bn1 = norm_layer(inter_channels)\n",
        "        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock = DropBlock2D(dropblock_prob, 3)\n",
        "        self.rsoftmax = rSoftMax(radix, groups)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn0(x)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            x = self.dropblock(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        batch, rchannel = x.shape[:2]\n",
        "        if self.radix > 1:\n",
        "            if torch.__version__ < '1.5':\n",
        "                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n",
        "            else:\n",
        "                splited = torch.split(x, rchannel//self.radix, dim=1)\n",
        "            gap = sum(splited) \n",
        "        else:\n",
        "            gap = x\n",
        "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
        "        gap = self.fc1(gap)\n",
        "\n",
        "        if self.use_bn:\n",
        "            gap = self.bn1(gap)\n",
        "        gap = self.relu(gap)\n",
        "\n",
        "        atten = self.fc2(gap)\n",
        "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
        "\n",
        "        if self.radix > 1:\n",
        "            if torch.__version__ < '1.5':\n",
        "                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n",
        "            else:\n",
        "                attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
        "            out = sum([att*split for (att, split) in zip(attens, splited)])\n",
        "        else:\n",
        "            out = atten * x\n",
        "        return out.contiguous()\n",
        "\n",
        "class rSoftMax(nn.Module):\n",
        "    def __init__(self, radix, cardinality):\n",
        "        super().__init__()\n",
        "        self.radix = radix\n",
        "        self.cardinality = cardinality\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.size(0)\n",
        "        if self.radix > 1:\n",
        "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            x = x.reshape(batch, -1)\n",
        "        else:\n",
        "            x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/zhanghang1989/ResNeSt/blob/11eb547225c6b98bdf6cab774fb58dffc53362b1/resnest/torch/resnet.py\n",
        "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "## Created by: Hang Zhang\n",
        "## Email: zhanghang0704@gmail.com\n",
        "## Copyright (c) 2020\n",
        "##\n",
        "## LICENSE file in the root directory of this source tree \n",
        "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\"\"\"ResNet variants\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from .splat import SplAtConv2d\n",
        "\n",
        "__all__ = ['ResNet', 'Bottleneck']\n",
        "\n",
        "class DropBlock2D(object):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class GlobalAvgPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
        "        super(GlobalAvgPool2d, self).__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"ResNet Bottleneck\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-argument\n",
        "    expansion = 4\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 radix=1, cardinality=1, bottleneck_width=64,\n",
        "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
        "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(group_width)\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        self.radix = radix\n",
        "        self.avd = avd and (stride > 1 or is_first)\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        if self.avd:\n",
        "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
        "            if radix == 1:\n",
        "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
        "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
        "\n",
        "        if radix >= 1:\n",
        "            self.conv2 = SplAtConv2d(\n",
        "                group_width, group_width, kernel_size=3,\n",
        "                stride=stride, padding=dilation,\n",
        "                dilation=dilation, groups=cardinality, bias=False,\n",
        "                radix=radix, rectify=rectified_conv,\n",
        "                rectify_avg=rectify_avg,\n",
        "                norm_layer=norm_layer,\n",
        "                dropblock_prob=dropblock_prob)\n",
        "        elif rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv2 = RFConv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False,\n",
        "                average_mode=rectify_avg)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "        else:\n",
        "            self.conv2 = nn.Conv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            group_width, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes*4)\n",
        "\n",
        "        if last_gamma:\n",
        "            from torch.nn.init import zeros_\n",
        "            zeros_(self.bn3.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.dilation = dilation\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.avd and self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        if self.radix == 0:\n",
        "            out = self.bn2(out)\n",
        "            if self.dropblock_prob > 0.0:\n",
        "                out = self.dropblock2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "        if self.avd and not self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet Variants\n",
        "    Parameters\n",
        "    ----------\n",
        "    block : Block\n",
        "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
        "    layers : list of int\n",
        "        Numbers of layers in each block\n",
        "    classes : int, default 1000\n",
        "        Number of classification classes.\n",
        "    dilated : bool, default False\n",
        "        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n",
        "        typically used in Semantic Segmentation.\n",
        "    norm_layer : object\n",
        "        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n",
        "        for Synchronized Cross-GPU BachNormalization).\n",
        "    Reference:\n",
        "        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
        "        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-variable\n",
        "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n",
        "                 num_classes=1000, dilated=False, dilation=1,\n",
        "                 deep_stem=False, stem_width=64, avg_down=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 avd=False, avd_first=False,\n",
        "                 final_drop=0.0, dropblock_prob=0,\n",
        "                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n",
        "        self.cardinality = groups\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        # ResNet-D params\n",
        "        self.inplanes = stem_width*2 if deep_stem else 64\n",
        "        self.avg_down = avg_down\n",
        "        self.last_gamma = last_gamma\n",
        "        # ResNeSt params\n",
        "        self.radix = radix\n",
        "        self.avd = avd\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "        self.rectified_conv = rectified_conv\n",
        "        self.rectify_avg = rectify_avg\n",
        "        if rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            conv_layer = RFConv2d\n",
        "        else:\n",
        "            conv_layer = nn.Conv2d\n",
        "        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n",
        "        if deep_stem:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                                   bias=False, **conv_kwargs)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
        "        if dilated or dilation == 4:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=4, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        elif dilation==2:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           dilation=1, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        else:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        self.avgpool = GlobalAvgPool2d()\n",
        "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, norm_layer):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n",
        "                    dropblock_prob=0.0, is_first=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            down_layers = []\n",
        "            if self.avg_down:\n",
        "                if dilation == 1:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                else:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=1, bias=False))\n",
        "            else:\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=stride, bias=False))\n",
        "            down_layers.append(norm_layer(planes * block.expansion))\n",
        "            downsample = nn.Sequential(*down_layers)\n",
        "\n",
        "        layers = []\n",
        "        if dilation == 1 or dilation == 2:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        elif dilation == 4:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        else:\n",
        "            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=dilation, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.drop:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "#https://github.com/zhanghang1989/ResNeSt/blob/master/resnest/torch/resnest.py\n",
        "import torch\n",
        "#from .resnet import ResNet, Bottleneck\n",
        "\n",
        "__all__ = ['resnest50', 'resnest101', 'resnest200', 'resnest269']\n",
        "\n",
        "_url_format = 'https://s3.us-west-1.wasabisys.com/resnest/torch/{}-{}.pth'\n",
        "\n",
        "_model_sha256 = {name: checksum for checksum, name in [\n",
        "    ('528c19ca', 'resnest50'),\n",
        "    ('22405ba7', 'resnest101'),\n",
        "    ('75117900', 'resnest200'),\n",
        "    ('0cc87c48', 'resnest269'),\n",
        "    ]}\n",
        "\n",
        "def short_hash(name):\n",
        "    if name not in _model_sha256:\n",
        "        raise ValueError('Pretrained model for {name} is not available.'.format(name=name))\n",
        "    return _model_sha256[name][:8]\n",
        "\n",
        "resnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n",
        "    name in _model_sha256.keys()\n",
        "}\n",
        "\n",
        "def resnest50(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest50'], progress=True, check_hash=True))\n",
        "    return model\n",
        "\n",
        "def resnest101(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=64, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest101'], progress=True, check_hash=True))\n",
        "    return model\n",
        "\n",
        "def resnest200(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 24, 36, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=64, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest200'], progress=True, check_hash=True))\n",
        "    return model\n",
        "\n",
        "def resnest269(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 30, 48, 8],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=64, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest269'], progress=True, check_hash=True))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvjF6Mju2tVW"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQYzPLBdbD-w",
        "cellView": "form"
      },
      "source": [
        "#@title getting pytorch-loss-functions\n",
        "%cd /content\n",
        "!git clone https://github.com/styler00dollar/pytorch-loss-functions pytorchloss\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-vTO-jUfMhT"
      },
      "source": [
        "# restart from here if you reset your notebook\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8H0Y5LF1G8d"
      },
      "source": [
        "# Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HozYJLs0B67",
        "cellView": "form"
      },
      "source": [
        "#@title partialconv.py\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "#\n",
        "# Source: https://github.com/NVIDIA/partialconv/blob/master/models/partialconv2d.py\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, cuda\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False  \n",
        "\n",
        "        if 'return_mask' in kwargs:\n",
        "            self.return_mask = kwargs['return_mask']\n",
        "            kwargs.pop('return_mask')\n",
        "        else:\n",
        "            self.return_mask = False\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "            \n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None, None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask_in=None):\n",
        "        assert len(input.shape) == 4\n",
        "        if mask_in is not None or self.last_size != tuple(input.shape):\n",
        "            self.last_size = tuple(input.shape)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask_in is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                else:\n",
        "                    mask = mask_in\n",
        "                        \n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                #make sure the value of self.mask_ratio for the entries in the interior (no need for padding) have value 1. If not, you replace with the line below.\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        # if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "        #     self.update_mask.to(input)\n",
        "        #     self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-5xc1MR0F-K",
        "cellView": "form"
      },
      "source": [
        "#@title deformconv2d.py\n",
        "import torch.nn as nn\n",
        "import torchvision.ops as O\n",
        "\n",
        "\n",
        "class DeformConv2d(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
        "        super(DeformConv2d, self).__init__()\n",
        "\n",
        "        self.conv_offset = nn.Conv2d(in_nc, 2 * (kernel_size**2), kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.conv_offset.weight.data.zero_()\n",
        "        self.conv_offset.bias.data.zero_()\n",
        "\n",
        "        self.dcn_conv = O.DeformConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset = self.conv_offset(x)\n",
        "        return self.dcn_conv(x, offset=offset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2suCcAi1F2r"
      },
      "source": [
        "# Data (for inpainting)\n",
        "\n",
        "The Dataloader can be customized in several different ways, here are some examples. The most basic one is this one, which does load an image and maybe outputs an image mask or an autogenerated mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ccjLoMPGoT",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, root, transform=None, size=256):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mask_dir = '/content/masks'\n",
        "        self.files = glob.glob(self.mask_dir + '/**/*.png', recursive=True)\n",
        "        files_jpg = glob.glob(self.mask_dir + '/**/*.jpg', recursive=True)\n",
        "        self.files.extend(files_jpg)\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        sample = Image.open(sample_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(np.array(sample), cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        if random.uniform(0, 1) < 0.5:\n",
        "          # generating mask automatically with 50% chance\n",
        "          mask = DS.random_mask(height=self.size, width=self.size)\n",
        "          mask = torch.from_numpy(mask)\n",
        "\n",
        "        else:\n",
        "          # load random mask from folder\n",
        "          mask = cv2.imread(random.choice([x for x in self.files]), cv2.IMREAD_UNCHANGED)\n",
        "          mask = cv2.resize(mask, (self.size,self.size), interpolation=cv2.INTER_NEAREST)\n",
        "          \n",
        "          # flip mask randomly\n",
        "          if 0.3 < random.uniform(0, 1) <= 0.66:\n",
        "            mask = np.flip(mask, axis=0)\n",
        "          elif 0.66 < random.uniform(0, 1) <= 1:\n",
        "            mask = np.flip(mask, axis=1)\n",
        "\n",
        "          mask = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)/255\n",
        "\n",
        "        #sample = torch.from_numpy(sample)\n",
        "        sample = transforms.ToTensor()(sample)\n",
        "\n",
        "        # apply mask\n",
        "        masked = sample * mask\n",
        "        return masked, mask, sample\n",
        "\n",
        "        # EdgeConnect\n",
        "        #return masked, mask, sample, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return masked, mask, sample, edges\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_mask(height=256, width=256,\n",
        "                    min_stroke=1, max_stroke=4,\n",
        "                    min_vertex=1, max_vertex=12,\n",
        "                    min_brush_width_divisor=16, max_brush_width_divisor=10):\n",
        "        mask = np.ones((height, width))\n",
        "\n",
        "        min_brush_width = height // min_brush_width_divisor\n",
        "        max_brush_width = height // max_brush_width_divisor\n",
        "        max_angle = 2*np.pi\n",
        "        num_stroke = np.random.randint(min_stroke, max_stroke+1)\n",
        "        average_length = np.sqrt(height*height + width*width) / 8\n",
        "\n",
        "        for _ in range(num_stroke):\n",
        "            num_vertex = np.random.randint(min_vertex, max_vertex+1)\n",
        "            start_x = np.random.randint(width)\n",
        "            start_y = np.random.randint(height)\n",
        "\n",
        "            for _ in range(num_vertex):\n",
        "                angle = np.random.uniform(max_angle)\n",
        "                length = np.clip(np.random.normal(average_length, average_length//2), 0, 2*average_length)\n",
        "                brush_width = np.random.randint(min_brush_width, max_brush_width+1)\n",
        "                end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
        "                end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
        "\n",
        "                cv2.line(mask, (start_y, start_x), (end_y, end_x), 0., brush_width)\n",
        "\n",
        "                start_x, start_y = end_x, end_y\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.fliplr(mask)\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.flipud(mask)\n",
        "        return mask.reshape((1,)+mask.shape).astype(np.float32) \n",
        "\n",
        "\n",
        "\n",
        "class DS_green_from_mask(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        #sample = Image.open(sample_path).convert('RGB')\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(sample, cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        green_mask = 1-np.all(sample == [0,255,0], axis=-1).astype(int)\n",
        "        green_mask = torch.from_numpy(green_mask).unsqueeze(0)\n",
        "        sample = torch.from_numpy(sample.astype(np.float32)).permute(2, 0, 1)/255\n",
        "        sample = sample * green_mask\n",
        "\n",
        "        # train_batch[0] = masked\n",
        "        # train_batch[1] = mask\n",
        "        # train_batch[2] = path\n",
        "        return sample, green_mask, sample_path \n",
        "\n",
        "        # EdgeConnect\n",
        "        #return sample, green_mask, sample_path, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return sample, green_mask, sample_path, edges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWnqGVcPcvm",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', validation_path: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 256\n",
        "    def setup(self, stage=None):\n",
        "        img_tf = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "            #transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFNetdataset_train = DS(self.training_dir, img_tf, self.size)\n",
        "        self.DFNetdataset_validation = DS_green_from_mask(self.validation_dir, img_tf)\n",
        "        self.DFNetdataset_test = DS_green_from_mask(self.test_dir)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmHVQJ44L6Ch"
      },
      "source": [
        "# Data (16x16 256px (for inpainting))\n",
        "\n",
        "Uses 16x16 image grids to avoid problems with Colab. Colab tends to freeze if you upload too many files. Training will be slower, but will result in fewer files and avoids crashing Colab. Only recommended for usage with a lot of files, like 5-6 digit amount of files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMAg-B1rL4wj",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, root, transform=None, size=256):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mask_dir = '/content/masks'\n",
        "        self.files = glob.glob(self.mask_dir + '/**/*.png', recursive=True)\n",
        "        files_jpg = glob.glob(self.mask_dir + '/**/*.jpg', recursive=True)\n",
        "        self.files.extend(files_jpg)\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        #sample = Image.open(sample_path).convert('RGB')\n",
        "        sample = cv2.imread(sample_path)\n",
        "\n",
        "\n",
        "        x_rand = random.randint(0,15)\n",
        "        y_rand = random.randint(0,15)\n",
        "\n",
        "        sample = sample[x_rand*256:(x_rand+1)*256, y_rand*256:(y_rand+1)*256]\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        #sample = torch.from_numpy(sample)\n",
        "\n",
        "        #if self.transform:\n",
        "        #    sample = self.transform(sample)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(np.array(sample), cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        if random.uniform(0, 1) < 0.5:\n",
        "          # generating mask automatically with 50% chance\n",
        "          mask = DS.random_mask(height=self.size, width=self.size)\n",
        "          mask = torch.from_numpy(mask)\n",
        "\n",
        "        else:\n",
        "          # load random mask from folder\n",
        "          mask = cv2.imread(random.choice([x for x in self.files]), cv2.IMREAD_UNCHANGED)\n",
        "          mask = cv2.resize(mask, (self.size,self.size), interpolation=cv2.INTER_NEAREST)\n",
        "          \n",
        "          # flip mask randomly\n",
        "          if 0.3 < random.uniform(0, 1) <= 0.66:\n",
        "            mask = np.flip(mask, axis=0)\n",
        "          elif 0.66 < random.uniform(0, 1) <= 1:\n",
        "            mask = np.flip(mask, axis=1)\n",
        "\n",
        "          mask = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)/255\n",
        "\n",
        "        sample = torch.from_numpy(sample).permute(2, 0, 1)/255\n",
        "        #sample = transforms.ToTensor()(sample)\n",
        "\n",
        "        # apply mask\n",
        "        #print(sample.shape)\n",
        "        #print(mask.shape)\n",
        "        masked = sample * mask\n",
        "        return masked, mask, sample\n",
        "\n",
        "        # EdgeConnect\n",
        "        #return masked, mask, sample, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return masked, mask, sample, edges\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_mask(height=256, width=256,\n",
        "                    min_stroke=1, max_stroke=4,\n",
        "                    min_vertex=1, max_vertex=12,\n",
        "                    min_brush_width_divisor=16, max_brush_width_divisor=10):\n",
        "        mask = np.ones((height, width))\n",
        "\n",
        "        min_brush_width = height // min_brush_width_divisor\n",
        "        max_brush_width = height // max_brush_width_divisor\n",
        "        max_angle = 2*np.pi\n",
        "        num_stroke = np.random.randint(min_stroke, max_stroke+1)\n",
        "        average_length = np.sqrt(height*height + width*width) / 8\n",
        "\n",
        "        for _ in range(num_stroke):\n",
        "            num_vertex = np.random.randint(min_vertex, max_vertex+1)\n",
        "            start_x = np.random.randint(width)\n",
        "            start_y = np.random.randint(height)\n",
        "\n",
        "            for _ in range(num_vertex):\n",
        "                angle = np.random.uniform(max_angle)\n",
        "                length = np.clip(np.random.normal(average_length, average_length//2), 0, 2*average_length)\n",
        "                brush_width = np.random.randint(min_brush_width, max_brush_width+1)\n",
        "                end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
        "                end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
        "\n",
        "                cv2.line(mask, (start_y, start_x), (end_y, end_x), 0., brush_width)\n",
        "\n",
        "                start_x, start_y = end_x, end_y\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.fliplr(mask)\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.flipud(mask)\n",
        "        return mask.reshape((1,)+mask.shape).astype(np.float32) \n",
        "\n",
        "\n",
        "\n",
        "class DS_green_from_mask(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        #sample = Image.open(sample_path).convert('RGB')\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(sample, cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        green_mask = 1-np.all(sample == [0,255,0], axis=-1).astype(int)\n",
        "        green_mask = torch.from_numpy(green_mask).unsqueeze(0)\n",
        "        sample = torch.from_numpy(sample.astype(np.float32)).permute(2, 0, 1)/255\n",
        "        sample = sample * green_mask\n",
        "\n",
        "        # train_batch[0] = masked\n",
        "        # train_batch[1] = mask\n",
        "        # train_batch[2] = path\n",
        "        return sample, green_mask, sample_path \n",
        "\n",
        "        # EdgeConnect\n",
        "        #return sample, green_mask, sample_path, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return sample, green_mask, sample_path, edges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHG-dzJEMRgD",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', validation_path: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 256\n",
        "    def setup(self, stage=None):\n",
        "        img_tf = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "            #transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFNetdataset_train = DS(self.training_dir, img_tf, self.size)\n",
        "        self.DFNetdataset_validation = DS_green_from_mask(self.validation_dir, img_tf)\n",
        "        self.DFNetdataset_test = DS_green_from_mask(self.test_dir)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqpFnEbQyzCR"
      },
      "source": [
        "# Data (16x16 256px, batch (for inpainting))\n",
        "\n",
        "Uses 16x16 image grids to avoid problems with Colab. Colab tends to freeze if you upload too many files. Optionally, also assumes that the masks are 4x4 (untested). Only recommended for usage with a lot of files, like 5-6 digit amount of files. Returns a batch within dataloader and not with ``getitem`` and not only one image to improve speed. Assumes you use ``batch_size 1`` and the usage of ``batch_size_DL`` in ``Trainer()``. The higher the batch_size, the higher the training speed will benefit from this.\n",
        "\n",
        "A few benchmarks, with 4k images and non-grid masks.\n",
        "```\n",
        "Tesla T4: \n",
        "RFR (batch_size=5):\n",
        "256px:       1-1.08 s/it\n",
        "4k (normal): 1-1.1 s/it\n",
        "4k (batch):  1-1.1 s/it\n",
        "\n",
        "DFNet (batch_size=20):\n",
        "256px:       2,2-2,6 it/s\n",
        "4k (normal): 2.8-3.4 s/it\n",
        "4k (batch):  2.3-2.6 it/s\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUjNz2--zXBF",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, root, transform=None, size=256, batch_size_DL = 3):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mask_dir = '/content/masks'\n",
        "        self.files = glob.glob(self.mask_dir + '/**/*.png', recursive=True)\n",
        "        files_jpg = glob.glob(self.mask_dir + '/**/*.jpg', recursive=True)\n",
        "        self.files.extend(files_jpg)\n",
        "\n",
        "        self.size = size\n",
        "        self.batch_size = batch_size_DL\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        sample = cv2.imread(sample_path)\n",
        "\n",
        "        #batch_size = 10\n",
        "        pos_total = []\n",
        "        self.total_size = 0\n",
        "\n",
        "        while True:\n",
        "          # determine random position\n",
        "          x_rand = random.randint(0,15)\n",
        "          y_rand = random.randint(0,15)\n",
        "          \n",
        "          pos_rand = [x_rand, y_rand]\n",
        "\n",
        "          if (pos_rand in pos_total) != True:\n",
        "            pos_total.append(pos_rand)\n",
        "            self.total_size += 1\n",
        "\n",
        "          # return batchsize\n",
        "          if self.total_size == self.batch_size:\n",
        "            break\n",
        "\n",
        "        self.total_size = 0\n",
        "        for i in pos_total:\n",
        "          # creating sample if for start\n",
        "          \"\"\"\n",
        "          print(\"pos_total\")\n",
        "          print(pos_total)\n",
        "\n",
        "          print(\"i\")\n",
        "          print(i)\n",
        "          \"\"\"\n",
        "          if self.total_size == 0:\n",
        "            sample_add = sample[i[0]*256:(i[0]+1)*256, i[1]*256:(i[1]+1)*256]\n",
        "            sample_add = cv2.cvtColor(sample_add, cv2.COLOR_BGR2RGB)\n",
        "            sample_add = torch.from_numpy(sample_add).permute(2, 0, 1).unsqueeze(0)/255\n",
        "\n",
        "            # if edges are required\n",
        "            \"\"\"\n",
        "            grayscale = cv2.cvtColor(np.array(sample_add), cv2.COLOR_RGB2GRAY)\n",
        "            edges = cv2.Canny(grayscale,100,150)\n",
        "            grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "            edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "            \"\"\"\n",
        "\n",
        "            self.total_size += 1\n",
        "          else:\n",
        "            sample_add2 = sample[i[0]*256:(i[0]+1)*256, i[1]*256:(i[1]+1)*256]\n",
        "            sample_add2 = cv2.cvtColor(sample_add2, cv2.COLOR_BGR2RGB)\n",
        "            # if edges are required\n",
        "            \"\"\"\n",
        "            grayscale = cv2.cvtColor(np.array(sample_add2), cv2.COLOR_RGB2GRAY)\n",
        "            edges = cv2.Canny(grayscale,100,150)\n",
        "            grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "            edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "            \"\"\"\n",
        "            sample_add2 = torch.from_numpy(sample_add2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "            sample_add = torch.cat((sample_add, sample_add2), dim=0)\n",
        "\n",
        "        # getting mask batch\n",
        "        \n",
        "        self.total_size = 0\n",
        "        for i in range(self.batch_size):\n",
        "          # randommly loading one mask\n",
        "\n",
        "          if random.uniform(0, 1) < 0.5:\n",
        "            # generating mask automatically with 50% chance\n",
        "            mask = DS.random_mask(height=self.size, width=self.size)\n",
        "            mask = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)\n",
        "            #print(\"random mask\")\n",
        "            #print(mask.shape)\n",
        "\n",
        "          else:\n",
        "            # load random mask from folder\n",
        "            mask = cv2.imread(random.choice([x for x in self.files]), cv2.IMREAD_UNCHANGED)\n",
        "            mask = cv2.resize(mask, (self.size,self.size), interpolation=cv2.INTER_NEAREST)\n",
        "            \n",
        "            # flip mask randomly\n",
        "            if 0.3 < random.uniform(0, 1) <= 0.66:\n",
        "              mask = np.flip(mask, axis=0)\n",
        "            elif 0.66 < random.uniform(0, 1) <= 1:\n",
        "              mask = np.flip(mask, axis=1)\n",
        "            mask = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
        "            #print(\"read mask\")\n",
        "            #print(mask.shape)\n",
        "\n",
        "          if self.total_size == 0:\n",
        "            mask_add = mask/255\n",
        "            self.total_size += 1\n",
        "          else:\n",
        "            mask_add2 = mask/255\n",
        "            mask_add = torch.cat((mask_add, mask_add2), dim=0)\n",
        "            self.total_size += 1\n",
        "\n",
        "        # apply mask\n",
        "        masked = sample_add * mask_add\n",
        "\n",
        "        return masked, mask_add, sample_add\n",
        "\n",
        "        # EdgeConnect\n",
        "        #return masked, mask, sample, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return masked, mask, sample, edges\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_mask(height=256, width=256,\n",
        "                    min_stroke=1, max_stroke=4,\n",
        "                    min_vertex=1, max_vertex=12,\n",
        "                    min_brush_width_divisor=16, max_brush_width_divisor=10):\n",
        "        mask = np.ones((height, width))\n",
        "\n",
        "        min_brush_width = height // min_brush_width_divisor\n",
        "        max_brush_width = height // max_brush_width_divisor\n",
        "        max_angle = 2*np.pi\n",
        "        num_stroke = np.random.randint(min_stroke, max_stroke+1)\n",
        "        average_length = np.sqrt(height*height + width*width) / 8\n",
        "\n",
        "        for _ in range(num_stroke):\n",
        "            num_vertex = np.random.randint(min_vertex, max_vertex+1)\n",
        "            start_x = np.random.randint(width)\n",
        "            start_y = np.random.randint(height)\n",
        "\n",
        "            for _ in range(num_vertex):\n",
        "                angle = np.random.uniform(max_angle)\n",
        "                length = np.clip(np.random.normal(average_length, average_length//2), 0, 2*average_length)\n",
        "                brush_width = np.random.randint(min_brush_width, max_brush_width+1)\n",
        "                end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
        "                end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
        "\n",
        "                cv2.line(mask, (start_y, start_x), (end_y, end_x), 0., brush_width)\n",
        "\n",
        "                start_x, start_y = end_x, end_y\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.fliplr(mask)\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.flipud(mask)\n",
        "        return mask.reshape((1,)+mask.shape).astype(np.float32) \n",
        "\n",
        "\n",
        "\n",
        "class DS_green_from_mask(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        #sample = Image.open(sample_path).convert('RGB')\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(sample, cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        green_mask = 1-np.all(sample == [0,255,0], axis=-1).astype(int)\n",
        "        green_mask = torch.from_numpy(green_mask).unsqueeze(0)\n",
        "        sample = torch.from_numpy(sample.astype(np.float32)).permute(2, 0, 1)/255\n",
        "        sample = sample * green_mask\n",
        "\n",
        "        # train_batch[0] = masked\n",
        "        # train_batch[1] = mask\n",
        "        # train_batch[2] = path\n",
        "        return sample, green_mask, sample_path \n",
        "\n",
        "        # EdgeConnect\n",
        "        #return sample, green_mask, sample_path, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return sample, green_mask, sample_path, edges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cjyC95SzZHu",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', validation_path: str = './', test_path: str = './', batch_size: int = 5, batch_size_DL: int = 2, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.batch_size_DL = batch_size_DL\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 256\n",
        "    def setup(self, stage=None):\n",
        "        img_tf = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "            #transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFNetdataset_train = DS(self.training_dir, img_tf, self.size, batch_size_DL = self.batch_size_DL)\n",
        "        self.DFNetdataset_validation = DS_green_from_mask(self.validation_dir, img_tf)\n",
        "        self.DFNetdataset_test = DS_green_from_mask(self.test_dir)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9U1M1wGNe3D",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py (adding squeeze)\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "logdir='/content/'\n",
        "writer = SummaryWriter(logdir=logdir)\n",
        "\n",
        "from adamp import AdamP\n",
        "#from adamp import SGDP\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    ############################\n",
        "    # generators with one output, no AMP means nan loss during training\n",
        "\n",
        "    #self.netG = RRDBNet(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, upscale=4, norm_type='null',\n",
        "    #            act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D',\n",
        "    #            finalact=None, gaussian_noise=True, plus=False, \n",
        "    #            nr=3)\n",
        "\n",
        "\n",
        "    # DFNet\n",
        "    self.netG = DFNet(c_img=3, c_mask=1, c_alpha=3,\n",
        "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3, 3, 3, 3, 3, 3, 3, 3],\n",
        "            blend_layers=[0, 1, 2, 3, 4, 5], conv_type='partial')\n",
        "    \n",
        "    # AdaFill\n",
        "    #self.netG = InpaintNet()\n",
        "\n",
        "    # MEDFE (batch_size: 1, no AMP)\n",
        "    #self.netG = MEDFEGenerator()\n",
        "\n",
        "    # RFR\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = RFRNet(conv_type='partial')\n",
        "\n",
        "    # LBAM\n",
        "    #self.netG = LBAMModel(inputChannels=4, outputChannels=3)\n",
        "\n",
        "    # DMFN\n",
        "    #self.netG = InpaintingGenerator(in_nc=4, out_nc=3,nf=64,n_res=8,\n",
        "    #      norm='in', activation='relu')\n",
        "\n",
        "    # partial\n",
        "    #self.netG = Model()\n",
        "\n",
        "    # RN\n",
        "    #self.netG = G_Net(input_channels=3, residual_blocks=8, threshold=0.8)\n",
        "    # using rn init to avoid errors\n",
        "    #RN_arch = rn_initialize_weights(self.netG, scale=0.1)\n",
        "\n",
        "\n",
        "    ############################\n",
        "\n",
        "    # generators with two outputs\n",
        "\n",
        "    # deepfillv1\n",
        "    #self.netG = InpaintSANet()\n",
        "\n",
        "    # deepfillv2\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = GatedGenerator(in_channels=4, out_channels=3, \n",
        "    #  latent_channels=64, pad_type='zero', activation='lrelu', norm='in', conv_type = 'partial')\n",
        "\n",
        "    # Adaptive\n",
        "    # [Warning] Adaptive does not like PatchGAN, Multiscale and ResNet.\n",
        "    #self.netG = PyramidNet(in_channels=3, residual_blocks=1, init_weights='True')\n",
        "\n",
        "    ############################\n",
        "    # exotic generators\n",
        "\n",
        "    # Pluralistic\n",
        "    #self.netG = PluralisticGenerator(ngf_E=opt_net['ngf_E'], z_nc_E=opt_net['z_nc_E'], img_f_E=opt_net['img_f_E'], layers_E=opt_net['layers_E'], norm_E=opt_net['norm_E'], activation_E=opt_net['activation_E'],\n",
        "    #            ngf_G=opt_net['ngf_G'], z_nc_G=opt_net['z_nc_G'], img_f_G=opt_net['img_f_G'], L_G=opt_net['L_G'], output_scale_G=opt_net['output_scale_G'], norm_G=opt_net['norm_G'], activation_G=opt_net['activation_G'])\n",
        "\n",
        "    \n",
        "    # EdgeConnect\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #self.netG = EdgeConnectModel(residual_blocks_edge=8,\n",
        "    #        residual_blocks_inpaint=8, use_spectral_norm=True,\n",
        "    #        conv_type_edge='normal', conv_type_inpaint='normal')\n",
        "\n",
        "    # FRRN\n",
        "    #self.netG = FRRNet()\n",
        "\n",
        "    # PRVS\n",
        "    #self.netG = PRVSNet()\n",
        "\n",
        "    # CSA\n",
        "    #self.netG = InpaintNet(c_img=3, norm='instance', act_en='leaky_relu', \n",
        "    #                           act_de='relu')\n",
        "\n",
        "\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "    ############################\n",
        "\n",
        "\n",
        "    # discriminators\n",
        "    # size refers to input shape of tensor\n",
        "\n",
        "    self.netD = context_encoder()\n",
        "\n",
        "    # VGG\n",
        "    #self.netD = Discriminator_VGG(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN')\n",
        "    #self.netD = Discriminator_VGG_fea(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D',\n",
        "    #     arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4)\n",
        "    #self.netD = Discriminator_VGG_128_SN()\n",
        "    #self.netD = VGGFeatureExtractor(feature_layer=34,use_bn=False,use_input_norm=True,device=torch.device('cpu'),z_norm=False)\n",
        "\n",
        "    # PatchGAN\n",
        "    #self.netD = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #    use_sigmoid=False, getIntermFeat=False, patch=True, use_spectral_norm=False)\n",
        "\n",
        "    # Multiscale\n",
        "    #self.netD = MultiscaleDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #             use_sigmoid=False, num_D=3, getIntermFeat=False)\n",
        "\n",
        "    # ResNet\n",
        "    #self.netD = Discriminator_ResNet_128(in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA')\n",
        "    #self.netD = ResNet101FeatureExtractor(use_input_norm=True, device=torch.device('cpu'), z_norm=False)\n",
        "    \n",
        "    # MINC\n",
        "    #self.netD = MINCNet()\n",
        "\n",
        "    # Pixel\n",
        "    #self.netD = PixelDiscriminator(input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d)\n",
        "\n",
        "    # EfficientNet\n",
        "    #from efficientnet_pytorch import EfficientNet\n",
        "    #self.netD = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    # ResNeSt\n",
        "    # [\"resnest50\", \"resnest101\", \"resnest200\", \"resnest269\"]\n",
        "    #self.netD = resnest50(pretrained=True)\n",
        "\n",
        "    # need fixing\n",
        "    #FileNotFoundError: [Errno 2] No such file or directory: '../experiments/pretrained_models/VGG16minc_53.pth'\n",
        "    #self.netD = MINCFeatureExtractor(feature_layer=34, use_bn=False, use_input_norm=True, device=torch.device('cpu'))\n",
        "\n",
        "    # Transformer (Warning: uses own init!)\n",
        "    #self.netD  = TranformerDiscriminator(img_size=256, patch_size=1, in_chans=3, num_classes=1, embed_dim=64, depth=7,\n",
        "    #             num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "    #             drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm)\n",
        "    \n",
        "\n",
        "    weights_init(self.netD, 'kaiming')\n",
        "\n",
        "\n",
        "    # loss functions\n",
        "    self.l1 = nn.L1Loss()\n",
        "    l_hfen_type = L1CosineSim()\n",
        "    self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "    self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "    self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "    self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "    self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "    self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "    self.OFLoss = OFLoss()\n",
        "    self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "    self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "    self.StyleLoss = StyleLoss()\n",
        "    self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "    self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "    layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "        distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "        use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "    self.MSELoss = torch.nn.MSELoss()\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "\n",
        "    # metrics\n",
        "    self.psnr_metric = PSNR()\n",
        "    self.ssim_metric = SSIM()\n",
        "    self.ae_metric = AE()\n",
        "    self.mse_metric = MSE()\n",
        "\n",
        "\n",
        "  def forward(self, image, masks):\n",
        "      return self.netG(image, masks)\n",
        "\n",
        "  #def adversarial_loss(self, y_hat, y):\n",
        "  #    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      # train_batch[0][0] = batch_size\n",
        "      # train_batch[0] = masked\n",
        "      # train_batch[1] = mask\n",
        "      # train_batch[2] = original\n",
        "\n",
        "      # train generator\n",
        "      ############################\n",
        "      # generate fake (1 output)\n",
        "      squeeze0 = torch.squeeze(train_batch[0], 0)\n",
        "      squeeze1 = torch.squeeze(train_batch[1], 0)\n",
        "      squeeze2 = torch.squeeze(train_batch[2], 0)\n",
        "      out = self(squeeze0,squeeze1)\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      out = squeeze0*(squeeze1)+out*(1-squeeze1)\n",
        "\n",
        "      ############################\n",
        "      # generate fake (2 outputs)\n",
        "      #out, other_img = self(train_batch[0],train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # exotic generators\n",
        "      # CSA\n",
        "      #coarse_result, out, csa, csa_d = self(train_batch[0],train_batch[1])\n",
        "      \n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #out, other_img = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "      \n",
        "      # PVRS\n",
        "      #out, _ ,edge_small, edge_big = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "      # FRRN\n",
        "      #out, mid_x, mid_mask = self(train_batch[0], train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # loss calculation\n",
        "      total_loss = 0\n",
        "      \"\"\"\n",
        "      HFENLoss_forward = self.HFENLoss(out, train_batch[0])\n",
        "      total_loss += HFENLoss_forward\n",
        "      ElasticLoss_forward = self.ElasticLoss(out, train_batch[0])\n",
        "      total_loss += ElasticLoss_forward\n",
        "      RelativeL1_forward = self.RelativeL1(out, train_batch[0])\n",
        "      total_loss += RelativeL1_forward\n",
        "      \"\"\"\n",
        "      #print(\"out\")\n",
        "      #print(out.shape)\n",
        "      #print(\"squeeze2\")\n",
        "      #print(squeeze2.shape)\n",
        "      L1CosineSim_forward = 5*self.L1CosineSim(out, squeeze2)\n",
        "      total_loss += L1CosineSim_forward\n",
        "      #self.log('loss/L1CosineSim', L1CosineSim_forward)\n",
        "      writer.add_scalar('loss/L1CosineSim', L1CosineSim_forward, self.trainer.global_step)\n",
        "\n",
        "      \"\"\"\n",
        "      ClipL1_forward = self.ClipL1(out, train_batch[0])\n",
        "      total_loss += ClipL1_forward\n",
        "      FFTloss_forward = self.FFTloss(out, train_batch[0])\n",
        "      total_loss += FFTloss_forward\n",
        "      OFLoss_forward = self.OFLoss(out)\n",
        "      total_loss += OFLoss_forward\n",
        "      GPLoss_forward = self.GPLoss(out, train_batch[0])\n",
        "      total_loss += GPLoss_forward\n",
        "      \n",
        "      CPLoss_forward = 0.1*self.CPLoss(out, train_batch[0])\n",
        "      total_loss += CPLoss_forward\n",
        "      \n",
        "\n",
        "      Contextual_Loss_forward = self.Contextual_Loss(out, train_batch[0])\n",
        "      total_loss += Contextual_Loss_forward\n",
        "      self.log('loss/contextual', Contextual_Loss_forward)\n",
        "      \"\"\"\n",
        "\n",
        "      #style_forward = 240*self.StyleLoss(out, train_batch[2])\n",
        "      #total_loss += style_forward\n",
        "      #self.log('loss/style', style_forward)\n",
        "\n",
        "      tv_forward = 0.0000005*self.TVLoss(out)\n",
        "      total_loss += tv_forward\n",
        "      #self.log('loss/tv', tv_forward)\n",
        "      writer.add_scalar('loss/tv', tv_forward, self.trainer.global_step)\n",
        "\n",
        "      perceptual_forward = 2*self.PerceptualLoss(out, squeeze2)\n",
        "      total_loss += perceptual_forward\n",
        "      #self.log('loss/perceptual', perceptual_forward)\n",
        "      writer.add_scalar('loss/perceptual', perceptual_forward, self.trainer.global_step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #########################\n",
        "      # exotic loss\n",
        "\n",
        "      # if model has two output, also calculate loss for such an image\n",
        "      # example with just l1 loss\n",
        "      \n",
        "      #l1_stage1 = self.L1Loss(other_img, train_batch[0])\n",
        "      #self.log('loss/l1_stage1', l1_stage1)\n",
        "      #total_loss += l1_stage1\n",
        "\n",
        "\n",
        "      # CSA Loss\n",
        "      \"\"\"\n",
        "      recon_loss = self.L1Loss(coarse_result, train_batch[2]) + self.L1Loss(out, train_batch[2])\n",
        "      cons = ConsistencyLoss()\n",
        "      cons_loss = cons(csa, csa_d, train_batch[2], train_batch[1])\n",
        "      self.log('loss/recon_loss', recon_loss)\n",
        "      total_loss += recon_loss\n",
        "      self.log('loss/cons_loss', cons_loss)\n",
        "      total_loss += cons_loss\n",
        "      \"\"\"\n",
        "\n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #l1_edge = self.L1Loss(other_img, train_batch[3])\n",
        "      #self.log('loss/l1_edge', l1_edge)\n",
        "      #total_loss += l1_edge\n",
        "\n",
        "      # PVRS\n",
        "      \"\"\"\n",
        "      edge_big_l1 = self.L1Loss(edge_big, train_batch[3])\n",
        "      edge_small_l1 = self.L1Loss(edge_small, torch.nn.functional.interpolate(train_batch[3], scale_factor = 0.5))\n",
        "      self.log('loss/edge_big_l1', edge_big_l1)\n",
        "      total_loss += edge_big_l1\n",
        "      self.log('loss/edge_small_l1', edge_small_l1)\n",
        "      total_loss += edge_small_l1\n",
        "      \"\"\" \n",
        "\n",
        "      # FRRN\n",
        "      \"\"\"\n",
        "      mid_l1_loss = 0\n",
        "      for idx in range(len(mid_x) - 1):\n",
        "          mid_l1_loss += self.L1Loss(mid_x[idx] * mid_mask[idx], train_batch[2] * mid_mask[idx])\n",
        "      self.log('loss/mid_l1_loss', mid_l1_loss)\n",
        "      total_loss += mid_l1_loss\n",
        "      \"\"\"\n",
        "\n",
        "      #self.log('loss/g_loss', total_loss)\n",
        "      writer.add_scalar('loss/g_loss', total_loss, self.trainer.global_step)\n",
        "\n",
        "      #return total_loss\n",
        "      #########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # train discriminator\n",
        "      # resizing input if needed\n",
        "      #train_batch[2] = torch.nn.functional.interpolate(train_batch[2], (128,128), align_corners=False, mode='bilinear')\n",
        "      #out = torch.nn.functional.interpolate(out, (128,128), align_corners=False, mode='bilinear')\n",
        "\n",
        "      Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "      valid = Variable(Tensor(out.shape).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(out.shape).fill_(0.0), requires_grad=False)\n",
        "      dis_real_loss = self.MSELoss(squeeze2, valid)\n",
        "      dis_fake_loss = self.MSELoss(out, fake)\n",
        "\n",
        "      d_loss = (dis_real_loss + dis_fake_loss) / 2\n",
        "      #self.log('loss/d_loss', d_loss)\n",
        "      writer.add_scalar('loss/d_loss', d_loss, self.trainer.global_step)\n",
        "\n",
        "      return total_loss+d_loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      #optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-3)\n",
        "      optimizer = AdamP(self.netG.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "      #optimizer = SGDP(self.netG.parameters(), lr=0.1, weight_decay=1e-5, momentum=0.9, nesterov=True)\n",
        "      return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "\n",
        "    #########################\n",
        "    # generate fake (one output generator)\n",
        "    out = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    #########################\n",
        "    # generate fake (two output generator)\n",
        "    #out, _ = self(train_batch[0],train_batch[1])\n",
        "\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "    #########################\n",
        "    # CSA\n",
        "    #_, out, _, _ = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    # EdgeConnect\n",
        "    # train_batch[3] = edges\n",
        "    # train_batch[4] = grayscale\n",
        "    #out, _ = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "\n",
        "    # PVRS\n",
        "    #out, _ ,_, _ = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "    # FRRN\n",
        "    #out, _, _ = self(train_batch[0], train_batch[1])\n",
        "\n",
        "    \"\"\"\n",
        "    # Validation metrics work, but they need an origial source image, which is\n",
        "    # not implemented. Change dataloader to provide LR and HR if you want metrics.\n",
        "    self.log('metrics/PSNR', self.psnr_metric(train_batch[2], out))\n",
        "    self.log('metrics/SSIM', self.ssim_metric(train_batch[2], out))\n",
        "    self.log('metrics/MSE', self.mse_metric(train_batch[2], out))\n",
        "    self.log('metrics/LPIPS', self.PerceptualLoss(out, train_batch[2]))\n",
        "    \"\"\"\n",
        "\n",
        "    validation_output = '/content/validation_output/' #@param\n",
        "\n",
        "    # train_batch[3] can contain multiple files, depending on the batch_size\n",
        "    for f in train_batch[2]:\n",
        "      # data is processed as a batch, to save indididual files, a counter is used\n",
        "      counter = 0\n",
        "      if not os.path.exists(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0])):\n",
        "        os.makedirs(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0]))\n",
        "\n",
        "      filename_with_extention = os.path.basename(f)\n",
        "      filename = os.path.splitext(filename_with_extention)[0]\n",
        "      save_image(out[counter], os.path.join(validation_output, filename, str(self.trainer.global_step) + '.png'))\n",
        "\n",
        "      counter += 1\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "    test_output = '/content/test_output/' #@param\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv2IFzTH9W8V"
      },
      "source": [
        "# Data (Simple lr/hr folder loader (for 3-channel super resolution))\n",
        "\n",
        "Just take this if you want to provide lr and hr with folders. Same for validation. Applies random crop if hr_size is smaller than the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuShtAL-RZG",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, lr_path, hr_path, hr_size, scale):\n",
        "        self.samples = []\n",
        "        for hr_path, _, fnames in sorted(os.walk(hr_path)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(hr_path, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + hr_path)\n",
        "        self.hr_size = hr_size\n",
        "        self.scale = scale\n",
        "        self.lr_path = lr_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # getting hr image\n",
        "        hr_path = self.samples[index]\n",
        "        hr_image = cv2.imread(hr_path)\n",
        "        hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # getting lr image\n",
        "        lr_path = os.path.join(self.lr_path, os.path.basename(hr_path))\n",
        "        lr_image = cv2.imread(lr_path)\n",
        "        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # checking for hr_size limitation\n",
        "        if hr_image.shape[0] > self.hr_size or hr_image.shape[1] > self.hr_size:\n",
        "          # image too big, random crop\n",
        "          random_pos1 = random.randint(0,hr_image.shape[0]-self.hr_size)\n",
        "          random_pos2 = random.randint(0,hr_image.shape[0]-self.hr_size)\n",
        "\n",
        "          image_hr = hr_image[random_pos1:random_pos1+self.hr_size, random_pos2:random_pos2+self.hr_size]\n",
        "          image_lr = lr_image[int(random_pos1/self.scale):int((random_pos2+self.hr_size)/self.scale), int(random_pos2/self.scale):int((random_pos2+self.hr_size)/self.scale)]\n",
        "\n",
        "        # to tensor\n",
        "        hr_image = torch.from_numpy(hr_image).permute(2, 0, 1)/255\n",
        "        lr_image = torch.from_numpy(lr_image).permute(2, 0, 1)/255\n",
        "\n",
        "        return lr_image, hr_image\n",
        "\n",
        "\n",
        "class DS_val(Dataset):\n",
        "    def __init__(self, lr_path, hr_path):\n",
        "        self.samples = []\n",
        "        for hr_path, _, fnames in sorted(os.walk(hr_path)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(hr_path, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + hr_path)\n",
        "\n",
        "        self.lr_path = lr_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # getting hr image\n",
        "        hr_path = self.samples[index]\n",
        "        hr_image = cv2.imread(hr_path)\n",
        "\n",
        "        # getting lr image\n",
        "        lr_path = os.path.join(self.lr_path, os.path.basename(hr_path))\n",
        "        lr_image = cv2.imread(lr_path)\n",
        "\n",
        "        # to tensor\n",
        "        hr_image = torch.from_numpy(hr_image).permute(2, 0, 1)/255\n",
        "        lr_image = torch.from_numpy(lr_image).permute(2, 0, 1)/255\n",
        "\n",
        "        return lr_image, hr_image, lr_path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx7_X3Yr-WOv",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dir_lr: str = './',  dir_hr: str = './', val_lr: str = './', val_hr: str = './', batch_size: int = 5, num_workers: int = 2, hr_size = 256, scale = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dir_lr = dir_lr\n",
        "        self.dir_hr = dir_hr\n",
        "\n",
        "        self.val_lr = val_lr\n",
        "        self.val_hr = val_hr\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.hr_size = hr_size\n",
        "        self.scale = scale\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.DFNetdataset_train = DS(lr_path=self.dir_lr, hr_path=self.dir_hr, hr_size = self.hr_size, scale = self.scale)\n",
        "        self.DFNetdataset_validation = DS_val(self.val_lr, self.val_hr)\n",
        "        self.DFNetdataset_test = DS_val(self.val_lr, self.val_hr)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur82Z5hO3GgZ"
      },
      "source": [
        "# Data (3x3 400px, batch (for 1-channel super resolution))\n",
        "This is for ESRGAN, reads images as single channel image and creates lr/hr pairs by using random downscaling. Uses 3x3 images as input for 4x training. Uses lr/hr folders for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qemhtmrl3YAt",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, root, transform=None, size=256, batch_size_DL = 3, scale=4, image_size=400, amount_tiles=3):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        #self.size = size\n",
        "        self.image_size = image_size # how big one tile is\n",
        "        self.scale = scale\n",
        "        self.batch_size = batch_size_DL\n",
        "        self.interpolation_method = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4]\n",
        "        self.amount_tiles = amount_tiles\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        sample = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        \n",
        "        pos_total = []\n",
        "\n",
        "        self.total_size = 0 # the current amount of images that got a random position\n",
        "\n",
        "        while True:\n",
        "          # determine random position\n",
        "          x_rand = random.randint(0,self.amount_tiles-1)\n",
        "          y_rand = random.randint(0,self.amount_tiles-1)\n",
        "          \n",
        "          pos_rand = [x_rand, y_rand]\n",
        "\n",
        "          if (pos_rand in pos_total) != True:\n",
        "            pos_total.append(pos_rand)\n",
        "            self.total_size += 1\n",
        "\n",
        "          # return batchsize\n",
        "          if self.total_size == self.batch_size:\n",
        "            break\n",
        "\n",
        "        self.total_size = 0 # counter for making sure array gets appended if processed images > 1\n",
        "        \n",
        "        for i in pos_total:\n",
        "          # creating sample if for start\n",
        "          if self.total_size == 0:\n",
        "            # cropping from hr image\n",
        "            image_hr = sample[i[0]*self.image_size:(i[0]+1)*self.image_size, i[1]*self.image_size:(i[1]+1)*self.image_size]\n",
        "            # creating lr on the fly\n",
        "            #image_lr = cv2.resize(image_hr, (int(self.image_size/self.scale), int(self.image_size/self.scale)), interpolation=random.choice(self.interpolation_method))\n",
        "            image_lr = cv2.resize(image_hr, (int(self.image_size/self.scale), int(self.image_size/self.scale)), interpolation=random.choice(self.interpolation_method))\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"-----------------------\")\n",
        "            print(i[0]*(self.image_size/self.scale))\n",
        "            print((i[0]+1)*(self.image_size/self.scale))\n",
        "             \n",
        "            print(i[1]*(self.image_size/self.scale))\n",
        "            print((i[1]+1)*(self.image_size/self.scale))\n",
        "            \"\"\"\n",
        "            #image_lr = image_lr[i[0]*(self.image_size/self.scale):(i[0]+1)*(self.image_size/self.scale), i[1]*(self.image_size/self.scale):(i[1]+1)*(self.image_size/self.scale)]\n",
        "\n",
        "\n",
        "            # creating torch tensor\n",
        "            image_hr = torch.from_numpy(image_hr).unsqueeze(2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "            image_lr = torch.from_numpy(image_lr).unsqueeze(2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "            \n",
        "\n",
        "            # if edges are required\n",
        "            \"\"\"\n",
        "            grayscale = cv2.cvtColor(np.array(sample_add), cv2.COLOR_RGB2GRAY)\n",
        "            edges = cv2.Canny(grayscale,100,150)\n",
        "            grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "            edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "            \"\"\"\n",
        "\n",
        "            self.total_size += 1\n",
        "          else:\n",
        "            # cropping from hr image\n",
        "            image_hr2 = sample[i[0]*self.image_size:(i[0]+1)*self.image_size, i[1]*self.image_size:(i[1]+1)*self.image_size]\n",
        "            # creating lr on the fly\n",
        "            #image_lr2 = cv2.resize(image_hr2, (int(self.image_size/self.scale), int(self.image_size/self.scale)), interpolation=random.choice(self.interpolation_method))\n",
        "            #image_lr2 = image_lr2[i[0]*(self.image_size/self.scale):(i[0]+1)*(self.image_size/self.scale), i[1]*(self.image_size/self.scale):(i[1]+1)*(self.image_size/self.scale)]\n",
        "            image_lr2 = cv2.resize(image_hr2, (int(self.image_size/self.scale), int(self.image_size/self.scale)), interpolation=random.choice(self.interpolation_method))\n",
        "\n",
        "\n",
        "\n",
        "            # if edges are required\n",
        "            \"\"\"\n",
        "            grayscale = cv2.cvtColor(np.array(sample_add2), cv2.COLOR_RGB2GRAY)\n",
        "            edges = cv2.Canny(grayscale,100,150)\n",
        "            grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "            edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "            \"\"\"\n",
        "            # creating torch tensor\n",
        "            image_hr2 = torch.from_numpy(image_hr2).unsqueeze(2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "            image_hr = torch.cat((image_hr, image_hr2), dim=0)\n",
        "            \n",
        "            image_lr2 = torch.from_numpy(image_lr2).unsqueeze(2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "            image_lr = torch.cat((image_lr, image_lr2), dim=0)\n",
        "\n",
        "        return image_lr, image_hr\n",
        "\n",
        "\n",
        "class DS_val(Dataset):\n",
        "    def __init__(self, lr_path, hr_path):\n",
        "        self.samples = []\n",
        "        for hr_path, _, fnames in sorted(os.walk(hr_path)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(hr_path, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + hr_path)\n",
        "\n",
        "        self.lr_path = lr_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # getting hr image\n",
        "        hr_path = self.samples[index]\n",
        "        hr_image = cv2.imread(hr_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # getting lr image\n",
        "        lr_path = os.path.join(self.lr_path, os.path.basename(hr_path))\n",
        "        lr_image = cv2.imread(lr_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "\n",
        "\n",
        "        hr_image = torch.from_numpy(hr_image).unsqueeze(2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "        lr_image = torch.from_numpy(lr_image).unsqueeze(2).permute(2, 0, 1).unsqueeze(0)/255\n",
        "\n",
        "\n",
        "        return lr_image, hr_image, lr_path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq55H4zV3aRv",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', val_lr: str = './', val_hr: str = './', batch_size: int = 5, batch_size_DL: int = 2, num_workers: int = 2, hr_size=256, scale = 4, image_size = 400, amount_tiles=3):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.val_lr = val_lr\n",
        "        self.val_hr = val_hr\n",
        "\n",
        "        #self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.batch_size_DL = batch_size_DL\n",
        "        self.num_workers = num_workers\n",
        "        self.hr_size = hr_size\n",
        "        self.scale = scale\n",
        "        self.image_size = image_size\n",
        "        self.amount_tiles = amount_tiles\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.DFNetdataset_train = DS(self.training_dir, self.hr_size, batch_size_DL = self.batch_size_DL, scale=self.scale, image_size = self.image_size, amount_tiles = self.amount_tiles)\n",
        "        self.DFNetdataset_validation = DS_val(self.val_lr, self.val_hr)\n",
        "        self.DFNetdataset_test = DS_val(self.val_lr, self.val_hr)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6PgVGbN3dD6",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py (adding squeeze)\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "logdir='/content/'\n",
        "writer = SummaryWriter(logdir=logdir)\n",
        "\n",
        "from adamp import AdamP\n",
        "#from adamp import SGDP\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    ############################\n",
        "    # generators with one output, no AMP means nan loss during training\n",
        "\n",
        "    self.netG = RRDBNet(in_nc=1, out_nc=1, nf=64, nb=23, gc=32, upscale=4, norm_type=None,\n",
        "                act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D',\n",
        "                finalact=None, gaussian_noise=True, plus=False, \n",
        "                nr=3)\n",
        "\n",
        "\n",
        "    # DFNet\n",
        "    #self.netG = DFNet(c_img=3, c_mask=1, c_alpha=3,\n",
        "    #        mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "    #        en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3, 3, 3, 3, 3, 3, 3, 3],\n",
        "    #        blend_layers=[0, 1, 2, 3, 4, 5], conv_type='partial')\n",
        "    \n",
        "    # AdaFill\n",
        "    #self.netG = InpaintNet()\n",
        "\n",
        "    # MEDFE (batch_size: 1, no AMP)\n",
        "    #self.netG = MEDFEGenerator()\n",
        "\n",
        "    # RFR\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = RFRNet(conv_type='partial')\n",
        "\n",
        "    # LBAM\n",
        "    #self.netG = LBAMModel(inputChannels=4, outputChannels=3)\n",
        "\n",
        "    # DMFN\n",
        "    #self.netG = InpaintingGenerator(in_nc=4, out_nc=3,nf=64,n_res=8,\n",
        "    #      norm='in', activation='relu')\n",
        "\n",
        "    # partial\n",
        "    #self.netG = Model()\n",
        "\n",
        "    # RN\n",
        "    #self.netG = G_Net(input_channels=3, residual_blocks=8, threshold=0.8)\n",
        "    # using rn init to avoid errors\n",
        "    #RN_arch = rn_initialize_weights(self.netG, scale=0.1)\n",
        "\n",
        "\n",
        "    ############################\n",
        "\n",
        "    # generators with two outputs\n",
        "\n",
        "    # deepfillv1\n",
        "    #self.netG = InpaintSANet()\n",
        "\n",
        "    # deepfillv2\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = GatedGenerator(in_channels=4, out_channels=3, \n",
        "    #  latent_channels=64, pad_type='zero', activation='lrelu', norm='in', conv_type = 'partial')\n",
        "\n",
        "    # Adaptive\n",
        "    # [Warning] Adaptive does not like PatchGAN, Multiscale and ResNet.\n",
        "    #self.netG = PyramidNet(in_channels=3, residual_blocks=1, init_weights='True')\n",
        "\n",
        "    ############################\n",
        "    # exotic generators\n",
        "\n",
        "    # Pluralistic\n",
        "    #self.netG = PluralisticGenerator(ngf_E=opt_net['ngf_E'], z_nc_E=opt_net['z_nc_E'], img_f_E=opt_net['img_f_E'], layers_E=opt_net['layers_E'], norm_E=opt_net['norm_E'], activation_E=opt_net['activation_E'],\n",
        "    #            ngf_G=opt_net['ngf_G'], z_nc_G=opt_net['z_nc_G'], img_f_G=opt_net['img_f_G'], L_G=opt_net['L_G'], output_scale_G=opt_net['output_scale_G'], norm_G=opt_net['norm_G'], activation_G=opt_net['activation_G'])\n",
        "\n",
        "    \n",
        "    # EdgeConnect\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #self.netG = EdgeConnectModel(residual_blocks_edge=8,\n",
        "    #        residual_blocks_inpaint=8, use_spectral_norm=True,\n",
        "    #        conv_type_edge='normal', conv_type_inpaint='normal')\n",
        "\n",
        "    # FRRN\n",
        "    #self.netG = FRRNet()\n",
        "\n",
        "    # PRVS\n",
        "    #self.netG = PRVSNet()\n",
        "\n",
        "    # CSA\n",
        "    #self.netG = InpaintNet(c_img=3, norm='instance', act_en='leaky_relu', \n",
        "    #                           act_de='relu')\n",
        "\n",
        "\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "    ############################\n",
        "\n",
        "\n",
        "    # discriminators\n",
        "    # size refers to input shape of tensor\n",
        "\n",
        "    self.netD = context_encoder()\n",
        "\n",
        "    # VGG\n",
        "    #self.netD = Discriminator_VGG(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN')\n",
        "    #self.netD = Discriminator_VGG_fea(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D',\n",
        "    #     arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4)\n",
        "    #self.netD = Discriminator_VGG_128_SN()\n",
        "    #self.netD = VGGFeatureExtractor(feature_layer=34,use_bn=False,use_input_norm=True,device=torch.device('cpu'),z_norm=False)\n",
        "\n",
        "    # PatchGAN\n",
        "    #self.netD = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #    use_sigmoid=False, getIntermFeat=False, patch=True, use_spectral_norm=False)\n",
        "\n",
        "    # Multiscale\n",
        "    #self.netD = MultiscaleDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #             use_sigmoid=False, num_D=3, getIntermFeat=False)\n",
        "\n",
        "    # ResNet\n",
        "    #self.netD = Discriminator_ResNet_128(in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA')\n",
        "    #self.netD = ResNet101FeatureExtractor(use_input_norm=True, device=torch.device('cpu'), z_norm=False)\n",
        "    \n",
        "    # MINC\n",
        "    #self.netD = MINCNet()\n",
        "\n",
        "    # Pixel\n",
        "    #self.netD = PixelDiscriminator(input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d)\n",
        "\n",
        "    # EfficientNet\n",
        "    #from efficientnet_pytorch import EfficientNet\n",
        "    #self.netD = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    # ResNeSt\n",
        "    # [\"resnest50\", \"resnest101\", \"resnest200\", \"resnest269\"]\n",
        "    #self.netD = resnest50(pretrained=True)\n",
        "\n",
        "    # need fixing\n",
        "    #FileNotFoundError: [Errno 2] No such file or directory: '../experiments/pretrained_models/VGG16minc_53.pth'\n",
        "    #self.netD = MINCFeatureExtractor(feature_layer=34, use_bn=False, use_input_norm=True, device=torch.device('cpu'))\n",
        "\n",
        "    # Transformer (Warning: uses own init!)\n",
        "    #self.netD  = TranformerDiscriminator(img_size=256, patch_size=1, in_chans=3, num_classes=1, embed_dim=64, depth=7,\n",
        "    #             num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "    #             drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm)\n",
        "    \n",
        "\n",
        "    weights_init(self.netD, 'kaiming')\n",
        "\n",
        "\n",
        "    # loss functions\n",
        "    self.l1 = nn.L1Loss()\n",
        "    l_hfen_type = L1CosineSim()\n",
        "    self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "    self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "    self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "    self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "    self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "    self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "    self.OFLoss = OFLoss()\n",
        "    self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "    self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "    self.StyleLoss = StyleLoss()\n",
        "    self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "    self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "    layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "        distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "        use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "    self.MSELoss = torch.nn.MSELoss()\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "\n",
        "    # metrics\n",
        "    self.psnr_metric = PSNR()\n",
        "    self.ssim_metric = SSIM()\n",
        "    self.ae_metric = AE()\n",
        "    self.mse_metric = MSE()\n",
        "\n",
        "\n",
        "  # inpainting\n",
        "  #def forward(self, image, masks):\n",
        "  #    return self.netG(image, masks)\n",
        "\n",
        "  # super resolution\n",
        "  def forward(self, image):\n",
        "    return self.netG(image)\n",
        "\n",
        "  #def adversarial_loss(self, y_hat, y):\n",
        "  #    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      # inpainting:\n",
        "      # train_batch[0][0] = batch_size\n",
        "      # train_batch[0] = masked\n",
        "      # train_batch[1] = mask\n",
        "      # train_batch[2] = original\n",
        "\n",
        "      # super resolution\n",
        "      # train_batch[0] = lr\n",
        "      # train_batch[1] = hr\n",
        "\n",
        "      # train generator\n",
        "      ############################\n",
        "      # generate fake (1 output)\n",
        "      squeeze0 = torch.squeeze(train_batch[0], 0)\n",
        "      squeeze1 = torch.squeeze(train_batch[1], 0)\n",
        "      #squeeze2 = torch.squeeze(train_batch[2], 0)\n",
        "      #out = self(squeeze0,squeeze1)\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = squeeze0*(squeeze1)+out*(1-squeeze1)\n",
        "\n",
        "      ############################\n",
        "      # generate fake (2 outputs)\n",
        "      #out, other_img = self(train_batch[0],train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # exotic generators\n",
        "      # CSA\n",
        "      #coarse_result, out, csa, csa_d = self(train_batch[0],train_batch[1])\n",
        "      \n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #out, other_img = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "      \n",
        "      # PVRS\n",
        "      #out, _ ,edge_small, edge_big = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "      # FRRN\n",
        "      #out, mid_x, mid_mask = self(train_batch[0], train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # ESRGAN\n",
        "      out = self.netG(squeeze0, squeeze1)\n",
        "\n",
        "      ############################\n",
        "      # loss calculation\n",
        "      total_loss = 0\n",
        "      \"\"\"\n",
        "      HFENLoss_forward = self.HFENLoss(out, train_batch[0])\n",
        "      total_loss += HFENLoss_forward\n",
        "      ElasticLoss_forward = self.ElasticLoss(out, train_batch[0])\n",
        "      total_loss += ElasticLoss_forward\n",
        "      RelativeL1_forward = self.RelativeL1(out, train_batch[0])\n",
        "      total_loss += RelativeL1_forward\n",
        "      \"\"\"\n",
        "      #print(\"out\")\n",
        "      #print(out.shape)\n",
        "      #print(\"squeeze2\")\n",
        "      #print(squeeze2.shape)\n",
        "      L1CosineSim_forward = 5*self.L1CosineSim(out, squeeze1)\n",
        "      total_loss += L1CosineSim_forward\n",
        "      #self.log('loss/L1CosineSim', L1CosineSim_forward)\n",
        "      writer.add_scalar('loss/L1CosineSim', L1CosineSim_forward, self.trainer.global_step)\n",
        "\n",
        "      \"\"\"\n",
        "      ClipL1_forward = self.ClipL1(out, train_batch[0])\n",
        "      total_loss += ClipL1_forward\n",
        "      FFTloss_forward = self.FFTloss(out, train_batch[0])\n",
        "      total_loss += FFTloss_forward\n",
        "      OFLoss_forward = self.OFLoss(out)\n",
        "      total_loss += OFLoss_forward\n",
        "      GPLoss_forward = self.GPLoss(out, train_batch[0])\n",
        "      total_loss += GPLoss_forward\n",
        "      \n",
        "      CPLoss_forward = 0.1*self.CPLoss(out, train_batch[0])\n",
        "      total_loss += CPLoss_forward\n",
        "      \n",
        "\n",
        "      Contextual_Loss_forward = self.Contextual_Loss(out, train_batch[0])\n",
        "      total_loss += Contextual_Loss_forward\n",
        "      self.log('loss/contextual', Contextual_Loss_forward)\n",
        "      \"\"\"\n",
        "\n",
        "      #style_forward = 240*self.StyleLoss(out, train_batch[2])\n",
        "      #total_loss += style_forward\n",
        "      #self.log('loss/style', style_forward)\n",
        "\n",
        "      tv_forward = 0.0000005*self.TVLoss(out)\n",
        "      total_loss += tv_forward\n",
        "      #self.log('loss/tv', tv_forward)\n",
        "      writer.add_scalar('loss/tv', tv_forward, self.trainer.global_step)\n",
        "\n",
        "      perceptual_forward = 2*self.PerceptualLoss(out, squeeze1)\n",
        "      total_loss += perceptual_forward\n",
        "      #self.log('loss/perceptual', perceptual_forward)\n",
        "      writer.add_scalar('loss/perceptual', perceptual_forward, self.trainer.global_step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #########################\n",
        "      # exotic loss\n",
        "\n",
        "      # if model has two output, also calculate loss for such an image\n",
        "      # example with just l1 loss\n",
        "      \n",
        "      #l1_stage1 = self.L1Loss(other_img, train_batch[0])\n",
        "      #self.log('loss/l1_stage1', l1_stage1)\n",
        "      #total_loss += l1_stage1\n",
        "\n",
        "\n",
        "      # CSA Loss\n",
        "      \"\"\"\n",
        "      recon_loss = self.L1Loss(coarse_result, train_batch[2]) + self.L1Loss(out, train_batch[2])\n",
        "      cons = ConsistencyLoss()\n",
        "      cons_loss = cons(csa, csa_d, train_batch[2], train_batch[1])\n",
        "      self.log('loss/recon_loss', recon_loss)\n",
        "      total_loss += recon_loss\n",
        "      self.log('loss/cons_loss', cons_loss)\n",
        "      total_loss += cons_loss\n",
        "      \"\"\"\n",
        "\n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #l1_edge = self.L1Loss(other_img, train_batch[3])\n",
        "      #self.log('loss/l1_edge', l1_edge)\n",
        "      #total_loss += l1_edge\n",
        "\n",
        "      # PVRS\n",
        "      \"\"\"\n",
        "      edge_big_l1 = self.L1Loss(edge_big, train_batch[3])\n",
        "      edge_small_l1 = self.L1Loss(edge_small, torch.nn.functional.interpolate(train_batch[3], scale_factor = 0.5))\n",
        "      self.log('loss/edge_big_l1', edge_big_l1)\n",
        "      total_loss += edge_big_l1\n",
        "      self.log('loss/edge_small_l1', edge_small_l1)\n",
        "      total_loss += edge_small_l1\n",
        "      \"\"\" \n",
        "\n",
        "      # FRRN\n",
        "      \"\"\"\n",
        "      mid_l1_loss = 0\n",
        "      for idx in range(len(mid_x) - 1):\n",
        "          mid_l1_loss += self.L1Loss(mid_x[idx] * mid_mask[idx], train_batch[2] * mid_mask[idx])\n",
        "      self.log('loss/mid_l1_loss', mid_l1_loss)\n",
        "      total_loss += mid_l1_loss\n",
        "      \"\"\"\n",
        "\n",
        "      #self.log('loss/g_loss', total_loss)\n",
        "      writer.add_scalar('loss/g_loss', total_loss, self.trainer.global_step)\n",
        "\n",
        "      #return total_loss\n",
        "      #########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # train discriminator\n",
        "      # resizing input if needed\n",
        "      #train_batch[2] = torch.nn.functional.interpolate(train_batch[2], (128,128), align_corners=False, mode='bilinear')\n",
        "      #out = torch.nn.functional.interpolate(out, (128,128), align_corners=False, mode='bilinear')\n",
        "\n",
        "      Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "      valid = Variable(Tensor(out.shape).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(out.shape).fill_(0.0), requires_grad=False)\n",
        "      dis_real_loss = self.MSELoss(squeeze1, valid)\n",
        "      dis_fake_loss = self.MSELoss(out, fake)\n",
        "\n",
        "      d_loss = (dis_real_loss + dis_fake_loss) / 2\n",
        "      #self.log('loss/d_loss', d_loss)\n",
        "      writer.add_scalar('loss/d_loss', d_loss, self.trainer.global_step)\n",
        "\n",
        "      return total_loss+d_loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      #optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-3)\n",
        "      optimizer = AdamP(self.netG.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "      #optimizer = SGDP(self.netG.parameters(), lr=0.1, weight_decay=1e-5, momentum=0.9, nesterov=True)\n",
        "      return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    # inpainting\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "\n",
        "    # super resolution\n",
        "    # train_batch[0] = lr\n",
        "    # train_batch[1] = hr\n",
        "    # train_batch[2] = lr_path\n",
        "\n",
        "    #########################\n",
        "    # generate fake (one output generator)\n",
        "    #out = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    #########################\n",
        "    # generate fake (two output generator)\n",
        "    #out, _ = self(train_batch[0],train_batch[1])\n",
        "\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "    #########################\n",
        "    # CSA\n",
        "    #_, out, _, _ = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    # EdgeConnect\n",
        "    # train_batch[3] = edges\n",
        "    # train_batch[4] = grayscale\n",
        "    #out, _ = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "\n",
        "    # PVRS\n",
        "    #out, _ ,_, _ = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "    # FRRN\n",
        "    #out, _, _ = self(train_batch[0], train_batch[1])\n",
        "\n",
        "    # ESRGAN\n",
        "    out = self.netG(train_batch[0].squeeze(0), train_batch[1].squeeze(0))\n",
        "\n",
        "\n",
        "    # Validation metrics work, but they need an origial source image\n",
        "    #self.log('metrics/PSNR', self.psnr_metric(train_batch[1], out))\n",
        "    #self.log('metrics/SSIM', self.ssim_metric(train_batch[1], out))\n",
        "    #self.log('metrics/MSE', self.mse_metric(train_batch[1], out))\n",
        "    #self.log('metrics/LPIPS', self.PerceptualLoss(out, train_batch[1]))\n",
        "\n",
        "    validation_output = '/content/validation_output/' #@param\n",
        "\n",
        "    # train_batch[3] can contain multiple files, depending on the batch_size\n",
        "    for f in train_batch[2]:\n",
        "      # data is processed as a batch, to save indididual files, a counter is used\n",
        "      counter = 0\n",
        "      if not os.path.exists(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0])):\n",
        "        os.makedirs(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0]))\n",
        "\n",
        "      filename_with_extention = os.path.basename(f)\n",
        "      filename = os.path.splitext(filename_with_extention)[0]\n",
        "      save_image(out[counter], os.path.join(validation_output, filename, str(self.trainer.global_step) + '.png'))\n",
        "\n",
        "      counter += 1\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # inpainting\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "\n",
        "    # super resolution\n",
        "    # train_batch[0] = lr\n",
        "    # train_batch[1] = hr\n",
        "\n",
        "    test_output = '/content/test_output/' #@param\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0BPZ2OU03Ae"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEn4yCogbFaW",
        "cellView": "form"
      },
      "source": [
        "#@title checkpoint.py\n",
        "#https://github.com/PyTorchLightning/pytorch-lightning/issues/2534\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    \"\"\"\n",
        "    Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n",
        "    based on validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_step_frequency,\n",
        "        prefix=\"Checkpoint\",\n",
        "        use_modelcheckpoint_filename=False,\n",
        "        save_path = '/content/'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "                use_modelcheckpoint_filename=False\n",
        "            use_modelcheckpoint_filename: just use the ModelCheckpoint callback's\n",
        "                default filename, don't use ours.\n",
        "        \"\"\"\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "        self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            if self.use_modelcheckpoint_filename:\n",
        "                filename = trainer.checkpoint_callback.filename\n",
        "            else:\n",
        "                filename = f\"{self.prefix}_{epoch}_{global_step}.ckpt\"\n",
        "            #ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            ckpt_path = os.path.join(self.save_path, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "\n",
        "            # saving normal .pth models\n",
        "            #https://github.com/PyTorchLightning/pytorch-lightning/issues/4114\n",
        "            torch.save(trainer.model.netG.state_dict(), f\"{self.prefix}_{epoch}_{global_step}_G.pth\")\n",
        "            torch.save(trainer.model.netD.state_dict(), f\"{self.prefix}_{epoch}_{global_step}_D.pth\")\n",
        "\n",
        "            # run validation once checkpoint was made\n",
        "            trainer.run_evaluation()\n",
        "\n",
        "\n",
        "    def on_train_end(self, trainer, pl_module):\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        ckpt_path = os.path.join(self.save_path, f\"{self.prefix}_{epoch}_{global_step}.ckpt\")\n",
        "        trainer.save_checkpoint(ckpt_path)\n",
        "        print(\"Checkpoint \" + f\"{self.prefix}_{epoch}_{global_step}.ckpt\" + \" saved.\")\n",
        "\n",
        "#Trainer(callbacks=[CheckpointEveryNSteps()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EfvO0fBULzf",
        "cellView": "form"
      },
      "source": [
        "#@title utils.py\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_like(x, target, mode='bilinear'):\n",
        "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
        "\n",
        "\n",
        "def list2nparray(lst, dtype=None):\n",
        "    \"\"\"fast conversion from nested list to ndarray by pre-allocating space\"\"\"\n",
        "    if isinstance(lst, np.ndarray):\n",
        "        return lst\n",
        "    assert isinstance(lst, (list, tuple)), 'bad type: {}'.format(type(lst))\n",
        "    assert lst, 'attempt to convert empty list to np array'\n",
        "    if isinstance(lst[0], np.ndarray):\n",
        "        dim1 = lst[0].shape\n",
        "        assert all(i.shape == dim1 for i in lst)\n",
        "        if dtype is None:\n",
        "            dtype = lst[0].dtype\n",
        "            assert all(i.dtype == dtype for i in lst), \\\n",
        "                'bad dtype: {} {}'.format(dtype, set(i.dtype for i in lst))\n",
        "    elif isinstance(lst[0], (int, float, complex, np.number)):\n",
        "        return np.array(lst, dtype=dtype)\n",
        "    else:\n",
        "        dim1 = list2nparray(lst[0])\n",
        "        if dtype is None:\n",
        "            dtype = dim1.dtype\n",
        "        dim1 = dim1.shape\n",
        "    shape = [len(lst)] + list(dim1)\n",
        "    rst = np.empty(shape, dtype=dtype)\n",
        "    for idx, i in enumerate(lst):\n",
        "        rst[idx] = i\n",
        "    return rst\n",
        "\n",
        "\n",
        "def get_img_list(path):\n",
        "    return sorted(list(Path(path).glob('*.png'))) + \\\n",
        "        sorted(list(Path(path).glob('*.jpg'))) + \\\n",
        "        sorted(list(Path(path).glob('*.jpeg')))\n",
        "\n",
        "\n",
        "def gen_miss(img, mask, output):\n",
        "\n",
        "    imgs = get_img_list(img)\n",
        "    masks = get_img_list(mask)\n",
        "    print('Total images:', len(imgs), len(masks))\n",
        "\n",
        "    out = Path(output)\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for i, (img, mask) in tqdm(enumerate(zip(imgs, masks))):\n",
        "        path = out.joinpath('miss_%04d.png' % (i+1))\n",
        "        img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
        "        mask = cv2.imread(str(mask), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, img.shape[:2][::-1])\n",
        "        mask = mask[..., np.newaxis]\n",
        "        miss = img * (mask > 127) + 255 * (mask <= 127)\n",
        "        cv2.imwrite(str(path), miss)\n",
        "\n",
        "def merge_imgs(dirs, output, row=1, gap=2, res=512):\n",
        "\n",
        "    image_list = [get_img_list(path) for path in dirs]\n",
        "    img_count = [len(image) for image in image_list]\n",
        "    print('Total images:', img_count)\n",
        "    assert min(img_count) > 0, 'Please check the path of empty folder.'\n",
        "\n",
        "    output_dir = Path(output)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    n_img = len(dirs)\n",
        "    row = row\n",
        "    column = (n_img - 1) // row + 1\n",
        "    print('Row:', row)\n",
        "    print('Column:', column)\n",
        "\n",
        "    for i, unit in tqdm(enumerate(zip(*image_list))):\n",
        "        name = output_dir.joinpath('merge_%04d.png' % i)\n",
        "        merge = np.ones([\n",
        "            res*row + (row+1)*gap, res*column + (column+1)*gap, 3], np.uint8) * 255\n",
        "        for j, img in enumerate(unit):\n",
        "            r = j // column\n",
        "            c = j - r * column\n",
        "            img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
        "            if img.shape[:2] != (res, res):\n",
        "                img = cv2.resize(img, (res, res))\n",
        "            start_h, start_w = (r + 1) * gap + r * res, (c + 1) * gap + c * res\n",
        "            merge[start_h: start_h + res, start_w: start_w + res] = img\n",
        "        cv2.imwrite(str(name), merge)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzAz39HJuJGy",
        "cellView": "form"
      },
      "source": [
        "#@title metrics.py (removing lpips import)\n",
        "%%writefile /content/pytorchloss/metrics.py\n",
        "#https://github.com/huster-wgm/Pytorch-metrics/blob/master/metrics.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\"\"\"\n",
        "  @Email:  guangmingwu2010@gmail.com \\\n",
        "           guozhilingty@gmail.com\n",
        "  @Copyright: go-hiroaki & Chokurei\n",
        "  @License: MIT\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import lpips\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "def _binarize(y_data, threshold):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : [float] 4-d tensor in [batch_size, channels, img_rows, img_cols]\n",
        "        threshold : [float] [0.0, 1.0]\n",
        "    return 4-d binarized y_data\n",
        "    \"\"\"\n",
        "    y_data[y_data < threshold] = 0.0\n",
        "    y_data[y_data >= threshold] = 1.0\n",
        "    return y_data\n",
        "\n",
        "def _argmax(y_data, dim):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : 4-d tensor in [batch_size, chs, img_rows, img_cols]\n",
        "        dim : int\n",
        "    return 3-d [int] y_data\n",
        "    \"\"\"\n",
        "    return torch.argmax(y_data, dim).int()\n",
        "\n",
        "\n",
        "def _get_tp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "        y_pred : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_positive\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_fp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_positive\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_tn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_negative\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_negative\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_weights(y_true, nb_ch):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        nb_ch : int\n",
        "    return [float] weights\n",
        "    \"\"\"\n",
        "    batch_size, img_rows, img_cols = y_true.shape\n",
        "    pixels = batch_size * img_rows * img_cols\n",
        "    weights = [torch.sum(y_true==ch).item() / pixels for ch in range(nb_ch)]\n",
        "    return weights\n",
        "\n",
        "\n",
        "class CFMatrix(object):\n",
        "    def __init__(self, des=None):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ConfusionMatrix\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return confusion matrix\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 4).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch), :] = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class OAAcc(object):\n",
        "    def __init__(self, des=\"Overall Accuracy\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"OAcc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (tp+tn)/total\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "\n",
        "        nb_tp_tn = torch.sum(y_true == y_pred).float()\n",
        "        mperforms = nb_tp_tn / (batch_size * img_rows * img_cols)\n",
        "        performs = None\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Precision(object):\n",
        "    def __init__(self, des=\"Precision\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Prec\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fp)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Recall(object):\n",
        "    def __init__(self, des=\"Recall\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Reca\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fn)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class F1Score(object):\n",
        "    def __init__(self, des=\"F1Score\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"F1Sc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return 2*precision*recall/(precision+recall)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = 2 * _precision * _recall / (_precision + _recall + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "                _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "                performs[int(ch)] = 2 * _precision * \\\n",
        "                    _recall / (_precision + _recall + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Kappa(object):\n",
        "    def __init__(self, des=\"Kappa\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Kapp\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (Po-Pe)/(1-Pe)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "            Po = (nb_tp + nb_tn) / nb_total\n",
        "            Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn) +\n",
        "                  (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "            mperforms = (Po - Pe) / (1 - Pe + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "                Po = (nb_tp + nb_tn) / nb_total\n",
        "                Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn)\n",
        "                      + (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "                performs[int(ch)] = (Po - Pe) / (1 - Pe + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Jaccard(object):\n",
        "    def __init__(self, des=\"Jaccard\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Jacc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return intersection / (sum-intersection)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            _intersec = torch.sum(y_true * y_pred).float()\n",
        "            _sum = torch.sum(y_true + y_pred).float()\n",
        "            mperforms = _intersec / (_sum - _intersec + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                _intersec = torch.sum(y_true_ch * y_pred_ch).float()\n",
        "                _sum = torch.sum(y_true_ch + y_pred_ch).float()\n",
        "                performs[int(ch)] = _intersec / (_sum - _intersec + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class MSE(object):\n",
        "    def __init__(self, des=\"Mean Square Error\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return mean_squared_error, smaller the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "\n",
        "class PSNR(object):\n",
        "    def __init__(self, des=\"Peak Signal to Noise Ratio\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"PSNR\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return PSNR, larger the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        mse = torch.mean((y_pred - y_true) ** 2)\n",
        "        return 10 * torch.log10(1 / mse)\n",
        "\n",
        "\n",
        "class SSIM(object):\n",
        "    '''\n",
        "    modified from https://github.com/jorge-pessoa/pytorch-msssim\n",
        "    '''\n",
        "    def __init__(self, des=\"structural similarity index\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SSIM\"\n",
        "\n",
        "    def gaussian(self, w_size, sigma):\n",
        "        gauss = torch.Tensor([math.exp(-(x - w_size//2)**2/float(2*sigma**2)) for x in range(w_size)])\n",
        "        return gauss/gauss.sum()\n",
        "\n",
        "    def create_window(self, w_size, channel=1):\n",
        "        _1D_window = self.gaussian(w_size, 1.5).unsqueeze(1)\n",
        "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "        window = _2D_window.expand(channel, 1, w_size, w_size).contiguous()\n",
        "        return window\n",
        "\n",
        "    def __call__(self, y_pred, y_true, w_size=11, size_average=True, full=False):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            w_size : int, default 11\n",
        "            size_average : boolean, default True\n",
        "            full : boolean, default False\n",
        "        return ssim, larger the better\n",
        "        \"\"\"\n",
        "        # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "        if torch.max(y_pred) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(y_pred) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "\n",
        "        padd = 0\n",
        "        (_, channel, height, width) = y_pred.size()\n",
        "        window = self.create_window(w_size, channel=channel).to(y_pred.device)\n",
        "\n",
        "        mu1 = F.conv2d(y_pred, window, padding=padd, groups=channel)\n",
        "        mu2 = F.conv2d(y_true, window, padding=padd, groups=channel)\n",
        "\n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "\n",
        "        sigma1_sq = F.conv2d(y_pred * y_pred, window, padding=padd, groups=channel) - mu1_sq\n",
        "        sigma2_sq = F.conv2d(y_true * y_true, window, padding=padd, groups=channel) - mu2_sq\n",
        "        sigma12 = F.conv2d(y_pred * y_true, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "        C1 = (0.01 * L) ** 2\n",
        "        C2 = (0.03 * L) ** 2\n",
        "\n",
        "        v1 = 2.0 * sigma12 + C2\n",
        "        v2 = sigma1_sq + sigma2_sq + C2\n",
        "        cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "        ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "        if size_average:\n",
        "            ret = ssim_map.mean()\n",
        "        else:\n",
        "            ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "        if full:\n",
        "            return ret, cs\n",
        "        return ret\n",
        "\n",
        "\n",
        "class LPIPS(object):\n",
        "    '''\n",
        "    borrowed from https://github.com/richzhang/PerceptualSimilarity\n",
        "    '''\n",
        "    def __init__(self, cuda, des=\"Learned Perceptual Image Patch Similarity\", version=\"0.1\"):\n",
        "        self.des = des\n",
        "        self.version = version\n",
        "        self.model = lpips.PerceptualLoss(model='net-lin',net='alex',use_gpu=cuda)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LPIPS\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, normalized=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            normalized : change [0,1] => [-1,1] (default by LPIPS)\n",
        "        return LPIPS, smaller the better\n",
        "        \"\"\"\n",
        "        if normalized:\n",
        "            y_pred = y_pred * 2.0 - 1.0\n",
        "            y_true = y_true * 2.0 - 1.0\n",
        "        return self.model.forward(y_pred, y_true)\n",
        "\n",
        "\n",
        "class AE(object):\n",
        "    \"\"\"\n",
        "    Modified from matlab : colorangle.m, MATLAB V2019b\n",
        "    angle = acos(RGB1' * RGB2 / (norm(RGB1) * norm(RGB2)));\n",
        "    angle = 180 / pi * angle;\n",
        "    \"\"\"\n",
        "    def __init__(self, des='average Angular Error'):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "        return average AE, smaller the better\n",
        "        \"\"\"\n",
        "        dotP = torch.sum(y_pred * y_true, dim=1)\n",
        "        Norm_pred = torch.sqrt(torch.sum(y_pred * y_pred, dim=1))\n",
        "        Norm_true = torch.sqrt(torch.sum(y_true * y_true, dim=1))\n",
        "        ae = 180 / math.pi * torch.acos(dotP / (Norm_pred * Norm_true + eps))\n",
        "        return ae.mean(1).mean(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for ch in [3, 1]:\n",
        "        batch_size, img_row, img_col = 1, 224, 224\n",
        "        y_true = torch.rand(batch_size, ch, img_row, img_col)\n",
        "        noise = torch.zeros(y_true.size()).data.normal_(0, std=0.1)\n",
        "        y_pred = y_true + noise\n",
        "        for cuda in [False, True]:\n",
        "            if cuda:\n",
        "                y_pred = y_pred.cuda()\n",
        "                y_true = y_true.cuda()\n",
        "\n",
        "            print('#'*20, 'Cuda : {} ; size : {}'.format(cuda, y_true.size()))\n",
        "            ########### similarity metrics\n",
        "            metric = MSE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = PSNR()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = SSIM()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = LPIPS(cuda)\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = AE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            ########### accuracy metrics\n",
        "            metric = OAAcc()\n",
        "            maccu, accu = metric(y_pred, y_true)\n",
        "            print('mAccu:', maccu, 'Accu', accu)\n",
        "\n",
        "            metric = Precision()\n",
        "            mprec, prec = metric(y_pred, y_true)\n",
        "            print('mPrec:', mprec, 'Prec', prec)\n",
        "\n",
        "            metric = Recall()\n",
        "            mreca, reca = metric(y_pred, y_true)\n",
        "            print('mReca:', mreca, 'Reca', reca)\n",
        "\n",
        "            metric = F1Score()\n",
        "            mf1sc, f1sc = metric(y_pred, y_true)\n",
        "            print('mF1sc:', mf1sc, 'F1sc', f1sc)\n",
        "\n",
        "            metric = Kappa()\n",
        "            mkapp, kapp = metric(y_pred, y_true)\n",
        "            print('mKapp:', mkapp, 'Kapp', kapp)\n",
        "\n",
        "            metric = Jaccard()\n",
        "            mjacc, jacc = metric(y_pred, y_true)\n",
        "            print('mJacc:', mjacc, 'Jacc', jacc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bwPuRzjW40R",
        "cellView": "form"
      },
      "source": [
        "#@title init.py\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    print('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbdN2xrNLe1B",
        "cellView": "form"
      },
      "source": [
        "#@title block.py\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from models.modules.architectures.convolutions.partialconv2d import PartialConv2d #TODO\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "#from models.networks import weights_init_normal, weights_init_xavier, weights_init_kaiming, weights_init_orthogonal\n",
        "\n",
        "\n",
        "####################\n",
        "# Basic blocks\n",
        "####################\n",
        "\n",
        "# Swish activation funtion\n",
        "def swish_func(x, beta=1.0):\n",
        "    \"\"\"\n",
        "    \"Swish: a Self-Gated Activation Function\"\n",
        "    Searching for Activation Functions (https://arxiv.org/abs/1710.05941)\n",
        "    \n",
        "    If beta=1 applies the Sigmoid Linear Unit (SiLU) function element-wise\n",
        "    If beta=0, Swish becomes the scaled linear function (identity \n",
        "      activation) f(x) = x/2\n",
        "    As beta -> ∞, the sigmoid component converges to approach a 0-1 function\n",
        "      (unit step), and multiplying that by x gives us f(x)=2max(0,x), which \n",
        "      is the ReLU multiplied by a constant factor of 2, so Swish becomes like \n",
        "      the ReLU function.\n",
        "    \n",
        "    Including beta, Swish can be loosely viewed as a smooth function that \n",
        "      nonlinearly interpolate between identity (linear) and ReLU function.\n",
        "      The degree of interpolation can be controlled by the model if beta is \n",
        "      set as a trainable parameter.\n",
        "    \n",
        "    Alt: 1.78718727865 * (x * sigmoid(x) - 0.20662096414)\n",
        "    \"\"\"\n",
        "    \n",
        "    # In-place implementation, may consume less GPU memory: \n",
        "    \"\"\" \n",
        "    result = x.clone()\n",
        "    torch.sigmoid_(beta*x)\n",
        "    x *= result\n",
        "    return x\n",
        "    #\"\"\"\n",
        "    \n",
        "    # Normal out-of-place implementation:\n",
        "    #\"\"\"\n",
        "    return x * torch.sigmoid(beta * x)\n",
        "    #\"\"\"\n",
        "    \n",
        "# Swish module\n",
        "class Swish(nn.Module):\n",
        "    \n",
        "    __constants__ = ['beta', 'slope', 'inplace']\n",
        "    \n",
        "    def __init__(self, beta=1.0, slope=1.67653251702, inplace=False):\n",
        "        \"\"\"\n",
        "        Shape:\n",
        "        - Input: (N, *) where * means, any number of additional\n",
        "          dimensions\n",
        "        - Output: (N, *), same shape as the input\n",
        "        \"\"\"\n",
        "        super(Swish).__init__()\n",
        "        self.inplace = inplace\n",
        "        # self.beta = beta # user-defined beta parameter, non-trainable\n",
        "        # self.beta = beta * torch.nn.Parameter(torch.ones(1)) # learnable beta parameter, create a tensor out of beta\n",
        "        self.beta = torch.nn.Parameter(torch.tensor(beta)) # learnable beta parameter, create a tensor out of beta\n",
        "        self.beta.requiresGrad = True # set requiresGrad to true to make it trainable\n",
        "\n",
        "        self.slope = slope / 2 # user-defined \"slope\", non-trainable\n",
        "        # self.slope = slope * torch.nn.Parameter(torch.ones(1)) # learnable slope parameter, create a tensor out of slope\n",
        "        # self.slope = torch.nn.Parameter(torch.tensor(slope)) # learnable slope parameter, create a tensor out of slope\n",
        "        # self.slope.requiresGrad = True # set requiresGrad to true to true to make it trainable\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        # Disabled, using inplace causes:\n",
        "        # \"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"\n",
        "        if self.inplace:\n",
        "            input.mul_(torch.sigmoid(self.beta*input))\n",
        "            return 2 * self.slope * input\n",
        "        else:\n",
        "            return 2 * self.slope * swish_func(input, self.beta)\n",
        "        \"\"\"\n",
        "        return 2 * self.slope * swish_func(input, self.beta)\n",
        "\n",
        "\n",
        "def act(act_type, inplace=True, neg_slope=0.2, n_prelu=1, beta=1.0):\n",
        "    # helper selecting activation\n",
        "    # neg_slope: for leakyrelu and init of prelu\n",
        "    # n_prelu: for p_relu num_parameters\n",
        "    # beta: for swish\n",
        "    act_type = act_type.lower()\n",
        "    if act_type == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act_type == 'leakyrelu' or act_type == 'lrelu':\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act_type == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    elif act_type == 'Tanh' or act_type == 'tanh':  # [-1, 1] range output\n",
        "        layer = nn.Tanh()\n",
        "    elif act_type == 'sigmoid':  # [0, 1] range output\n",
        "        layer = nn.Sigmoid()\n",
        "    elif act_type == 'swish':\n",
        "        layer = Swish(beta=beta, inplace=inplace)\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, *kwargs):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x, *kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "def norm(norm_type, nc):\n",
        "    \"\"\"Return a normalization layer\n",
        "    Parameters:\n",
        "        norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
        "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
        "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
        "    \"\"\"\n",
        "    norm_type = norm_type.lower()\n",
        "    if norm_type == 'batch':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "        # norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "    elif norm_type == 'instance':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "        # norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
        "    # elif norm_type == 'layer':\n",
        "    #     return lambda num_features: nn.GroupNorm(1, num_features)\n",
        "    elif norm_type == 'none':\n",
        "        def norm_layer(x): return Identity()\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def add_spectral_norm(module, use_spectral_norm=False):\n",
        "    \"\"\" Add spectral norm to any module passed if use_spectral_norm = True,\n",
        "    else, returns the original module without change\n",
        "    \"\"\"\n",
        "    if use_spectral_norm:\n",
        "        return nn.utils.spectral_norm(module)\n",
        "    return module\n",
        "\n",
        "\n",
        "def pad(pad_type, padding):\n",
        "    \"\"\"\n",
        "    helper selecting padding layer\n",
        "    if padding is 'zero', can be done with conv layers\n",
        "    \"\"\"\n",
        "    pad_type = pad_type.lower()\n",
        "    if padding == 0:\n",
        "        return None\n",
        "    if pad_type == 'reflect':\n",
        "        layer = nn.ReflectionPad2d(padding)\n",
        "    elif pad_type == 'replicate':\n",
        "        layer = nn.ReplicationPad2d(padding)\n",
        "    elif pad_type == 'zero':\n",
        "        layer = nn.ZeroPad2d(padding)\n",
        "    else:\n",
        "        raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def get_valid_padding(kernel_size, dilation):\n",
        "    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    return padding\n",
        "\n",
        "\n",
        "class ConcatBlock(nn.Module):\n",
        "    # Concat the output of a submodule to its input\n",
        "    def __init__(self, submodule):\n",
        "        super(ConcatBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.cat((x, self.sub(x)), dim=1)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Identity .. \\n|' + self.sub.__repr__().replace('\\n', '\\n|')\n",
        "\n",
        "\n",
        "class ShortcutBlock(nn.Module):\n",
        "    # Elementwise sum the output of a submodule to its input\n",
        "    def __init__(self, submodule):\n",
        "        super(ShortcutBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x + self.sub(x)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Identity + \\n|' + self.sub.__repr__().replace('\\n', '\\n|')\n",
        "\n",
        "\n",
        "def sequential(*args):\n",
        "    # Flatten Sequential. It unwraps nn.Sequential.\n",
        "    if len(args) == 1:\n",
        "        if isinstance(args[0], OrderedDict):\n",
        "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "        return args[0]  # No sequential is needed.\n",
        "    modules = []\n",
        "    for module in args:\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            for submodule in module.children():\n",
        "                modules.append(submodule)\n",
        "        elif isinstance(module, nn.Module):\n",
        "            modules.append(module)\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True, \\\n",
        "               pad_type='zero', norm_type=None, act_type='relu', mode='CNA', convtype='Conv2D', \\\n",
        "               spectral_norm=False):\n",
        "    \"\"\"\n",
        "    Conv layer with padding, normalization, activation\n",
        "    mode: CNA --> Conv -> Norm -> Act\n",
        "        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n",
        "    \"\"\"\n",
        "    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wrong conv mode [{:s}]'.format(mode)\n",
        "    padding = get_valid_padding(kernel_size, dilation)\n",
        "    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
        "    padding = padding if pad_type == 'zero' else 0\n",
        "    \n",
        "    if convtype=='PartialConv2D':\n",
        "        c = PartialConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    elif convtype=='DeformConv2D':\n",
        "        c = DeformConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    elif convtype=='Conv3D':\n",
        "        c = nn.Conv3d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "                dilation=dilation, bias=bias, groups=groups)\n",
        "    else: #default case is standard 'Conv2D':\n",
        "        c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "                dilation=dilation, bias=bias, groups=groups) #normal conv2d\n",
        "            \n",
        "    if spectral_norm:\n",
        "        c = nn.utils.spectral_norm(c)\n",
        "    \n",
        "    a = act(act_type) if act_type else None\n",
        "    if 'CNA' in mode:\n",
        "        n = norm(norm_type, out_nc) if norm_type else None\n",
        "        return sequential(p, c, n, a)\n",
        "    elif mode == 'NAC':\n",
        "        if norm_type is None and act_type is not None:\n",
        "            a = act(act_type, inplace=False)\n",
        "            # Important!\n",
        "            # input----ReLU(inplace)----Conv--+----output\n",
        "            #        |________________________|\n",
        "            # inplace ReLU will modify the input, therefore wrong output\n",
        "        n = norm(norm_type, in_nc) if norm_type else None\n",
        "        return sequential(n, a, p, c)\n",
        "\n",
        "\n",
        "def make_layer(basic_block, num_basic_block, **kwarg):\n",
        "    \"\"\"Make layers by stacking the same blocks.\n",
        "    Args:\n",
        "        basic_block (nn.module): nn.module class for basic block. (block)\n",
        "        num_basic_block (int): number of blocks. (n_layers)\n",
        "    Returns:\n",
        "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for _ in range(num_basic_block):\n",
        "        layers.append(basic_block(**kwarg))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class Mean(nn.Module):\n",
        "  def __init__(self, dim: list, keepdim=False):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.keepdim = keepdim\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.mean(x, self.dim, self.keepdim)\n",
        "\n",
        "\n",
        "####################\n",
        "# initialize modules\n",
        "####################\n",
        "\n",
        "@torch.no_grad()\n",
        "def default_init_weights(module_list, init_type='kaiming', scale=1, bias_fill=0, **kwargs):\n",
        "    \"\"\"Initialize network weights.\n",
        "    Args:\n",
        "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "        init_type (str): the type of initialization in: 'normal', 'kaiming' \n",
        "            or 'orthogonal'\n",
        "        scale (float): Scale initialized weights, especially for residual\n",
        "            blocks. Default: 1. (for 'kaiming')\n",
        "        bias_fill (float): The value to fill bias. Default: 0\n",
        "        kwargs (dict): Other arguments for initialization function:\n",
        "            mean and/or std for 'normal'.\n",
        "            a and/or mode for 'kaiming'\n",
        "            gain for 'orthogonal' and xavier\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO\n",
        "    # logger.info('Initialization method [{:s}]'.format(init_type))\n",
        "    if not isinstance(module_list, list):\n",
        "        module_list = [module_list]\n",
        "    for module in module_list:\n",
        "        for m in module.modules():\n",
        "            if init_type == 'normal':\n",
        "                weights_init_normal(m, bias_fill=bias_fill, **kwargs)\n",
        "            if init_type == 'xavier':\n",
        "                weights_init_xavier(m, scale=scale, bias_fill=bias_fill, **kwargs)    \n",
        "            elif init_type == 'kaiming':\n",
        "                weights_init_kaiming(m, scale=scale, bias_fill=bias_fill, **kwargs)\n",
        "            elif init_type == 'orthogonal':\n",
        "                weights_init_orthogonal(m, bias_fill=bias_fill)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [{:s}] not implemented'.format(init_type))\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Upsampler\n",
        "####################\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    r\"\"\"Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\n",
        "\n",
        "    The input data is assumed to be of the form\n",
        "    `minibatch x channels x [optional depth] x [optional height] x width`.\n",
        "\n",
        "    Args:\n",
        "        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional):\n",
        "            output spatial sizes\n",
        "        scale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional):\n",
        "            multiplier for spatial size. Has to match input size if it is a tuple.\n",
        "        mode (str, optional): the upsampling algorithm: one of ``'nearest'``,\n",
        "            ``'linear'``, ``'bilinear'``, ``'bicubic'`` and ``'trilinear'``.\n",
        "            Default: ``'nearest'``\n",
        "        align_corners (bool, optional): if ``True``, the corner pixels of the input\n",
        "            and output tensors are aligned, and thus preserving the values at\n",
        "            those pixels. This only has effect when :attr:`mode` is\n",
        "            ``'linear'``, ``'bilinear'``, or ``'trilinear'``. Default: ``False``\n",
        "    \"\"\"\n",
        "    # To prevent warning: nn.Upsample is deprecated\n",
        "    # https://discuss.pytorch.org/t/which-function-is-better-for-upsampling-upsampling-or-interpolate/21811/8\n",
        "    # From: https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#Upsample\n",
        "    # Alternative: https://discuss.pytorch.org/t/using-nn-function-interpolate-inside-nn-sequential/23588/2?u=ptrblck\n",
        "    \n",
        "    def __init__(self, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "        super(Upsample, self).__init__()\n",
        "        if isinstance(scale_factor, tuple):\n",
        "            self.scale_factor = tuple(float(factor) for factor in scale_factor)\n",
        "        else:\n",
        "            self.scale_factor = float(scale_factor) if scale_factor else None\n",
        "        self.mode = mode\n",
        "        self.size = size\n",
        "        self.align_corners = align_corners\n",
        "        # self.interp = nn.functional.interpolate\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "        # return self.interp(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "    \n",
        "    def extra_repr(self):\n",
        "        if self.scale_factor is not None:\n",
        "            info = 'scale_factor=' + str(self.scale_factor)\n",
        "        else:\n",
        "            info = 'size=' + str(self.size)\n",
        "        info += ', mode=' + self.mode\n",
        "        return info\n",
        "\n",
        "def pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
        "                        pad_type='zero', norm_type=None, act_type='relu', convtype='Conv2D'):\n",
        "    \"\"\"\n",
        "    Pixel shuffle layer\n",
        "    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n",
        "    Neural Network, CVPR17)\n",
        "    \"\"\"\n",
        "    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias, \\\n",
        "                        pad_type=pad_type, norm_type=None, act_type=None, convtype=convtype)\n",
        "    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "\n",
        "    n = norm(norm_type, out_nc) if norm_type else None\n",
        "    a = act(act_type) if act_type else None\n",
        "    return sequential(conv, pixel_shuffle, n, a)\n",
        "\n",
        "def upconv_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
        "                pad_type='zero', norm_type=None, act_type='relu', mode='nearest', convtype='Conv2D'):\n",
        "    \"\"\"\n",
        "    Upconv layer described in https://distill.pub/2016/deconv-checkerboard/\n",
        "    Example to replace deconvolutions: \n",
        "        - from: nn.ConvTranspose2d(in_nc, out_nc, kernel_size=4, stride=2, padding=1)\n",
        "        - to: upconv_block(in_nc, out_nc,kernel_size=3, stride=1, act_type=None)\n",
        "    \"\"\"\n",
        "    # upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n",
        "    upscale_factor = (1, upscale_factor, upscale_factor) if convtype == 'Conv3D' else upscale_factor\n",
        "    upsample = Upsample(scale_factor=upscale_factor, mode=mode) #Updated to prevent the \"nn.Upsample is deprecated\" Warning\n",
        "    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias, \\\n",
        "                        pad_type=pad_type, norm_type=norm_type, act_type=act_type, convtype=convtype)\n",
        "    return sequential(upsample, conv)\n",
        "\n",
        "# PPON\n",
        "def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1):\n",
        "    padding = int((kernel_size - 1) / 2) * dilation\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=True, dilation=dilation, groups=groups)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# ESRGANplus\n",
        "####################\n",
        "\n",
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, sigma=0.1, is_relative_detach=False):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "        self.is_relative_detach = is_relative_detach\n",
        "        self.noise = torch.tensor(0, dtype=torch.float).to(torch.device('cuda'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.sigma != 0:\n",
        "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
        "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
        "            x = x + sampled_noise\n",
        "        return x \n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "# TODO: Not used:\n",
        "# https://github.com/github-pengge/PyTorch-progressive_growing_of_gans/blob/master/models/base_model.py\n",
        "class minibatch_std_concat_layer(nn.Module):\n",
        "    def __init__(self, averaging='all'):\n",
        "        super(minibatch_std_concat_layer, self).__init__()\n",
        "        self.averaging = averaging.lower()\n",
        "        if 'group' in self.averaging:\n",
        "            self.n = int(self.averaging[5:])\n",
        "        else:\n",
        "            assert self.averaging in ['all', 'flat', 'spatial', 'none', 'gpool'], 'Invalid averaging mode'%self.averaging\n",
        "        self.adjusted_std = lambda x, **kwargs: torch.sqrt(torch.mean((x - torch.mean(x, **kwargs)) ** 2, **kwargs) + 1e-8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = list(x.size())\n",
        "        target_shape = copy.deepcopy(shape)\n",
        "        vals = self.adjusted_std(x, dim=0, keepdim=True)\n",
        "        if self.averaging == 'all':\n",
        "            target_shape[1] = 1\n",
        "            vals = torch.mean(vals, dim=1, keepdim=True)\n",
        "        elif self.averaging == 'spatial':\n",
        "            if len(shape) == 4:\n",
        "                vals = mean(vals, axis=[2,3], keepdim=True)             # torch.mean(torch.mean(vals, 2, keepdim=True), 3, keepdim=True)\n",
        "        elif self.averaging == 'none':\n",
        "            target_shape = [target_shape[0]] + [s for s in target_shape[1:]]\n",
        "        elif self.averaging == 'gpool':\n",
        "            if len(shape) == 4:\n",
        "                vals = mean(x, [0,2,3], keepdim=True)                   # torch.mean(torch.mean(torch.mean(x, 2, keepdim=True), 3, keepdim=True), 0, keepdim=True)\n",
        "        elif self.averaging == 'flat':\n",
        "            target_shape[1] = 1\n",
        "            vals = torch.FloatTensor([self.adjusted_std(x)])\n",
        "        else:                                                           # self.averaging == 'group'\n",
        "            target_shape[1] = self.n\n",
        "            vals = vals.view(self.n, self.shape[1]/self.n, self.shape[2], self.shape[3])\n",
        "            vals = mean(vals, axis=0, keepdim=True).view(1, self.n, 1, 1)\n",
        "        vals = vals.expand(*target_shape)\n",
        "        return torch.cat([x, vals], 1)\n",
        "\n",
        "\n",
        "####################\n",
        "# Useful blocks\n",
        "####################\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    \"\"\" \n",
        "        Implementation of Self attention Block according to paper \n",
        "        'Self-Attention Generative Adversarial Networks' (https://arxiv.org/abs/1805.08318)\n",
        "        Flexible Self Attention (FSA) layer according to paper\n",
        "        Efficient Super Resolution For Large-Scale Images Using Attentional GAN (https://arxiv.org/pdf/1812.04821.pdf)\n",
        "          The FSA layer borrows the self attention layer from SAGAN, \n",
        "          and wraps it with a max-pooling layer to reduce the size \n",
        "          of the feature maps and enable large-size images to fit in memory.\n",
        "        Used in Generator and Discriminator Networks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, max_pool=False, poolsize = 4, spectral_norm=False, ret_attention=False): #in_dim = in_feature_maps\n",
        "        super(SelfAttentionBlock,self).__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        self.ret_attention = ret_attention\n",
        "        \n",
        "        if self.max_pool:\n",
        "            self.pooled = nn.MaxPool2d(kernel_size=self.poolsize, stride=self.poolsize) #kernel_size=4, stride=4\n",
        "            # Note: can test using strided convolutions instead of MaxPool2d! :\n",
        "            #upsample_block_num = int(math.log(scale_factor, 2))\n",
        "            #self.pooled = nn.Conv2d .... strided conv\n",
        "            # upsample_o = [UpconvBlock(in_channels=in_dim, out_channels=in_dim, upscale_factor=2, mode='bilinear', act_type='leakyrelu') for _ in range(upsample_block_num)]\n",
        "            ## upsample_o.append(nn.Conv2d(nf, in_nc, kernel_size=9, stride=1, padding=4))\n",
        "            ## self.upsample_o = nn.Sequential(*upsample_o)\n",
        "\n",
        "            # self.upsample_o = B.Upsample(scale_factor=self.poolsize, mode='bilinear', align_corners=False) \n",
        "            \n",
        "        self.conv_f = add_spectral_norm(\n",
        "            nn.Conv1d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1, padding = 0), \n",
        "            use_spectral_norm=spectral_norm) #query_conv \n",
        "        self.conv_g = add_spectral_norm(\n",
        "            nn.Conv1d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1, padding = 0), \n",
        "            use_spectral_norm=spectral_norm) #key_conv \n",
        "        self.conv_h = add_spectral_norm(\n",
        "            nn.Conv1d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1, padding = 0), \n",
        "            use_spectral_norm=spectral_norm) #value_conv \n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1)) # Trainable interpolation parameter\n",
        "        self.softmax  = nn.Softmax(dim = -1)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                input : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature \n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.max_pool: #Downscale with Max Pool\n",
        "            x = self.pooled(input)\n",
        "        else:\n",
        "            x = input\n",
        "            \n",
        "        batch_size, C, width, height = x.size()\n",
        "        \n",
        "        N = width * height\n",
        "        x = x.view(batch_size, -1, N)\n",
        "        f = self.conv_f(x) #proj_query  # B X CX(N)\n",
        "        g = self.conv_g(x) #proj_key    # B X C x (*W*H)\n",
        "        h = self.conv_h(x) #proj_value  # B X C X N\n",
        "\n",
        "        s = torch.bmm(f.permute(0, 2, 1), g) # energy, transpose check\n",
        "        # get probabilities\n",
        "        attention = self.softmax(s) #beta #attention # BX (N) X (N) \n",
        "        \n",
        "        out = torch.bmm(h, attention.permute(0,2,1))\n",
        "        out = out.view(batch_size, C, width, height) \n",
        "        \n",
        "        if self.max_pool: #Upscale to original size\n",
        "            # out = self.upsample_o(out)\n",
        "            out = Upsample(size=(input.shape[2],input.shape[3]), mode='bicubic', align_corners=False)(out) #bicubic (PyTorch > 1.0) | bilinear others.\n",
        "        \n",
        "        out = self.gamma*out + input #Add original input\n",
        "        \n",
        "        if self.ret_attention:\n",
        "            return out, attention\n",
        "        else:\n",
        "            return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVGp0I1Ind4T",
        "cellView": "form"
      },
      "source": [
        "#@title spectral_norm.py\n",
        "\"\"\"\n",
        "spectral_norm.py (12-2-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/spectral_norm.py\n",
        "\"\"\"\n",
        "'''\n",
        "Copy from pytorch github repo\n",
        "Spectral Normalization from https://arxiv.org/abs/1802.05957\n",
        "'''\n",
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class SpectralNorm(object):\n",
        "    def __init__(self, name='weight', n_power_iterations=1, dim=0, eps=1e-12):\n",
        "        self.name = name\n",
        "        self.dim = dim\n",
        "        if n_power_iterations <= 0:\n",
        "            raise ValueError('Expected n_power_iterations to be positive, but '\n",
        "                             'got n_power_iterations={}'.format(n_power_iterations))\n",
        "        self.n_power_iterations = n_power_iterations\n",
        "        self.eps = eps\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        u = getattr(module, self.name + '_u')\n",
        "        weight_mat = weight\n",
        "        if self.dim != 0:\n",
        "            # permute dim to front\n",
        "            weight_mat = weight_mat.permute(self.dim,\n",
        "                                            *[d for d in range(weight_mat.dim()) if d != self.dim])\n",
        "        height = weight_mat.size(0)\n",
        "        weight_mat = weight_mat.reshape(height, -1)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.n_power_iterations):\n",
        "                # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n",
        "                # are the first left and right singular vectors.\n",
        "                # This power iteration produces approximations of `u` and `v`.\n",
        "                v = normalize(torch.matmul(weight_mat.t(), u), dim=0, eps=self.eps)\n",
        "                u = normalize(torch.matmul(weight_mat, v), dim=0, eps=self.eps)\n",
        "\n",
        "        sigma = torch.dot(u, torch.matmul(weight_mat, v))\n",
        "        weight = weight / sigma\n",
        "        return weight, u\n",
        "\n",
        "    def remove(self, module):\n",
        "        weight = getattr(module, self.name)\n",
        "        delattr(module, self.name)\n",
        "        delattr(module, self.name + '_u')\n",
        "        delattr(module, self.name + '_orig')\n",
        "        module.register_parameter(self.name, torch.nn.Parameter(weight))\n",
        "\n",
        "    def __call__(self, module, inputs):\n",
        "        if module.training:\n",
        "            weight, u = self.compute_weight(module)\n",
        "            setattr(module, self.name, weight)\n",
        "            setattr(module, self.name + '_u', u)\n",
        "        else:\n",
        "            r_g = getattr(module, self.name + '_orig').requires_grad\n",
        "            getattr(module, self.name).detach_().requires_grad_(r_g)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(module, name, n_power_iterations, dim, eps):\n",
        "        fn = SpectralNorm(name, n_power_iterations, dim, eps)\n",
        "        weight = module._parameters[name]\n",
        "        height = weight.size(dim)\n",
        "\n",
        "        u = normalize(weight.new_empty(height).normal_(0, 1), dim=0, eps=fn.eps)\n",
        "        delattr(module, fn.name)\n",
        "        module.register_parameter(fn.name + \"_orig\", weight)\n",
        "        # We still need to assign weight back as fn.name because all sorts of\n",
        "        # things may assume that it exists, e.g., when initializing weights.\n",
        "        # However, we can't directly assign as it could be an nn.Parameter and\n",
        "        # gets added as a parameter. Instead, we register weight.data as a\n",
        "        # buffer, which will cause weight to be included in the state dict\n",
        "        # and also supports nn.init due to shared storage.\n",
        "        module.register_buffer(fn.name, weight.data)\n",
        "        module.register_buffer(fn.name + \"_u\", u)\n",
        "\n",
        "        module.register_forward_pre_hook(fn)\n",
        "        return fn\n",
        "\n",
        "\n",
        "def spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None):\n",
        "    r\"\"\"Applies spectral normalization to a parameter in the given module.\n",
        "\n",
        "    .. math::\n",
        "         \\mathbf{W} &= \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n",
        "         \\sigma(\\mathbf{W}) &= \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n",
        "\n",
        "    Spectral normalization stabilizes the training of discriminators (critics)\n",
        "    in Generaive Adversarial Networks (GANs) by rescaling the weight tensor\n",
        "    with spectral norm :math:`\\sigma` of the weight matrix calculated using\n",
        "    power iteration method. If the dimension of the weight tensor is greater\n",
        "    than 2, it is reshaped to 2D in power iteration method to get spectral\n",
        "    norm. This is implemented via a hook that calculates spectral norm and\n",
        "    rescales weight before every :meth:`~Module.forward` call.\n",
        "\n",
        "    See `Spectral Normalization for Generative Adversarial Networks`_ .\n",
        "\n",
        "    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): containing module\n",
        "        name (str, optional): name of weight parameter\n",
        "        n_power_iterations (int, optional): number of power iterations to\n",
        "            calculate spectal norm\n",
        "        eps (float, optional): epsilon for numerical stability in\n",
        "            calculating norms\n",
        "        dim (int, optional): dimension corresponding to number of outputs,\n",
        "            the default is 0, except for modules that are instances of\n",
        "            ConvTranspose1/2/3d, when it is 1\n",
        "\n",
        "    Returns:\n",
        "        The original module with the spectal norm hook\n",
        "\n",
        "    Example::\n",
        "\n",
        "        >>> m = spectral_norm(nn.Linear(20, 40))\n",
        "        Linear (20 -> 40)\n",
        "        >>> m.weight_u.size()\n",
        "        torch.Size([20])\n",
        "\n",
        "    \"\"\"\n",
        "    if dim is None:\n",
        "        if isinstance(\n",
        "                module,\n",
        "            (torch.nn.ConvTranspose1d, torch.nn.ConvTranspose2d, torch.nn.ConvTranspose3d)):\n",
        "            dim = 1\n",
        "        else:\n",
        "            dim = 0\n",
        "    SpectralNorm.apply(module, name, n_power_iterations, dim, eps)\n",
        "    return module\n",
        "\n",
        "\n",
        "def remove_spectral_norm(module, name='weight'):\n",
        "    r\"\"\"Removes the spectral normalization reparameterization from a module.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): containing module\n",
        "        name (str, optional): name of weight parameter\n",
        "\n",
        "    Example:\n",
        "        >>> m = spectral_norm(nn.Linear(40, 10))\n",
        "        >>> remove_spectral_norm(m)\n",
        "    \"\"\"\n",
        "    for k, hook in module._forward_pre_hooks.items():\n",
        "        if isinstance(hook, SpectralNorm) and hook.name == name:\n",
        "            hook.remove(module)\n",
        "            del module._forward_pre_hooks[k]\n",
        "            return module\n",
        "\n",
        "    raise ValueError(\"spectral_norm of '{}' not found in {}\".format(name, module))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNKbsu4VrQD4",
        "cellView": "form"
      },
      "source": [
        "#@title discriminator.py\n",
        "\"\"\"\n",
        "discriminators.py (12-2-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/architectures/discriminators.py\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "#from . import block as B\n",
        "from torch.nn.utils import spectral_norm as SN\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Discriminator\n",
        "####################\n",
        "\n",
        "\n",
        "# VGG style Discriminator\n",
        "class Discriminator_VGG(pl.LightningModule):\n",
        "    def __init__(self, size, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG, self).__init__()\n",
        "\n",
        "        conv_blocks = []\n",
        "        conv_blocks.append(conv_block(  in_nc, base_nf, kernel_size=3, stride=1, norm_type=None, \\\n",
        "            act_type=act_type, mode=mode))\n",
        "        conv_blocks.append(conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode))\n",
        "\n",
        "        cur_size = size // 2\n",
        "        cur_nc = base_nf\n",
        "        while cur_size > 4:\n",
        "            out_nc = cur_nc * 2 if cur_nc < 512 else cur_nc\n",
        "            conv_blocks.append(conv_block(cur_nc, out_nc, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode))\n",
        "            conv_blocks.append(conv_block(out_nc, out_nc, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode))\n",
        "            cur_nc = out_nc\n",
        "            cur_size //= 2\n",
        "\n",
        "        self.features = sequential(*conv_blocks)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 96*96\n",
        "class Discriminator_VGG_96(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_96, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 96, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 48, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 24, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 12, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 6, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 3, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VGG style Discriminator with input size 128*128, Spectral Normalization\n",
        "class Discriminator_VGG_128_SN(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(Discriminator_VGG_128_SN, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        self.conv0 = spectral_norm(nn.Conv2d(3, 64, 3, 1, 1))\n",
        "        self.conv1 = spectral_norm(nn.Conv2d(64, 64, 4, 2, 1))\n",
        "        # 64, 64\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(64, 128, 3, 1, 1))\n",
        "        self.conv3 = spectral_norm(nn.Conv2d(128, 128, 4, 2, 1))\n",
        "        # 32, 128\n",
        "        self.conv4 = spectral_norm(nn.Conv2d(128, 256, 3, 1, 1))\n",
        "        self.conv5 = spectral_norm(nn.Conv2d(256, 256, 4, 2, 1))\n",
        "        # 16, 256\n",
        "        self.conv6 = spectral_norm(nn.Conv2d(256, 512, 3, 1, 1))\n",
        "        self.conv7 = spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n",
        "        # 8, 512\n",
        "        self.conv8 = spectral_norm(nn.Conv2d(512, 512, 3, 1, 1))\n",
        "        self.conv9 = spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n",
        "        # 4, 512\n",
        "\n",
        "        # classifier\n",
        "        self.linear0 = spectral_norm(nn.Linear(512 * 4 * 4, 100))\n",
        "        self.linear1 = spectral_norm(nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.conv0(x))\n",
        "        x = self.lrelu(self.conv1(x))\n",
        "        x = self.lrelu(self.conv2(x))\n",
        "        x = self.lrelu(self.conv3(x))\n",
        "        x = self.lrelu(self.conv4(x))\n",
        "        x = self.lrelu(self.conv5(x))\n",
        "        x = self.lrelu(self.conv6(x))\n",
        "        x = self.lrelu(self.conv7(x))\n",
        "        x = self.lrelu(self.conv8(x))\n",
        "        x = self.lrelu(self.conv9(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lrelu(self.linear0(x))\n",
        "        x = self.linear1(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 128*128\n",
        "class Discriminator_VGG_128(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_128, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 64, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 32, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 16, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 8, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 4, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VGG style Discriminator with input size 192*192\n",
        "class Discriminator_VGG_192(pl.LightningModule): #vic in PPON is called Discriminator_192 \n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_192, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 192, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode) # 3-->64\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 64-->64, 96*96\n",
        "        # 96, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 64-->128\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 128-->128, 48*48\n",
        "        # 48, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 128-->256\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 256-->256, 24*24\n",
        "        # 24, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 256-->512\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512 12*12\n",
        "        # 12, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512 6*6\n",
        "        # 6, 512\n",
        "        conv10 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv11 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 3*3\n",
        "        # 3, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9, conv10, conv11)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1)) #vic PPON uses 128 and 128 instead of 100\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 256*256\n",
        "class Discriminator_VGG_256(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_256, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 256, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 128, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 64, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 32, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 16, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 8, 512\n",
        "        conv10 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv11 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 3*3\n",
        "        # 4, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9, conv10, conv11)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "####################\n",
        "# Perceptual Network\n",
        "####################\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class VGGFeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 feature_layer=34,\n",
        "                 use_bn=False,\n",
        "                 use_input_norm=True,\n",
        "                 device=torch.device('cpu'),\n",
        "                 z_norm=False): #Note: PPON uses cuda instead of CPU\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "        if use_bn:\n",
        "            model = torchvision.models.vgg19_bn(pretrained=True)\n",
        "        else:\n",
        "            model = torchvision.models.vgg19(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device) \n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)                 \n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class ResNet101FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self, use_input_norm=True, device=torch.device('cpu'), z_norm=False):\n",
        "        super(ResNet101FeatureExtractor, self).__init__()\n",
        "        model = torchvision.models.resnet101(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.children())[:8])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MINCNet(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(MINCNet, self).__init__()\n",
        "        self.ReLU = nn.ReLU(True)\n",
        "        self.conv11 = nn.Conv2d(3, 64, 3, 1, 1)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3, 1, 1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3, 1, 1)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv31 = nn.Conv2d(128, 256, 3, 1, 1)\n",
        "        self.conv32 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.conv33 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.maxpool3 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv41 = nn.Conv2d(256, 512, 3, 1, 1)\n",
        "        self.conv42 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv43 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.maxpool4 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv51 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv52 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv53 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ReLU(self.conv11(x))\n",
        "        out = self.ReLU(self.conv12(out))\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.ReLU(self.conv21(out))\n",
        "        out = self.ReLU(self.conv22(out))\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.ReLU(self.conv31(out))\n",
        "        out = self.ReLU(self.conv32(out))\n",
        "        out = self.ReLU(self.conv33(out))\n",
        "        out = self.maxpool3(out)\n",
        "        out = self.ReLU(self.conv41(out))\n",
        "        out = self.ReLU(self.conv42(out))\n",
        "        out = self.ReLU(self.conv43(out))\n",
        "        out = self.maxpool4(out)\n",
        "        out = self.ReLU(self.conv51(out))\n",
        "        out = self.ReLU(self.conv52(out))\n",
        "        out = self.conv53(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class MINCFeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, \\\n",
        "                device=torch.device('cpu')):\n",
        "        super(MINCFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.features = MINCNet()\n",
        "        self.features.load_state_dict(\n",
        "            torch.load('../experiments/pretrained_models/VGG16minc_53.pth'), strict=True)\n",
        "        self.features.eval()\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "#TODO\n",
        "# moved from models.modules.architectures.ASRResNet_arch, did not bring the self-attention layer\n",
        "# VGG style Discriminator with input size 128*128, with feature_maps extraction and self-attention\n",
        "class Discriminator_VGG_128_fea(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "         arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4):\n",
        "        super(Discriminator_VGG_128_fea, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        \n",
        "        # Self-Attention configuration\n",
        "        '''#TODO\n",
        "        self.self_attention = self_attention\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        '''\n",
        "        \n",
        "        # Remove BatchNorm2d if using spectral_norm\n",
        "        if spectral_norm:\n",
        "            norm_type = None\n",
        "        \n",
        "        self.conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        self.conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 64, 64\n",
        "        self.conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 32, 128\n",
        "        self.conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 16, 256\n",
        "        \n",
        "        '''#TODO\n",
        "        if self.self_attention:\n",
        "            self.FSA = SelfAttentionBlock(in_dim = base_nf*4, max_pool=self.max_pool, poolsize = self.poolsize, spectral_norm=spectral_norm)\n",
        "        '''\n",
        "\n",
        "        self.conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 8, 512\n",
        "        self.conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 4, 512\n",
        "        # self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            # conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    #TODO: modify to a listening dictionary like VGG_Model(), can select what maps to use\n",
        "    def forward(self, x, return_maps=False):\n",
        "        feature_maps = []\n",
        "        # x = self.features(x)\n",
        "        x = self.conv0(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv1(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv2(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv3(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv4(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv5(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv6(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv7(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv8(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv9(x)\n",
        "        feature_maps.append(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        if return_maps:\n",
        "            return [x, feature_maps]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator_VGG_fea(pl.LightningModule):\n",
        "    def __init__(self, size, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "         arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4):\n",
        "        super(Discriminator_VGG_fea, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        \n",
        "        # Self-Attention configuration\n",
        "        '''#TODO\n",
        "        self.self_attention = self_attention\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        '''\n",
        "        \n",
        "        # Remove BatchNorm2d if using spectral_norm\n",
        "        if spectral_norm:\n",
        "            norm_type = None\n",
        "\n",
        "        self.conv_blocks = []\n",
        "        self.conv_blocks.append(conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "        self.conv_blocks.append(conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "\n",
        "        cur_size = size // 2\n",
        "        cur_nc = base_nf\n",
        "        while cur_size > 4:\n",
        "            out_nc = cur_nc * 2 if cur_nc < 512 else cur_nc\n",
        "            self.conv_blocks.append(conv_block(cur_nc, out_nc, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "            self.conv_blocks.append(conv_block(out_nc, out_nc, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "            cur_nc = out_nc\n",
        "            cur_size //= 2\n",
        "        \n",
        "        '''#TODO\n",
        "        if self.self_attention:\n",
        "            self.FSA = SelfAttentionBlock(in_dim = base_nf*4, max_pool=self.max_pool, poolsize = self.poolsize, spectral_norm=spectral_norm)\n",
        "        '''\n",
        "\n",
        "        # self.features = sequential(*conv_blocks)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    #TODO: modify to a listening dictionary like VGG_Model(), can select what maps to use\n",
        "    def forward(self, x, return_maps=False):\n",
        "        feature_maps = []\n",
        "        # x = self.features(x)\n",
        "        for conv in self.conv_blocks:\n",
        "            # Fixes incorrect device error\n",
        "            device = x.device\n",
        "            conv = conv.to(device)\n",
        "            x = conv(x)\n",
        "            feature_maps.append(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        if return_maps:\n",
        "            return [x, feature_maps]\n",
        "        return x\n",
        "\n",
        "\n",
        "class NLayerDiscriminator(pl.LightningModule):\n",
        "    r\"\"\"\n",
        "    PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1611.07004v3.pdf\n",
        "    https://arxiv.org/pdf/1803.07422.pdf\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "        use_sigmoid=False, getIntermFeat=False, patch=True, use_spectral_norm=False):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int): the number of channels in input images\n",
        "            ndf (int): the number of filters in the last conv layer\n",
        "            n_layers (int): the number of conv layers in the discriminator\n",
        "            norm_layer (nn.Module): normalization layer (if not using Spectral Norm)\n",
        "            patch (bool): Select between an patch or a linear output\n",
        "            use_spectral_norm (bool): Select if Spectral Norm will be used\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "\n",
        "        if use_spectral_norm:\n",
        "            # disable Instance or Batch Norm if using Spectral Norm\n",
        "            norm_layer = Identity\n",
        "\n",
        "        #self.getIntermFeat = getIntermFeat # not used for now\n",
        "        #use_sigmoid not used for now\n",
        "        #TODO: test if there are benefits by incorporating the use of intermediate features from pix2pixHD\n",
        "\n",
        "        use_bias = False\n",
        "        kw = 4\n",
        "        padw = 1 # int(np.ceil((kw-1.0)/2))\n",
        "\n",
        "        sequence = [add_spectral_norm(\n",
        "                        nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), \n",
        "                        use_spectral_norm), \n",
        "                    nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                add_spectral_norm(\n",
        "                    nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias), \n",
        "                    use_spectral_norm),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            add_spectral_norm(\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias), \n",
        "                use_spectral_norm),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        if patch:\n",
        "            # output patches as results\n",
        "            sequence += [add_spectral_norm(\n",
        "                nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw), \n",
        "                use_spectral_norm)]  # output 1 channel prediction map\n",
        "        else:\n",
        "            # linear vector classification output\n",
        "            sequence += [Mean([1, 2]), nn.Linear(ndf * nf_mult, 1)]\n",
        "        \n",
        "        if use_sigmoid:\n",
        "            sequence += [nn.Sigmoid()]\n",
        "        \n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MultiscaleDiscriminator(pl.LightningModule):\n",
        "    r\"\"\"\n",
        "    Multiscale PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1711.11585.pdf\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n",
        "        \"\"\"Construct a pyramid of PatchGAN discriminators\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "            use_sigmoid     -- boolean to use sigmoid in patchGAN discriminators\n",
        "            num_D (int)     -- number of discriminators/downscales in the pyramid\n",
        "            getIntermFeat   -- boolean to get intermediate features (unused for now)\n",
        "        \"\"\"\n",
        "        super(MultiscaleDiscriminator, self).__init__()\n",
        "        self.num_D = num_D\n",
        "        self.n_layers = n_layers\n",
        "        self.getIntermFeat = getIntermFeat\n",
        "     \n",
        "        for i in range(num_D):\n",
        "            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
        "            if getIntermFeat:                                \n",
        "                for j in range(n_layers+2):\n",
        "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
        "            else:\n",
        "                setattr(self, 'layer'+str(i), netD.model)\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
        "\n",
        "    def singleD_forward(self, model, input):\n",
        "        if self.getIntermFeat:\n",
        "            result = [input]\n",
        "            for i in range(len(model)):\n",
        "                result.append(model[i](result[-1]))\n",
        "            return result[1:]\n",
        "        else:\n",
        "            return [model(input)]\n",
        "\n",
        "    def forward(self, input):        \n",
        "        num_D = self.num_D\n",
        "        result = []\n",
        "        input_downsampled = input\n",
        "        for i in range(num_D):\n",
        "            if self.getIntermFeat:\n",
        "                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n",
        "            else:\n",
        "                model = getattr(self, 'layer'+str(num_D-1-i))\n",
        "            result.append(self.singleD_forward(model, input_downsampled))\n",
        "            if i != (num_D-1):\n",
        "                input_downsampled = self.downsample(input_downsampled)\n",
        "        return result\n",
        "\n",
        "\n",
        "class PixelDiscriminator(pl.LightningModule):\n",
        "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        use_bias = False\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.net(input)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "models.py (21-12-20)\n",
        "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/context_encoder/models.py\n",
        "\"\"\"\n",
        "\n",
        "class context_encoder(pl.LightningModule):\n",
        "    def __init__(self, channels=3):\n",
        "        super(context_encoder, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = channels\n",
        "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "discriminators.py (12-2-20)\n",
        "https://github.com/JoeyBallentine/BasicSR/blob/resnet-discriminator/codes/models/modules/architectures/discriminators.py\n",
        "\"\"\"\n",
        "# Assume input range is [0, 1]\n",
        "class ResNet101FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self, use_input_norm=True, device=torch.device('cpu'), z_norm=False):\n",
        "        super(ResNet101FeatureExtractor, self).__init__()\n",
        "        model = torchvision.models.resnet101(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.children())[:8])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "# ResNet50 style Discriminator with input size 128*128\n",
        "class Discriminator_ResNet_128(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Structure based off of the ResNet50 configuration from this repository:\n",
        "    https://github.com/bentrevett/pytorch-image-classification\n",
        "    \"\"\"\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA'):\n",
        "        super(Discriminator_ResNet_128, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "\n",
        "        self.in_channels = base_nf\n",
        "        \n",
        "        # 128, 3\n",
        "        conv0 = conv_block(in_nc, self.in_channels, kernel_size=7, norm_type=norm_type, act_type=act_type, \\\n",
        "            mode=mode, stride=2, bias=False)\n",
        "        pool0 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # 32, 64\n",
        "\n",
        "        layer1 = self.get_resnet_layer(Bottleneck, 3, base_nf) # 32, 64\n",
        "        layer2 = self.get_resnet_layer(Bottleneck, 4, base_nf*2, stride = 2) # 16, 128\n",
        "        layer3 = self.get_resnet_layer(Bottleneck, 6, base_nf*4, stride = 2) # 8, 256\n",
        "        layer4 = self.get_resnet_layer(Bottleneck, 3, base_nf*8, stride = 2) # 4, 512\n",
        "\n",
        "        avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.features = sequential(conv0, pool0, layer1, layer2, layer3, layer4, avgpool)\n",
        "\n",
        "        self.classifier = nn.Linear(self.in_channels, 1)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n",
        "    \n",
        "        layers = []\n",
        "        \n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        \n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        \n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    \n",
        "    expansion = 4\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
        "                               stride = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
        "                               stride = stride, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n",
        "                               stride = 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, \n",
        "                             stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "            \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "                \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "            \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "    \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_-I2mZx9Ru"
      },
      "source": [
        "Inside ``CustomTrainClass`` it is possible to configure loss functions and weights. Configure logging path inside ``CustomTrainClass.py``. \n",
        "\n",
        "Warning: Don't use AMP with StyleLoss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKp21IihhGkL"
      },
      "source": [
        "**Warning**: Certain combinations of discriminator and generator can result in crappy validation images. Test for a short while and make sure it isn't a solid color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYAV2wG-rbLY",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "logdir='/content/'\n",
        "writer = SummaryWriter(logdir=logdir)\n",
        "\n",
        "from adamp import AdamP\n",
        "#from adamp import SGDP\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    ############################\n",
        "    # generators with one output, no AMP means nan loss during training\n",
        "    self.netG = RRDBNet(in_nc=3, out_nc=3, nf=128, nb=8, gc=32, upscale=4, norm_type=None,\n",
        "                act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D',\n",
        "                finalact=None, gaussian_noise=True, plus=False, \n",
        "                nr=3)\n",
        "\n",
        "    # DFNet\n",
        "    #self.netG = DFNet(c_img=3, c_mask=1, c_alpha=3,\n",
        "    #        mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "    #        en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3, 3, 3, 3, 3, 3, 3, 3],\n",
        "    #        blend_layers=[0, 1, 2, 3, 4, 5], conv_type='partial')\n",
        "    \n",
        "    # AdaFill\n",
        "    #self.netG = InpaintNet()\n",
        "\n",
        "    # MEDFE (batch_size: 1, no AMP)\n",
        "    #self.netG = MEDFEGenerator()\n",
        "\n",
        "    # RFR\n",
        "    # conv_type = partial or deform\n",
        "    # Warning: One testrun with deform resulted in Nan errors after ~60k iterations. It is also very slow. \n",
        "    # 'partial' is recommended, since this is what the official implementation does use.\n",
        "    #self.netG = RFRNet(conv_type='partial')\n",
        "\n",
        "    # LBAM\n",
        "    #self.netG = LBAMModel(inputChannels=4, outputChannels=3)\n",
        "\n",
        "    # DMFN\n",
        "    #self.netG = InpaintingGenerator(in_nc=4, out_nc=3,nf=64,n_res=8,\n",
        "    #      norm='in', activation='relu')\n",
        "\n",
        "    # partial\n",
        "    #self.netG = Model()\n",
        "\n",
        "    # RN\n",
        "    #self.netG = G_Net(input_channels=3, residual_blocks=8, threshold=0.8)\n",
        "    # using rn init to avoid errors\n",
        "    #RN_arch = rn_initialize_weights(self.netG, scale=0.1)\n",
        "\n",
        "\n",
        "    # DSNet\n",
        "    #self.netG = DSNet(layer_size=8, input_channels=3, upsampling_mode='nearest')\n",
        "\n",
        "\n",
        "    #DSNetRRDB\n",
        "    #self.netG = DSNetRRDB(layer_size=8, input_channels=3, upsampling_mode='nearest',\n",
        "    #            in_nc=4, out_nc=3, nf=128, nb=8, gc=32, upscale=1, norm_type=None,\n",
        "    #            act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D',\n",
        "    #            finalact=None, gaussian_noise=True, plus=False, \n",
        "    #            nr=3)\n",
        "\n",
        "\n",
        "    # DSNetDeoldify\n",
        "    #self.netG = DSNetDeoldify()\n",
        "\n",
        "    ############################\n",
        "\n",
        "    # generators with two outputs\n",
        "\n",
        "    # deepfillv1\n",
        "    #self.netG = InpaintSANet()\n",
        "\n",
        "    # deepfillv2\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = GatedGenerator(in_channels=4, out_channels=3, \n",
        "    #  latent_channels=64, pad_type='zero', activation='lrelu', norm='in', conv_type = 'partial')\n",
        "\n",
        "    # Adaptive\n",
        "    # [Warning] Adaptive does not like PatchGAN, Multiscale and ResNet.\n",
        "    #self.netG = PyramidNet(in_channels=3, residual_blocks=1, init_weights='True')\n",
        "\n",
        "    ############################\n",
        "    # exotic generators\n",
        "\n",
        "    # Pluralistic\n",
        "    #self.netG = PluralisticGenerator(ngf_E=opt_net['ngf_E'], z_nc_E=opt_net['z_nc_E'], img_f_E=opt_net['img_f_E'], layers_E=opt_net['layers_E'], norm_E=opt_net['norm_E'], activation_E=opt_net['activation_E'],\n",
        "    #            ngf_G=opt_net['ngf_G'], z_nc_G=opt_net['z_nc_G'], img_f_G=opt_net['img_f_G'], L_G=opt_net['L_G'], output_scale_G=opt_net['output_scale_G'], norm_G=opt_net['norm_G'], activation_G=opt_net['activation_G'])\n",
        "\n",
        "    \n",
        "    # EdgeConnect\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #self.netG = EdgeConnectModel(residual_blocks_edge=8,\n",
        "    #        residual_blocks_inpaint=8, use_spectral_norm=True,\n",
        "    #        conv_type_edge='normal', conv_type_inpaint='normal')\n",
        "\n",
        "    # FRRN\n",
        "    #self.netG = FRRNet()\n",
        "\n",
        "    # PRVS\n",
        "    #self.netG = PRVSNet()\n",
        "\n",
        "    # CSA\n",
        "    #self.netG = InpaintNet(c_img=3, norm='instance', act_en='leaky_relu', \n",
        "    #                           act_de='relu')\n",
        "\n",
        "    # deoldify\n",
        "    #self.netG = Unet34()\n",
        "\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "    ############################\n",
        "\n",
        "\n",
        "    # discriminators\n",
        "    # size refers to input shape of tensor\n",
        "\n",
        "    self.netD = context_encoder()\n",
        "\n",
        "    # VGG\n",
        "    #self.netD = Discriminator_VGG(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN')\n",
        "    #self.netD = Discriminator_VGG_fea(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D',\n",
        "    #     arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4)\n",
        "    #self.netD = Discriminator_VGG_128_SN()\n",
        "    #self.netD = VGGFeatureExtractor(feature_layer=34,use_bn=False,use_input_norm=True,device=torch.device('cpu'),z_norm=False)\n",
        "\n",
        "    # PatchGAN\n",
        "    #self.netD = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #    use_sigmoid=False, getIntermFeat=False, patch=True, use_spectral_norm=False)\n",
        "\n",
        "    # Multiscale\n",
        "    #self.netD = MultiscaleDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #             use_sigmoid=False, num_D=3, getIntermFeat=False)\n",
        "\n",
        "    # ResNet\n",
        "    #self.netD = Discriminator_ResNet_128(in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA')\n",
        "    #self.netD = ResNet101FeatureExtractor(use_input_norm=True, device=torch.device('cpu'), z_norm=False)\n",
        "    \n",
        "    # MINC\n",
        "    #self.netD = MINCNet()\n",
        "\n",
        "    # Pixel\n",
        "    #self.netD = PixelDiscriminator(input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d)\n",
        "\n",
        "    # EfficientNet\n",
        "    #from efficientnet_pytorch import EfficientNet\n",
        "    #self.netD = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    # ResNeSt\n",
        "    # [\"resnest50\", \"resnest101\", \"resnest200\", \"resnest269\"]\n",
        "    #self.netD = resnest50(pretrained=True)\n",
        "\n",
        "    # need fixing\n",
        "    #FileNotFoundError: [Errno 2] No such file or directory: '../experiments/pretrained_models/VGG16minc_53.pth'\n",
        "    #self.netD = MINCFeatureExtractor(feature_layer=34, use_bn=False, use_input_norm=True, device=torch.device('cpu'))\n",
        "\n",
        "    # Transformer (Warning: uses own init!)\n",
        "    #self.netD  = TranformerDiscriminator(img_size=256, patch_size=1, in_chans=3, num_classes=1, embed_dim=64, depth=7,\n",
        "    #             num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "    #             drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm)\n",
        "    \n",
        "\n",
        "    weights_init(self.netD, 'kaiming')\n",
        "\n",
        "\n",
        "    # loss functions\n",
        "    self.l1 = nn.L1Loss()\n",
        "    l_hfen_type = L1CosineSim()\n",
        "    self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "    self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "    self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "    self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "    self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "    self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "    self.OFLoss = OFLoss()\n",
        "    self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "    self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "    self.StyleLoss = StyleLoss()\n",
        "    self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "    self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "    layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "        distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "        use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "    self.MSELoss = torch.nn.MSELoss()\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "\n",
        "    # metrics\n",
        "    self.psnr_metric = PSNR()\n",
        "    self.ssim_metric = SSIM()\n",
        "    self.ae_metric = AE()\n",
        "    self.mse_metric = MSE()\n",
        "\n",
        "\n",
        "  def forward(self, image, masks):\n",
        "      return self.netG(image, masks)\n",
        "\n",
        "  #def adversarial_loss(self, y_hat, y):\n",
        "  #    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      # inpainting:\n",
        "      # train_batch[0][0] = batch_size\n",
        "      # train_batch[0] = masked\n",
        "      # train_batch[1] = mask\n",
        "      # train_batch[2] = original\n",
        "\n",
        "      # super resolution\n",
        "      # train_batch[0] = lr\n",
        "      # train_batch[1] = hr\n",
        "\n",
        "      # train generator\n",
        "      ############################\n",
        "      # generate fake (1 output)\n",
        "      #out = self(train_batch[0],train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # generate fake (2 outputs)\n",
        "      #out, other_img = self(train_batch[0],train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # exotic generators\n",
        "      # CSA\n",
        "      #coarse_result, out, csa, csa_d = self(train_batch[0],train_batch[1])\n",
        "      \n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #out, other_img = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "      \n",
        "      # PVRS\n",
        "      #out, _ ,edge_small, edge_big = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "      # FRRN\n",
        "      #out, mid_x, mid_mask = self(train_batch[0], train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      # deoldify\n",
        "      #out = self.netG(train_batch[0])\n",
        "\n",
        "\n",
        "      ############################\n",
        "      # ESRGAN\n",
        "      out = self.netG(train_batch[0])\n",
        "      \n",
        "\n",
        "\n",
        "      ############################\n",
        "      # loss calculation\n",
        "      total_loss = 0\n",
        "      \"\"\"\n",
        "      HFENLoss_forward = self.HFENLoss(out, train_batch[0])\n",
        "      total_loss += HFENLoss_forward\n",
        "      ElasticLoss_forward = self.ElasticLoss(out, train_batch[0])\n",
        "      total_loss += ElasticLoss_forward\n",
        "      RelativeL1_forward = self.RelativeL1(out, train_batch[0])\n",
        "      total_loss += RelativeL1_forward\n",
        "      \"\"\"\n",
        "      L1CosineSim_forward = 5*self.L1CosineSim(out, train_batch[1])\n",
        "      total_loss += L1CosineSim_forward\n",
        "      #self.log('loss/L1CosineSim', L1CosineSim_forward)\n",
        "      writer.add_scalar('loss/L1CosineSim', L1CosineSim_forward, self.trainer.global_step)\n",
        "\n",
        "      \"\"\"\n",
        "      ClipL1_forward = self.ClipL1(out, train_batch[0])\n",
        "      total_loss += ClipL1_forward\n",
        "      FFTloss_forward = self.FFTloss(out, train_batch[0])\n",
        "      total_loss += FFTloss_forward\n",
        "      OFLoss_forward = self.OFLoss(out)\n",
        "      total_loss += OFLoss_forward\n",
        "      GPLoss_forward = self.GPLoss(out, train_batch[0])\n",
        "      total_loss += GPLoss_forward\n",
        "      \n",
        "      CPLoss_forward = 0.1*self.CPLoss(out, train_batch[0])\n",
        "      total_loss += CPLoss_forward\n",
        "      \n",
        "\n",
        "      Contextual_Loss_forward = self.Contextual_Loss(out, train_batch[0])\n",
        "      total_loss += Contextual_Loss_forward\n",
        "      self.log('loss/contextual', Contextual_Loss_forward)\n",
        "      \"\"\"\n",
        "\n",
        "      #style_forward = 240*self.StyleLoss(out, train_batch[2])\n",
        "      #total_loss += style_forward\n",
        "      #self.log('loss/style', style_forward)\n",
        "\n",
        "      tv_forward = 0.0000005*self.TVLoss(out)\n",
        "      total_loss += tv_forward\n",
        "      #self.log('loss/tv', tv_forward)\n",
        "      writer.add_scalar('loss/tv', tv_forward, self.trainer.global_step)\n",
        "\n",
        "      perceptual_forward = 2*self.PerceptualLoss(out, train_batch[1])\n",
        "      total_loss += perceptual_forward\n",
        "      #self.log('loss/perceptual', perceptual_forward)\n",
        "      writer.add_scalar('loss/perceptual', perceptual_forward, self.trainer.global_step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #########################\n",
        "      # exotic loss\n",
        "\n",
        "      # if model has two output, also calculate loss for such an image\n",
        "      # example with just l1 loss\n",
        "      \n",
        "      #l1_stage1 = self.L1Loss(other_img, train_batch[0])\n",
        "      #self.log('loss/l1_stage1', l1_stage1)\n",
        "      #total_loss += l1_stage1\n",
        "\n",
        "\n",
        "      # CSA Loss\n",
        "      \"\"\"\n",
        "      recon_loss = self.L1Loss(coarse_result, train_batch[2]) + self.L1Loss(out, train_batch[2])\n",
        "      cons = ConsistencyLoss()\n",
        "      cons_loss = cons(csa, csa_d, train_batch[2], train_batch[1])\n",
        "      self.log('loss/recon_loss', recon_loss)\n",
        "      total_loss += recon_loss\n",
        "      self.log('loss/cons_loss', cons_loss)\n",
        "      total_loss += cons_loss\n",
        "      \"\"\"\n",
        "\n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #l1_edge = self.L1Loss(other_img, train_batch[3])\n",
        "      #self.log('loss/l1_edge', l1_edge)\n",
        "      #total_loss += l1_edge\n",
        "\n",
        "      # PVRS\n",
        "      \"\"\"\n",
        "      edge_big_l1 = self.L1Loss(edge_big, train_batch[3])\n",
        "      edge_small_l1 = self.L1Loss(edge_small, torch.nn.functional.interpolate(train_batch[3], scale_factor = 0.5))\n",
        "      self.log('loss/edge_big_l1', edge_big_l1)\n",
        "      total_loss += edge_big_l1\n",
        "      self.log('loss/edge_small_l1', edge_small_l1)\n",
        "      total_loss += edge_small_l1\n",
        "      \"\"\" \n",
        "\n",
        "      # FRRN\n",
        "      \"\"\"\n",
        "      mid_l1_loss = 0\n",
        "      for idx in range(len(mid_x) - 1):\n",
        "          mid_l1_loss += self.L1Loss(mid_x[idx] * mid_mask[idx], train_batch[2] * mid_mask[idx])\n",
        "      self.log('loss/mid_l1_loss', mid_l1_loss)\n",
        "      total_loss += mid_l1_loss\n",
        "      \"\"\"\n",
        "\n",
        "      #self.log('loss/g_loss', total_loss)\n",
        "      writer.add_scalar('loss/g_loss', total_loss, self.trainer.global_step)\n",
        "\n",
        "      #return total_loss\n",
        "      #########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # train discriminator\n",
        "      # resizing input if needed\n",
        "      #train_batch[2] = torch.nn.functional.interpolate(train_batch[2], (128,128), align_corners=False, mode='bilinear')\n",
        "      #out = torch.nn.functional.interpolate(out, (128,128), align_corners=False, mode='bilinear')\n",
        "\n",
        "      Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "      valid = Variable(Tensor(out.shape).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(out.shape).fill_(0.0), requires_grad=False)\n",
        "      dis_real_loss = self.MSELoss(train_batch[1], valid)\n",
        "      dis_fake_loss = self.MSELoss(out, fake)\n",
        "\n",
        "      d_loss = (dis_real_loss + dis_fake_loss) / 2\n",
        "      #self.log('loss/d_loss', d_loss)\n",
        "      writer.add_scalar('loss/d_loss', d_loss, self.trainer.global_step)\n",
        "\n",
        "      return total_loss+d_loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      #optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-3)\n",
        "      optimizer = AdamP(self.netG.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "      #optimizer = SGDP(self.netG.parameters(), lr=0.1, weight_decay=1e-5, momentum=0.9, nesterov=True)\n",
        "      return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    # inpainting\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "\n",
        "    # super resolution\n",
        "    # train_batch[0] = lr\n",
        "    # train_batch[1] = hr\n",
        "    # train_batch[2] = lr_path\n",
        "\n",
        "    #########################\n",
        "    # generate fake (one output generator)\n",
        "    #out = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    #########################\n",
        "    # generate fake (two output generator)\n",
        "    #out, _ = self(train_batch[0],train_batch[1])\n",
        "\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "    #########################\n",
        "    # CSA\n",
        "    #_, out, _, _ = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    # EdgeConnect\n",
        "    # train_batch[3] = edges\n",
        "    # train_batch[4] = grayscale\n",
        "    #out, _ = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "\n",
        "    # PVRS\n",
        "    #out, _ ,_, _ = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "    # FRRN\n",
        "    #out, _, _ = self(train_batch[0], train_batch[1])\n",
        "\n",
        "    # deoldify\n",
        "    #out = self.netG(train_batch[0])\n",
        "\n",
        "    ############################\n",
        "    # ESRGAN\n",
        "    out = self.netG(train_batch[0])\n",
        "\n",
        "    # Validation metrics work, but they need an origial source image.\n",
        "    # Change dataloader to provide LR and HR if you want metrics.\n",
        "    self.log('metrics/PSNR', self.psnr_metric(train_batch[1], out))\n",
        "    self.log('metrics/SSIM', self.ssim_metric(train_batch[1], out))\n",
        "    self.log('metrics/MSE', self.mse_metric(train_batch[1], out))\n",
        "    self.log('metrics/LPIPS', self.PerceptualLoss(out, train_batch[1]))\n",
        "\n",
        "    validation_output = '/content/validation_output/' #@param\n",
        "\n",
        "    # train_batch[2] can contain multiple files, depending on the batch_size\n",
        "    for f in train_batch[2]:\n",
        "      # data is processed as a batch, to save indididual files, a counter is used\n",
        "      counter = 0\n",
        "      if not os.path.exists(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0])):\n",
        "        os.makedirs(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0]))\n",
        "\n",
        "      filename_with_extention = os.path.basename(f)\n",
        "      filename = os.path.splitext(filename_with_extention)[0]\n",
        "\n",
        "      save_image(out[counter], os.path.join(validation_output, filename, str(self.trainer.global_step) + '.png'))\n",
        "\n",
        "      counter += 1\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # inpainting\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "\n",
        "    # super resolution\n",
        "    # train_batch[0] = lr\n",
        "    # train_batch[1] = hr\n",
        "    # train_batch[2] = lr_path\n",
        "    test_output = '/content/test_output/' #@param\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzBj6jlz5_Ze"
      },
      "source": [
        "# Inpaint Generators\n",
        "\n",
        "Only run of these cells and confure further inside ``CustomTrainClass``. If you run more, it could maybe cause problems. You should restart the notebook once this happens.\n",
        "\n",
        "Sidenote: Some files use ``.type(torch.cuda.FloatTensor)`` to avoid crashing. You could also try ``.type(torch.cuda.HalfTensor)``, but this is untested behaviour, but might help with AMP. ``[no AMP]`` indicates ``loss=nan`` if you actually try to use AMP.\n",
        "\n",
        "With one output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf0peHyRYcgs",
        "cellView": "form"
      },
      "source": [
        "#@title [DSNet](https://github.com/wangning-001/DSNet) (2021)\n",
        "\"\"\"\n",
        "DSNet.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/DSNet.py\n",
        "\n",
        "RegionNorm.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/RegionNorm.py\n",
        "\n",
        "ValidMigration.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/ValidMigration.py\n",
        "\n",
        "Attention.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/Attention.py\n",
        "\n",
        "deform_conv.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/deform_conv.py\n",
        "\"\"\"\n",
        "#from modules.Attention import PixelContextualAttention\n",
        "#from modules.RegionNorm import RBNModule, RCNModule\n",
        "#from modules.ValidMigration import ConvOffset2D\n",
        "#from modules.deform_conv import th_batch_map_offsets, th_generate_grid\n",
        "from __future__ import absolute_import, division\n",
        "from scipy.ndimage.interpolation import map_coordinates as sp_map_coordinates\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "def th_flatten(a):\n",
        "    \"\"\"Flatten tensor\"\"\"\n",
        "    return a.contiguous().view(a.nelement())\n",
        "\n",
        "\n",
        "def th_repeat(a, repeats, axis=0):\n",
        "    \"\"\"Torch version of np.repeat for 1D\"\"\"\n",
        "    assert len(a.size()) == 1\n",
        "    return th_flatten(torch.transpose(a.repeat(repeats, 1), 0, 1))\n",
        "\n",
        "\n",
        "def np_repeat_2d(a, repeats):\n",
        "    \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n",
        "\n",
        "    assert len(a.shape) == 2\n",
        "    a = np.expand_dims(a, 0)\n",
        "    a = np.tile(a, [repeats, 1, 1])\n",
        "    return a\n",
        "\n",
        "\n",
        "def th_gather_2d(input, coords):\n",
        "    inds = coords[:, 0]*input.size(1) + coords[:, 1]\n",
        "    x = torch.index_select(th_flatten(input), 0, inds)\n",
        "    return x.view(coords.size(0))\n",
        "\n",
        "\n",
        "def th_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n",
        "    Note that coords is transposed and only 2D is supported\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (s, s)\n",
        "    coords : tf.Tensor. shape = (n_points, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    assert order == 1\n",
        "    input_size = input.size(0)\n",
        "\n",
        "    coords = torch.clamp(coords, 0, input_size - 1)\n",
        "    coords_lt = coords.floor().long()\n",
        "    coords_rb = coords.ceil().long()\n",
        "    coords_lb = torch.stack([coords_lt[:, 0], coords_rb[:, 1]], 1)\n",
        "    coords_rt = torch.stack([coords_rb[:, 0], coords_lt[:, 1]], 1)\n",
        "\n",
        "    vals_lt = th_gather_2d(input,  coords_lt.detach())\n",
        "    vals_rb = th_gather_2d(input,  coords_rb.detach())\n",
        "    vals_lb = th_gather_2d(input,  coords_lb.detach())\n",
        "    vals_rt = th_gather_2d(input,  coords_rt.detach())\n",
        "\n",
        "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
        "\n",
        "    vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n",
        "    vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n",
        "    mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_coordinates(inputs, coords):\n",
        "    \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n",
        "    # coords = coords.clip(0, inputs.shape[1] - 1)\n",
        "\n",
        "    assert (coords.shape[2] == 2)\n",
        "    height = coords[:,:,0].clip(0, inputs.shape[1] - 1)\n",
        "    width = coords[:,:,1].clip(0, inputs.shape[2] - 1)\n",
        "    np.concatenate((np.expand_dims(height, axis=2), np.expand_dims(width, axis=2)), 2)\n",
        "\n",
        "    mapped_vals = np.array([\n",
        "        sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n",
        "        for input, coord in zip(inputs, coords)\n",
        "    ])\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def th_batch_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Batch version of th_map_coordinates\n",
        "    Only supports 2D feature maps\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (b, s, s)\n",
        "    coords : tf.Tensor. shape = (b, n_points, 2)\n",
        "    Returns\n",
        "    -------\n",
        "    tf.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = input.size(0)\n",
        "    input_height = input.size(1)\n",
        "    input_width = input.size(2)\n",
        "\n",
        "    n_coords = coords.size(1)\n",
        "\n",
        "    # coords = torch.clamp(coords, 0, input_size - 1)\n",
        "\n",
        "    coords = torch.cat((torch.clamp(coords.narrow(2, 0, 1), 0, input_height - 1), torch.clamp(coords.narrow(2, 1, 1), 0, input_width - 1)), 2)\n",
        "\n",
        "    assert (coords.size(1) == n_coords)\n",
        "\n",
        "    coords_lt = coords.floor().long()\n",
        "    coords_rb = coords.ceil().long()\n",
        "    coords_lb = torch.stack([coords_lt[..., 0], coords_rb[..., 1]], 2)\n",
        "    coords_rt = torch.stack([coords_rb[..., 0], coords_lt[..., 1]], 2)\n",
        "    idx = th_repeat(torch.arange(0, batch_size), n_coords).long()\n",
        "    idx = Variable(idx, requires_grad=False)\n",
        "    if input.is_cuda:\n",
        "        idx = idx.cuda()\n",
        "\n",
        "    def _get_vals_by_coords(input, coords):\n",
        "        indices = torch.stack([\n",
        "            idx, th_flatten(coords[..., 0]), th_flatten(coords[..., 1])\n",
        "        ], 1)\n",
        "        inds = indices[:, 0]*input.size(1)*input.size(2)+ indices[:, 1]*input.size(2) + indices[:, 2]\n",
        "        vals = th_flatten(input).index_select(0, inds)\n",
        "        vals = vals.view(batch_size, n_coords)\n",
        "        return vals\n",
        "\n",
        "    vals_lt = _get_vals_by_coords(input, coords_lt.detach())\n",
        "    vals_rb = _get_vals_by_coords(input, coords_rb.detach())\n",
        "    vals_lb = _get_vals_by_coords(input, coords_lb.detach())\n",
        "    vals_rt = _get_vals_by_coords(input, coords_rt.detach())\n",
        "\n",
        "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
        "    vals_t = coords_offset_lt[..., 0]*(vals_rt - vals_lt) + vals_lt\n",
        "    vals_b = coords_offset_lt[..., 0]*(vals_rb - vals_lb) + vals_lb\n",
        "    mapped_vals = coords_offset_lt[..., 1]* (vals_b - vals_t) + vals_t\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_offsets(input, offsets):\n",
        "    \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n",
        "\n",
        "    batch_size = input.shape[0]\n",
        "    input_height = input.shape[1]\n",
        "    input_width = input.shape[2]\n",
        "\n",
        "    offsets = offsets.reshape(batch_size, -1, 2)\n",
        "    grid = np.stack(np.mgrid[:input_height, :input_width], -1).reshape(-1, 2)\n",
        "    grid = np.repeat([grid], batch_size, axis=0)\n",
        "    coords = offsets + grid\n",
        "    # coords = coords.clip(0, input_size - 1)\n",
        "\n",
        "    mapped_vals = sp_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def th_generate_grid(batch_size, input_height, input_width, dtype, cuda):\n",
        "    grid = np.meshgrid(\n",
        "        range(input_height), range(input_width), indexing='ij'\n",
        "    )\n",
        "    grid = np.stack(grid, axis=-1)\n",
        "    grid = grid.reshape(-1, 2)\n",
        "\n",
        "    grid = np_repeat_2d(grid, batch_size)\n",
        "    grid = torch.from_numpy(grid).type(dtype)\n",
        "    if cuda:\n",
        "        grid = grid.cuda()\n",
        "    return Variable(grid, requires_grad=False)\n",
        "\n",
        "\n",
        "def th_batch_map_offsets(input, offsets, grid=None, order=1):\n",
        "    \"\"\"Batch map offsets into input\n",
        "    Parameters\n",
        "    ---------\n",
        "    input : torch.Tensor. shape = (b, s, s)\n",
        "    offsets: torch.Tensor. shape = (b, s, s, 2)\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "    batch_size = input.size(0)\n",
        "    input_height = input.size(1)\n",
        "    input_width = input.size(2)\n",
        "\n",
        "    offsets = offsets.view(batch_size, -1, 2)\n",
        "    if grid is None:\n",
        "        grid = th_generate_grid(batch_size, input_height, input_width, offsets.data.type(), offsets.data.is_cuda)\n",
        "\n",
        "    coords = offsets + grid\n",
        "\n",
        "    mapped_vals = th_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "class SEModule(pl.LightningModule):\n",
        "    def __init__(self, num_channel, squeeze_ratio=1.0):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.sequeeze_mod = nn.AdaptiveAvgPool2d(1)\n",
        "        self.num_channel = num_channel\n",
        "\n",
        "        blocks = [nn.Linear(num_channel, int(num_channel * squeeze_ratio)),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(int(num_channel * squeeze_ratio), num_channel),\n",
        "                  nn.Sigmoid()]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ori = x\n",
        "        x = self.sequeeze_mod(x)\n",
        "        x = x.view(x.size(0), 1, self.num_channel)\n",
        "        x = self.blocks(x)\n",
        "        x = x.view(x.size(0), self.num_channel, 1, 1)\n",
        "        x = ori * x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ContextualAttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, patch_size=3, propagate_size=3, stride=1):\n",
        "        super(ContextualAttentionModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        ###assume the masked area has value 1\n",
        "        bz, nc, w, h = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background * masks\n",
        "        background = F.pad(background,\n",
        "                           [self.patch_size // 2, self.patch_size // 2, self.patch_size // 2, self.patch_size // 2])\n",
        "        conv_kernels_all = background.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size,\n",
        "                                                                                     self.stride).contiguous().view(bz,\n",
        "                                                                                                                    nc,\n",
        "                                                                                                                    -1,\n",
        "                                                                                                                    self.patch_size,\n",
        "                                                                                                                    self.patch_size)\n",
        "        conv_kernels_all = conv_kernels_all.transpose(2, 1)\n",
        "        output_tensor = []\n",
        "        for i in range(bz):\n",
        "            mask = masks[i:i + 1]\n",
        "            feature_map = foreground[i:i + 1].contiguous()\n",
        "            # form convolutional kernels\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels ** 2, [1, 2, 3], keepdim=True) ** 0.5\n",
        "            conv_kernels = conv_kernels / norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding=self.patch_size // 2)\n",
        "            \"\"\"\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.conv2d(conv_result, self.prop_kernels, stride=1, padding=1, groups=conv_result.size(1))\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "            self.prop_kernels.requires_grad = False\n",
        "            self.prop_kernels = self.prop_kernels.cuda()\n",
        "            conv_result = F.conv2d(conv_result, self.prop_kernels, stride=1, padding=1, groups=conv_result.size(1))\n",
        "            \n",
        "            attention_scores = F.softmax(conv_result, dim=1)\n",
        "            ##propagate the scores\n",
        "            recovered_foreground = F.conv_transpose2d(attention_scores, conv_kernels, stride=1,\n",
        "                                                      padding=self.patch_size // 2)\n",
        "            # average the recovered value, at the same time make non-masked area 0\n",
        "            recovered_foreground = (recovered_foreground * (1 - mask)) / (self.patch_size ** 2)\n",
        "            # recover the image\n",
        "            final_output = recovered_foreground + feature_map * mask\n",
        "            output_tensor.append(final_output)\n",
        "        return torch.cat(output_tensor, dim=0)\n",
        "\n",
        "\n",
        "class PixelContextualAttention(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, inchannel, patch_size_list=[1], propagate_size_list=[3], stride_list=[1]):\n",
        "        assert isinstance(patch_size_list,\n",
        "                          list), \"patch_size should be a list containing scales, or you should use Contextual Attention to initialize your module\"\n",
        "        assert len(patch_size_list) == len(propagate_size_list) and len(propagate_size_list) == len(\n",
        "            stride_list), \"the input_lists should have same lengths\"\n",
        "        super(PixelContextualAttention, self).__init__()\n",
        "        for i in range(len(patch_size_list)):\n",
        "            name = \"CA_{:d}\".format(i)\n",
        "            setattr(self, name, ContextualAttentionModule(patch_size_list[i], propagate_size_list[i], stride_list[i]))\n",
        "        self.num_of_modules = len(patch_size_list)\n",
        "        self.SqueezeExc = SEModule(inchannel * 2)\n",
        "        self.combiner = nn.Conv2d(inchannel * 2, inchannel, kernel_size=1)\n",
        "\n",
        "    def forward(self, foreground, mask):\n",
        "        outputs = [foreground]\n",
        "        for i in range(self.num_of_modules):\n",
        "            name = \"CA_{:d}\".format(i)\n",
        "            CA_module = getattr(self, name)\n",
        "            outputs.append(CA_module(foreground, mask))\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        outputs = self.SqueezeExc(outputs)\n",
        "        outputs = self.combiner(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvOffset2D(nn.Conv2d):\n",
        "    \"\"\"ConvOffset2D\n",
        "\n",
        "    Convolutional layer responsible for learning the 2D offsets and output the\n",
        "    deformed feature map using bilinear interpolation\n",
        "\n",
        "    Note that this layer does not perform convolution on the deformed feature\n",
        "    map. See get_deform_cnn in cnn.py for usage\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n",
        "        \"\"\"Init\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filters : int\n",
        "            Number of channel of the input feature map\n",
        "        init_normal_stddev : float\n",
        "            Normal kernel initialization\n",
        "        **kwargs:\n",
        "            Pass to superclass. See Con2d layer in pytorch\n",
        "        \"\"\"\n",
        "        self.filters = filters\n",
        "        self._grid_param = None\n",
        "        super(ConvOffset2D, self).__init__(self.filters, self.filters*2, 3, padding=1, bias=False, **kwargs)\n",
        "        self.weight.data.copy_(self._init_weights(self.weight, init_normal_stddev))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Return the deformed featured map\"\"\"\n",
        "        x_shape = x.size()\n",
        "        offsets = super(ConvOffset2D, self).forward(x)\n",
        "\n",
        "        # offsets: (b*c, h, w, 2)\n",
        "        offsets = self._to_bc_h_w_2(offsets, x_shape)\n",
        "\n",
        "        # x: (b*c, h, w)\n",
        "        x = self._to_bc_h_w(x, x_shape)\n",
        "\n",
        "        # X_offset: (b*c, h, w)\n",
        "        x_offset = th_batch_map_offsets(x, offsets, grid=self._get_grid(self,x))\n",
        "\n",
        "        # x_offset: (b, h, w, c)\n",
        "        x_offset = self._to_b_c_h_w(x_offset, x_shape)\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_grid(self, x):\n",
        "        batch_size, input_height, input_width = x.size(0), x.size(1), x.size(2)\n",
        "        dtype, cuda = x.data.type(), x.data.is_cuda\n",
        "        if self._grid_param == (batch_size, input_height, input_width, dtype, cuda):\n",
        "            return self._grid\n",
        "        self._grid_param = (batch_size, input_height, input_width, dtype, cuda)\n",
        "        self._grid = th_generate_grid(batch_size, input_height, input_width, dtype, cuda)\n",
        "        return self._grid\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(weights, std):\n",
        "        fan_out = weights.size(0)\n",
        "        fan_in = weights.size(1) * weights.size(2) * weights.size(3)\n",
        "        w = np.random.normal(0.0, std, (fan_out, fan_in))\n",
        "        return torch.from_numpy(w.reshape(weights.size()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w_2(x, x_shape):\n",
        "        \"\"\"(b, 2c, h, w) -> (b*c, h, w, 2)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]), 2)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w(x, x_shape):\n",
        "        \"\"\"(b, c, h, w) -> (b*c, h, w)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_b_c_h_w(x, x_shape):\n",
        "        \"\"\"(b*c, h, w) -> (b, c, h, w)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RBNModule(pl.LightningModule):\n",
        "    _version = 2\n",
        "    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n",
        "                     'running_mean', 'running_var', 'num_batches_tracked']\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, affine=True, track_running_stats=True):\n",
        "        super(RBNModule, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.momentum = momentum\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1, 1))\n",
        "            self.register_buffer('running_var', torch.ones(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            nn.init.uniform_(self.weight)\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, mask_t):\n",
        "        input_m = input * mask_t\n",
        "        if self.training:\n",
        "            mask_mean = torch.mean(mask_t, (0, 2, 3), True)\n",
        "            x_mean = torch.mean(input_m, (0, 2, 3), True) / mask_mean\n",
        "            x_var = torch.mean(((input_m - x_mean) * mask_t) ** 2, (0, 2, 3), True) / mask_mean\n",
        "\n",
        "            x_out = self.weight * (input_m - x_mean) / torch.sqrt(x_var + self.eps) + self.bias\n",
        "\n",
        "            self.running_mean.mul_(self.momentum)\n",
        "            self.running_mean.add_((1 - self.momentum) * x_mean.data)\n",
        "            self.running_var.mul_(self.momentum)\n",
        "            self.running_var.add_((1 - self.momentum) * x_var.data)\n",
        "        else:\n",
        "            x_out = self.weight * (input_m - self.running_mean) / torch.sqrt(self.running_var + self.eps) + self.bias\n",
        "        return x_out * mask_t + input * (1 - mask_t)\n",
        "\n",
        "\n",
        "class RCNModule(pl.LightningModule):\n",
        "    _version = 2\n",
        "    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n",
        "                     'running_mean', 'running_var', 'num_batches_tracked']\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, affine=True, track_running_stats=True):\n",
        "        super(RCNModule, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.momentum = momentum\n",
        "        self.mean_weight = nn.Parameter(torch.ones(3))\n",
        "        self.var_weight = nn.Parameter(torch.ones(3))\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1, 1))\n",
        "            self.register_buffer('running_var', torch.ones(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            nn.init.uniform_(self.weight)\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, mask_t):\n",
        "        input_m = input * mask_t\n",
        "\n",
        "        if self.training:\n",
        "            mask_mean_bn = torch.mean(mask_t, (0, 2, 3), True)\n",
        "            mean_bn = torch.mean(input_m, (0, 2, 3), True) / mask_mean_bn\n",
        "            var_bn = torch.mean(((input_m - mean_bn) * mask_t) ** 2, (0, 2, 3), True) / mask_mean_bn\n",
        "\n",
        "            self.running_mean.mul_(self.momentum)\n",
        "            self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
        "            self.running_var.mul_(self.momentum)\n",
        "            self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
        "        else:\n",
        "            mean_bn = torch.autograd.Variable(self.running_mean)\n",
        "            var_bn = torch.autograd.Variable(self.running_var)\n",
        "\n",
        "        mask_mean_in = torch.mean(mask_t, (2, 3), True)\n",
        "        mean_in = torch.mean(input_m, (2, 3), True) / mask_mean_in\n",
        "        var_in = torch.mean(((input_m - mean_in) * mask_t) ** 2, (2, 3), True) / mask_mean_in\n",
        "\n",
        "        mask_mean_ln = torch.mean(mask_t, (1, 2, 3), True)\n",
        "        mean_ln = torch.mean(input_m, (1, 2, 3), True) / mask_mean_ln\n",
        "        var_ln = torch.mean(((input_m - mean_ln) * mask_t) ** 2, (1, 2, 3), True) / mask_mean_ln\n",
        "\n",
        "        mean_weight = F.softmax(self.mean_weight)\n",
        "        var_weight = F.softmax(self.var_weight)\n",
        "\n",
        "        x_mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn\n",
        "        x_var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn\n",
        "\n",
        "        x_out = self.weight * (input_m - x_mean) / torch.sqrt(x_var + self.eps) + self.bias\n",
        "        return x_out * mask_t + input * (1 - mask_t)\n",
        "\n",
        "\n",
        "class DSModule(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=False, rn=True, sample='none-3', activ='relu',\n",
        "                 conv_bias=False, defor=True):\n",
        "        super().__init__()\n",
        "        if sample == 'down-5':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 5, 2, 2, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(5,2,2)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 7, 2, 3, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(7, 2, 3)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 3, 2, 1, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(3, 2, 1)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_ch+2, out_ch, 3, 1, 1, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(3,1,1)\n",
        "            if defor:\n",
        "                self.offset0 = ConvOffset2D(in_ch-out_ch+1)\n",
        "                self.offset1 = ConvOffset2D(out_ch+1)\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(out_ch)\n",
        "        if rn:\n",
        "            # Regional Composite Normalization\n",
        "            self.rn = RCNModule(out_ch)\n",
        "\n",
        "            # Regional Batch Normalization\n",
        "            # self.rn = RBNModule(out_ch)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2, inplace = True)\n",
        "\n",
        "    def forward(self, input, input_mask):\n",
        "        if hasattr(self, 'offset'):\n",
        "            input = torch.cat([input, input_mask[:,:1,:,:]], dim = 1)\n",
        "            h = self.offset(input)\n",
        "            h = input*input_mask[:,:1,:,:] + (1-input_mask[:,:1,:,:])*h\n",
        "            h = self.conv(h)\n",
        "            h_mask = self.updatemask(input_mask[:,:1,:,:])\n",
        "            h = h*h_mask\n",
        "            h = self.rn(h, h_mask)\n",
        "        elif hasattr(self, 'offset0'):\n",
        "            h1_in = torch.cat([input[:,self.in_ch-self.out_ch:,:,:], input_mask[:,1:,:,:]], dim = 1)\n",
        "            m1_in = input_mask[:,1:,:,:]\n",
        "            h0 = torch.cat([input[:,:self.in_ch-self.out_ch,:,:], input_mask[:,:1,:,:]], dim = 1)\n",
        "            h1 = self.offset1(h1_in)\n",
        "            h1 = m1_in*h1_in + (1-m1_in)*h1\n",
        "            h = self.conv(torch.cat([h0,h1], dim = 1))\n",
        "            h = self.rn(h, input_mask[:,:1,:,:])\n",
        "            h_mask = F.interpolate(input_mask[:,:1,:,:], scale_factor=2, mode='nearest')\n",
        "        else:\n",
        "            h = self.conv(torch.cat([input, input_mask[:,:,:,:]], dim = 1))\n",
        "            h_mask = self.updatemask(input_mask[:,:1,:,:])\n",
        "            h = h*h_mask\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            h = self.bn(h)\n",
        "        if hasattr(self, 'activation'):\n",
        "            h = self.activation(h)\n",
        "        return h, h_mask\n",
        "\n",
        "\n",
        "class DSNet(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, upsampling_mode='nearest'):\n",
        "        super().__init__()\n",
        "        self.freeze_enc_bn = False\n",
        "        self.upsampling_mode = upsampling_mode\n",
        "        self.layer_size = layer_size\n",
        "        self.enc_1 = DSModule(input_channels, 64, rn=False, sample='down-7', defor = False)\n",
        "        self.enc_2 = DSModule(64, 128, sample='down-5')\n",
        "        self.enc_3 = DSModule(128, 256, sample='down-5')\n",
        "        self.enc_4 = DSModule(256, 512, sample='down-3')\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            setattr(self, name, DSModule(512, 512, sample='down-3'))\n",
        "\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'dec_{:d}'.format(i + 1)\n",
        "            setattr(self, name, DSModule(512 + 512, 512, activ='leaky'))\n",
        "        self.dec_4 = DSModule(512 + 256, 256, activ='leaky')\n",
        "        self.dec_3 = DSModule(256 + 128, 128, activ='leaky')\n",
        "        self.dec_2 = DSModule(128 + 64, 64, activ='leaky')\n",
        "        self.dec_1 = DSModule(64 + input_channels, input_channels,\n",
        "                              rn=False, activ=None, defor = False)\n",
        "        self.att = PixelContextualAttention(128)\n",
        "    def forward(self, input, input_mask):\n",
        "        input = input.type(torch.cuda.FloatTensor)\n",
        "        input_mask = input_mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        input_mask = input_mask[:,0:1,:,:]\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "        h_mask_dict = {}  # for the output of enc_N\n",
        "\n",
        "        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(\n",
        "                h_dict[h_key_prev], h_mask_dict[h_key_prev])\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h_key = 'h_{:d}'.format(self.layer_size)\n",
        "        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n",
        "        h_mask = F.interpolate(h_mask, scale_factor=2, mode='nearest')\n",
        "\n",
        "        for i in range(self.layer_size, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i - 1)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "\n",
        "            h = F.interpolate(h, scale_factor=2, mode=self.upsampling_mode)\n",
        "\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n",
        "            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim=1)\n",
        "            h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n",
        "            if i == 3:\n",
        "                h = self.att(h, input_mask[:,:1,:,:])\n",
        "        #return h, h_mask\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3w2bvCS6AoH",
        "cellView": "form"
      },
      "source": [
        "#@title [AdaFill_arch.py](https://github.com/ChajinShin/AdaFill-Image_Inpainting) (2021)\n",
        "\"\"\"\n",
        "network.py (3-2-20)\n",
        "https://github.com/ChajinShin/AdaFill-Image_Inpainting/blob/main/Model/AdaFill/src/network.py\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class ResModule(pl.LightningModule):\n",
        "    def __init__(self, num_features, normalization):\n",
        "        super(ResModule, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            normalization(num_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(num_features, num_features, kernel_size=3, stride=1, padding=1),\n",
        "            normalization(num_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(num_features, num_features, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "\n",
        "class InpaintNet(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(InpaintNet, self).__init__()\n",
        "        cnum = 64\n",
        "        num_of_resblock = 8\n",
        "        normalization_config = 'batch_norm'\n",
        "\n",
        "        if normalization_config == 'batch_norm':\n",
        "            normalization = nn.BatchNorm2d\n",
        "        elif normalization_config == 'instance_norm':\n",
        "            normalization = nn.InstanceNorm2d\n",
        "        else:\n",
        "            raise ValueError(\"batch normalization or instance normalization is only available\")\n",
        "\n",
        "        self.enc_conv1 = nn.Conv2d(in_channels=4, out_channels=cnum, kernel_size=3, stride=1, padding=1)  # cnum, 128, 128\n",
        "        self.enc_norm1 = normalization(num_features=cnum)\n",
        "        self.enc_activation1 = nn.ReLU(inplace=True)\n",
        "        self.enc_conv2 = nn.Conv2d(in_channels=cnum, out_channels=2*cnum, kernel_size=3, stride=2, padding=1)  # 2cnum, 64, 64\n",
        "        self.enc_norm2 = normalization(num_features=2*cnum)\n",
        "        self.enc_activation2 = nn.ReLU(inplace=True)\n",
        "        self.enc_conv3 = nn.Conv2d(in_channels=2*cnum, out_channels=4*cnum, kernel_size=3, stride=2, padding=1)  # 4cnum, 32, 32\n",
        "\n",
        "        res = [ResModule(4*cnum, normalization) for _ in range(num_of_resblock)]\n",
        "        self.res_module = nn.Sequential(\n",
        "            *res\n",
        "        )\n",
        "\n",
        "        self.dec_norm1 = normalization(4*cnum)\n",
        "        self.dec_activation1 = nn.ReLU(inplace=True)\n",
        "        self.dec_conv1 = nn.Conv2d(in_channels=4*cnum, out_channels=2*cnum, kernel_size=3, stride=1, padding=1)    # 2*cnum, 64, 64\n",
        "        self.dec_norm2 = normalization(num_features=2*cnum)\n",
        "        self.dec_activation2 = nn.ReLU(inplace=True)\n",
        "        self.dec_conv2 = nn.Conv2d(in_channels=2*cnum, out_channels=cnum, kernel_size=3, stride=1, padding=1)   # cnum, 128, 128\n",
        "        self.dec_norm3 = normalization(num_features=cnum)\n",
        "        self.dec_activation3 = nn.ReLU(inplace=True)\n",
        "        self.dec_conv3 = nn.Conv2d(in_channels=cnum, out_channels=3, kernel_size=3, stride=1, padding=1)    # 3, 128, 128\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        x = torch.cat((image, mask), 1)\n",
        "        # --------- encoder ----------------\n",
        "        x = self.enc_conv1(x)\n",
        "        x = self.enc_norm1(x)\n",
        "        x = self.enc_activation1(x)\n",
        "        size_1x = [x.size(2), x.size(3)]\n",
        "\n",
        "        x = self.enc_conv2(x)\n",
        "        x = self.enc_norm2(x)\n",
        "        x = self.enc_activation2(x)\n",
        "        size_2x = [x.size(2), x.size(3)]\n",
        "\n",
        "        x = self.enc_conv3(x)\n",
        "\n",
        "        # --------- res module ----------------\n",
        "        x = self.res_module(x)\n",
        "\n",
        "        # --------- decoder ----------------\n",
        "        x = self.dec_norm1(x)\n",
        "        x = self.dec_activation1(x)\n",
        "        x = nn.functional.interpolate(x, size=size_2x)\n",
        "        x = self.dec_conv1(x)\n",
        "\n",
        "        x = self.dec_norm2(x)\n",
        "        x = self.dec_activation2(x)\n",
        "        x = nn.functional.interpolate(x, size=size_1x)\n",
        "        x = self.dec_conv2(x)\n",
        "\n",
        "        x = self.dec_norm3(x)\n",
        "        x = self.dec_activation3(x)\n",
        "        x = self.dec_conv3(x)\n",
        "        x = self.tanh(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ849AKI6Nvr",
        "cellView": "form"
      },
      "source": [
        "#@title [MEDFE_arch.py](https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE) (2020)\n",
        "\"\"\"\n",
        "Encoder.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/9adf8898a142784976bb3e162a9fd864c224e01e/models/Encoder.py\n",
        "\n",
        "Decoder.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/9adf8898a142784976bb3e162a9fd864c224e01e/models/Decoder.py\n",
        "\n",
        "networks.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/9adf8898a142784976bb3e162a9fd864c224e01e/models/networks.py\n",
        "\n",
        "MEDFE.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/master/models/MEDFE.py\n",
        "\n",
        "PCconv.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/dd838b01d9786dc2c67de5d71869e5a60da28eb9/models/PCconv.py\n",
        "\n",
        "Selfpatch.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/dd838b01d9786dc2c67de5d71869e5a60da28eb9/util/Selfpatch.py\n",
        "\n",
        "util.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/dd838b01d9786dc2c67de5d71869e5a60da28eb9/util/util.py\n",
        "\n",
        "InnerCos.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/c7156eab4a9890888fa86e641cd685e21b78c31e/models/InnerCos.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torch.autograd import Variable\n",
        "import collections\n",
        "import inspect, re\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class InnerCos(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(InnerCos, self).__init__()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.target = None\n",
        "        self.down_model = nn.Sequential(\n",
        "            nn.Conv2d(256, 3, kernel_size=1,stride=1, padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def set_target(self, targetde, targetst):\n",
        "        self.targetst = F.interpolate(targetst, size=(32, 32), mode='bilinear')\n",
        "        self.targetde = F.interpolate(targetde, size=(32, 32), mode='bilinear')\n",
        "\n",
        "    def get_target(self):\n",
        "        return self.target\n",
        "\n",
        "    def forward(self, in_data):\n",
        "        loss_co = in_data[1]\n",
        "        self.ST = self.down_model(loss_co[0])\n",
        "        self.DE = self.down_model(loss_co[1])\n",
        "        #self.loss = self.criterion(self.ST, self.targetst)+self.criterion(self.DE, self.targetde)\n",
        "        self.output = in_data[0]\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, retain_graph=True):\n",
        "\n",
        "        self.loss.backward(retain_graph=retain_graph)\n",
        "        return self.loss\n",
        "\n",
        "    def __repr__(self):\n",
        "\n",
        "        return self.__class__.__name__\n",
        "\n",
        "# Converts a Tensor into a Numpy array\n",
        "# |imtype|: the desired type of the converted numpy array\n",
        "def tensor2im(image_tensor, imtype=np.uint8):\n",
        "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
        "    if image_numpy.shape[0] == 1:\n",
        "        image_numpy = np.tile(image_numpy, (3,1,1))\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "def diagnose_network(net, name='network'):\n",
        "    mean = 0.0\n",
        "    count = 0\n",
        "    for param in net.parameters():\n",
        "        if param.grad is not None:\n",
        "            mean += torch.mean(torch.abs(param.grad.data))\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        mean = mean / count\n",
        "    print(name)\n",
        "    print(mean)\n",
        "\n",
        "def binary_mask(in_mask, threshold):\n",
        "    assert in_mask.dim() == 2, \"mask must be 2 dimensions\"\n",
        "\n",
        "    output = torch.ByteTensor(in_mask.size())\n",
        "    output = (output > threshold).float().mul_(1)\n",
        "\n",
        "    return output\n",
        "\n",
        "def gussin(v):\n",
        "    outk = []\n",
        "    v = v\n",
        "    for i in range(32):\n",
        "        for k in range(32):\n",
        "\n",
        "            out = []\n",
        "            for x in range(32):\n",
        "                row = []\n",
        "                for y in range(32):\n",
        "                    cord_x = i\n",
        "                    cord_y = k\n",
        "                    dis_x = np.abs(x - cord_x)\n",
        "                    dis_y = np.abs(y - cord_y)\n",
        "                    dis_add = -(dis_x * dis_x + dis_y * dis_y)\n",
        "                    dis_add = dis_add / (2 * v * v)\n",
        "                    dis_add = math.exp(dis_add) / (2 * math.pi * v * v)\n",
        "\n",
        "                    row.append(dis_add)\n",
        "                out.append(row)\n",
        "\n",
        "            outk.append(out)\n",
        "\n",
        "    out = np.array(outk)\n",
        "    f = out.sum(-1).sum(-1)\n",
        "    q = []\n",
        "    for i in range(1024):\n",
        "        g = out[i] / f[i]\n",
        "        q.append(g)\n",
        "    out = np.array(q)\n",
        "    return torch.from_numpy(out)\n",
        "\n",
        "def cal_feat_mask(inMask, conv_layers, threshold):\n",
        "    assert inMask.dim() == 4, \"mask must be 4 dimensions\"\n",
        "    assert inMask.size(0) == 1, \"the first dimension must be 1 for mask\"\n",
        "    inMask = inMask.float()\n",
        "    convs = []\n",
        "    inMask = Variable(inMask, requires_grad = False)\n",
        "    for id_net in range(conv_layers):\n",
        "        conv = nn.Conv2d(1,1,4,2,1, bias=False)\n",
        "        conv.weight.data.fill_(1/16)\n",
        "        convs.append(conv)\n",
        "    lnet = nn.Sequential(*convs)\n",
        "    if inMask.is_cuda:\n",
        "\n",
        "        lnet = lnet.cuda()\n",
        "    output = lnet(inMask)\n",
        "    output = (output > threshold).float().mul_(1)\n",
        "\n",
        "    return output\n",
        "\n",
        "def cal_mask_given_mask_thred(img, mask, patch_size, stride, mask_thred):\n",
        "    assert img.dim() == 3, 'img has to be 3 dimenison!'\n",
        "    assert mask.dim() == 2, 'mask has to be 2 dimenison!'\n",
        "    dim = img.dim()\n",
        "    #math.floor 是向下取整\n",
        "    _, H, W = img.size(dim-3), img.size(dim-2), img.size(dim-1)\n",
        "    nH = int(math.floor((H-patch_size)/stride + 1))\n",
        "    nW = int(math.floor((W-patch_size)/stride + 1))\n",
        "    N = nH*nW\n",
        "\n",
        "    flag = torch.zeros(N).long()\n",
        "    offsets_tmp_vec = torch.zeros(N).long()\n",
        "    #返回的是一个list类型的数据\n",
        "\n",
        "    nonmask_point_idx_all = torch.zeros(N).long()\n",
        "\n",
        "    tmp_non_mask_idx = 0\n",
        "\n",
        "\n",
        "    mask_point_idx_all = torch.zeros(N).long()\n",
        "\n",
        "    tmp_mask_idx = 0\n",
        "    #所有的像素点都浏览一遍\n",
        "    for i in range(N):\n",
        "        h = int(math.floor(i/nW))\n",
        "        w = int(math.floor(i%nW))\n",
        "        # print(h, w)\n",
        "        #截取一个个1×1的小方片\n",
        "        mask_tmp = mask[h*stride:h*stride + patch_size,\n",
        "                        w*stride:w*stride + patch_size]\n",
        "\n",
        "\n",
        "        if torch.sum(mask_tmp) < mask_thred:\n",
        "            nonmask_point_idx_all[tmp_non_mask_idx] = i\n",
        "            tmp_non_mask_idx += 1\n",
        "        else:\n",
        "            mask_point_idx_all[tmp_mask_idx] = i\n",
        "            tmp_mask_idx += 1\n",
        "            flag[i] = 1\n",
        "            offsets_tmp_vec[i] = -1\n",
        "    # print(flag)  #checked\n",
        "    # print(offsets_tmp_vec) # checked\n",
        "\n",
        "    non_mask_num = tmp_non_mask_idx\n",
        "    mask_num = tmp_mask_idx\n",
        "\n",
        "    nonmask_point_idx = nonmask_point_idx_all.narrow(0, 0, non_mask_num)\n",
        "    mask_point_idx=mask_point_idx_all.narrow(0, 0, mask_num)\n",
        "\n",
        "    # get flatten_offsets\n",
        "    flatten_offsets_all = torch.LongTensor(N).zero_()\n",
        "    for i in range(N):\n",
        "        offset_value = torch.sum(offsets_tmp_vec[0:i+1])\n",
        "        if flag[i] == 1:\n",
        "            offset_value = offset_value + 1\n",
        "        # print(i+offset_value)\n",
        "        flatten_offsets_all[i+offset_value] = -offset_value\n",
        "\n",
        "    flatten_offsets = flatten_offsets_all.narrow(0, 0, non_mask_num)\n",
        "\n",
        "    # print('flatten_offsets')\n",
        "    # print(flatten_offsets)   # checked\n",
        "\n",
        "\n",
        "    # print('nonmask_point_idx')\n",
        "    # print(nonmask_point_idx)  #checked\n",
        "\n",
        "    return flag, nonmask_point_idx, flatten_offsets, mask_point_idx\n",
        "\n",
        "\n",
        "# sp_x: LongTensor\n",
        "# sp_y: LongTensor\n",
        "def cal_sps_for_Advanced_Indexing(h, w):\n",
        "    sp_y = torch.arange(0, w).long()\n",
        "    sp_y = torch.cat([sp_y]*h)\n",
        "\n",
        "    lst = []\n",
        "    for i in range(h):\n",
        "        lst.extend([i]*w)\n",
        "    sp_x = torch.from_numpy(np.array(lst))\n",
        "    return sp_x, sp_y\n",
        "\n",
        "\"\"\"\n",
        "def save_image(image_numpy, image_path):\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(image_path)\n",
        "\"\"\"\n",
        "def info(object, spacing=10, collapse=1):\n",
        "    \"\"\"Print methods and doc strings.\n",
        "    Takes module, class, list, dictionary, or string.\"\"\"\n",
        "    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n",
        "    processFunc = collapse and (lambda s: \" \".join(s.split())) or (lambda s: s)\n",
        "    print( \"\\n\".join([\"%s %s\" %\n",
        "                     (method.ljust(spacing),\n",
        "                      processFunc(str(getattr(object, method).__doc__)))\n",
        "                     for method in methodList]) )\n",
        "\n",
        "def varname(p):\n",
        "    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n",
        "        m = re.search(r'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)', line)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "def print_numpy(x, val=True, shp=False):\n",
        "    x = x.astype(np.float64)\n",
        "    if shp:\n",
        "        print('shape,', x.shape)\n",
        "    if val:\n",
        "        x = x.flatten()\n",
        "        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n",
        "            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n",
        "\n",
        "\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "\n",
        "class Selfpatch(object):\n",
        "    def buildAutoencoder(self, target_img, target_img_2, target_img_3, patch_size=1, stride=1):\n",
        "        nDim = 3\n",
        "        assert target_img.dim() == nDim, 'target image must be of dimension 3.'\n",
        "        C = target_img.size(0)\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor if torch.cuda.is_available else torch.Tensor\n",
        "\n",
        "        patches_features = self._extract_patches(target_img, patch_size, stride)\n",
        "        patches_features_f = self._extract_patches(target_img_3, patch_size, stride)\n",
        "\n",
        "        patches_on = self._extract_patches(target_img_2, 1, stride)\n",
        "\n",
        "        return patches_features_f, patches_features, patches_on\n",
        "\n",
        "    def build(self, target_img,  patch_size=5, stride=1):\n",
        "        nDim = 3\n",
        "        assert target_img.dim() == nDim, 'target image must be of dimension 3.'\n",
        "        C = target_img.size(0)\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor if torch.cuda.is_available else torch.Tensor\n",
        "\n",
        "        patches_features = self._extract_patches(target_img, patch_size, stride)\n",
        "\n",
        "        return patches_features\n",
        "\n",
        "    def _build(self, patch_size, stride, C, target_patches, npatches, normalize, interpolate, type):\n",
        "        # for each patch, divide by its L2 norm.\n",
        "        if type == 1:\n",
        "            enc_patches = target_patches.clone()\n",
        "            for i in range(npatches):\n",
        "                enc_patches[i] = enc_patches[i]*(1/(enc_patches[i].norm(2)+1e-8))\n",
        "\n",
        "            conv_enc = nn.Conv2d(npatches, npatches, kernel_size=1, stride=stride, bias=False, groups=npatches)\n",
        "            conv_enc.weight.data = enc_patches\n",
        "            return conv_enc\n",
        "\n",
        "        # normalize is not needed, it doesn't change the result!\n",
        "            if normalize:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            if interpolate:\n",
        "                raise NotImplementedError\n",
        "        else:\n",
        "\n",
        "            conv_dec = nn.ConvTranspose2d(npatches, C, kernel_size=patch_size, stride=stride, bias=False)\n",
        "            conv_dec.weight.data = target_patches\n",
        "            return conv_dec\n",
        "\n",
        "    def _extract_patches(self, img, patch_size, stride):\n",
        "        n_dim = 3\n",
        "        assert img.dim() == n_dim, 'image must be of dimension 3.'\n",
        "        kH, kW = patch_size, patch_size\n",
        "        dH, dW = stride, stride\n",
        "        input_windows = img.unfold(1, kH, dH).unfold(2, kW, dW)\n",
        "        i_1, i_2, i_3, i_4, i_5 = input_windows.size(0), input_windows.size(1), input_windows.size(2), input_windows.size(3), input_windows.size(4)\n",
        "        input_windows = input_windows.permute(1,2,0,3,4).contiguous().view(i_2*i_3, i_1, i_4, i_5)\n",
        "        patches_all = input_windows\n",
        "        return patches_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SE MODEL\n",
        "class SELayer(pl.LightningModule):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // reduction, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // reduction, channel, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c, 1, 1)\n",
        "        y = self.fc(y)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class Convnorm(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, sample='none-3', activ='leaky'):\n",
        "        super().__init__()\n",
        "        self.bn = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "\n",
        "        if sample == 'down-3':\n",
        "            self.conv = nn.Conv2d(in_ch, out_ch, 3, 2, 1, bias=False)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_ch, out_ch, 3, 1)\n",
        "        if activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = input\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        if hasattr(self, 'activation'):\n",
        "            out = self.activation(out[0])\n",
        "        return out\n",
        "\n",
        "\n",
        "class PCBActiv(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=True, sample='none-3', activ='leaky',\n",
        "                 conv_bias=False, innorm=False, inner=False, outer=False):\n",
        "        super().__init__()\n",
        "        if sample == 'same-5':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 5, 1, 2, bias=conv_bias)\n",
        "        elif sample == 'same-7':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 7, 1, 3, bias=conv_bias)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias)\n",
        "        else:\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias)\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.innorm = innorm\n",
        "        self.inner = inner\n",
        "        self.outer = outer\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = input\n",
        "        if self.inner:\n",
        "            out[0] = self.bn(out[0])\n",
        "            out[0] = self.activation(out[0])\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "            out[0] = self.activation(out[0])\n",
        "\n",
        "        elif self.innorm:\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "            out[0] = self.activation(out[0])\n",
        "        elif self.outer:\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "        else:\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "            if hasattr(self, 'activation'):\n",
        "                out[0] = self.activation(out[0])\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvDown(pl.LightningModule):\n",
        "    def __init__(self, in_c, out_c, kernel, stride, padding=0, dilation=1, groups=1, bias=False, layers=1, activ=True):\n",
        "        super().__init__()\n",
        "        nf_mult = 1\n",
        "        nums = out_c / 64\n",
        "        sequence = []\n",
        "\n",
        "        for i in range(1, layers + 1):\n",
        "            nf_mult_prev = nf_mult\n",
        "            if nums == 8:\n",
        "                if in_c == 512:\n",
        "\n",
        "                    nfmult = 1\n",
        "                else:\n",
        "                    nf_mult = 2\n",
        "\n",
        "            else:\n",
        "                nf_mult = min(2 ** i, 8)\n",
        "            if kernel != 1:\n",
        "\n",
        "                if activ == False and layers == 1:\n",
        "                    sequence += [\n",
        "                        nn.Conv2d(nf_mult_prev * in_c, nf_mult * in_c,\n",
        "                                  kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                        nn.InstanceNorm2d(nf_mult * in_c)\n",
        "                    ]\n",
        "                else:\n",
        "                    sequence += [\n",
        "                        nn.Conv2d(nf_mult_prev * in_c, nf_mult * in_c,\n",
        "                                  kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                        nn.InstanceNorm2d(nf_mult * in_c),\n",
        "                        nn.LeakyReLU(0.2, True)\n",
        "                    ]\n",
        "\n",
        "            else:\n",
        "\n",
        "                sequence += [\n",
        "                    nn.Conv2d(in_c, out_c,\n",
        "                              kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                    nn.InstanceNorm2d(out_c),\n",
        "                    nn.LeakyReLU(0.2, True)\n",
        "                ]\n",
        "\n",
        "            if activ == False:\n",
        "                if i + 1 == layers:\n",
        "                    if layers == 2:\n",
        "                        sequence += [\n",
        "                            nn.Conv2d(nf_mult * in_c, nf_mult * in_c,\n",
        "                                      kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                            nn.InstanceNorm2d(nf_mult * in_c)\n",
        "                        ]\n",
        "                    else:\n",
        "                        sequence += [\n",
        "                            nn.Conv2d(nf_mult_prev * in_c, nf_mult * in_c,\n",
        "                                      kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                            nn.InstanceNorm2d(nf_mult * in_c)\n",
        "                        ]\n",
        "                    break\n",
        "\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ConvUp(pl.LightningModule):\n",
        "    def __init__(self, in_c, out_c, kernel, stride, padding=0, dilation=1, groups=1, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_c, out_c, kernel,\n",
        "                              stride, padding, dilation, groups, bias)\n",
        "        self.bn = nn.InstanceNorm2d(out_c)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input, size):\n",
        "        out = F.interpolate(input=input, size=size, mode='bilinear')\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BASE(pl.LightningModule):\n",
        "    def __init__(self, inner_nc):\n",
        "        super(BASE, self).__init__()\n",
        "        se = SELayer(inner_nc, 16)\n",
        "        model = [se]\n",
        "        gus = gussin(1.5).cuda()\n",
        "        self.gus = torch.unsqueeze(gus, 1).double()\n",
        "        self.model = nn.Sequential(*model)\n",
        "        self.down = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, 1, 1, 0, bias=False),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        Nonparm = Selfpatch()\n",
        "        out_32 = self.model(x)\n",
        "        b, c, h, w = out_32.size()\n",
        "        gus = self.gus.float()\n",
        "        gus_out = out_32[0].expand(h * w, c, h, w)\n",
        "        gus_out = gus * gus_out\n",
        "        gus_out = torch.sum(gus_out, -1)\n",
        "        gus_out = torch.sum(gus_out, -1)\n",
        "        gus_out = gus_out.contiguous().view(b, c, h, w)\n",
        "        csa2_in = F.sigmoid(out_32)\n",
        "        csa2_f = torch.nn.functional.pad(csa2_in, (1, 1, 1, 1))\n",
        "        csa2_ff = torch.nn.functional.pad(out_32, (1, 1, 1, 1))\n",
        "        csa2_fff, csa2_f, csa2_conv = Nonparm.buildAutoencoder(csa2_f[0], csa2_in[0], csa2_ff[0], 3, 1)\n",
        "        csa2_conv = csa2_conv.expand_as(csa2_f)\n",
        "        csa_a = csa2_conv * csa2_f\n",
        "        csa_a = torch.mean(csa_a, 1)\n",
        "        a_c, a_h, a_w = csa_a.size()\n",
        "        csa_a = csa_a.contiguous().view(a_c, -1)\n",
        "        csa_a = F.softmax(csa_a, dim=1)\n",
        "        csa_a = csa_a.contiguous().view(a_c, 1, a_h, a_h)\n",
        "        out = csa_a * csa2_fff\n",
        "        out = torch.sum(out, -1)\n",
        "        out = torch.sum(out, -1)\n",
        "        out_csa = out.contiguous().view(b, c, h, w)\n",
        "        out_32 = torch.cat([gus_out, out_csa], 1)\n",
        "        out_32 = self.down(out_32)\n",
        "        return out_32\n",
        "\n",
        "\n",
        "class PartialConv(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        super().__init__()\n",
        "        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                    stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                   stride, padding, dilation, groups, False)\n",
        "\n",
        "        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n",
        "\n",
        "        # mask is not updated\n",
        "        for param in self.mask_conv.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, inputt):\n",
        "        # http://masc.cs.gmu.edu/wiki/partialconv\n",
        "        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n",
        "        # W^T* (M .* X) / sum(M) + b = [C(M .* X) – C(0)] / D(M) + C(0)\n",
        "\n",
        "        input = inputt[0]\n",
        "        mask = inputt[1].float().cuda()\n",
        "\n",
        "        output = self.input_conv(input * mask)\n",
        "        if self.input_conv.bias is not None:\n",
        "            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(\n",
        "                output)\n",
        "        else:\n",
        "            output_bias = torch.zeros_like(output)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_mask = self.mask_conv(mask)\n",
        "\n",
        "        no_update_holes = output_mask == 0\n",
        "        mask_sum = output_mask.masked_fill_(no_update_holes.bool(), 1.0)\n",
        "        output_pre = (output - output_bias) / mask_sum + output_bias\n",
        "        output = output_pre.masked_fill_(no_update_holes.bool(), 0.0)\n",
        "        new_mask = torch.ones_like(output)\n",
        "        new_mask = new_mask.masked_fill_(no_update_holes.bool(), 0.0)\n",
        "        out = []\n",
        "        out.append(output)\n",
        "        out.append(new_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PCconv(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(PCconv, self).__init__()\n",
        "        self.down_128 = ConvDown(64, 128, 4, 2, padding=1, layers=2)\n",
        "        self.down_64 = ConvDown(128, 256, 4, 2, padding=1)\n",
        "        self.down_32 = ConvDown(256, 256, 1, 1)\n",
        "        self.down_16 = ConvDown(512, 512, 4, 2, padding=1, activ=False)\n",
        "        self.down_8 = ConvDown(512, 512, 4, 2, padding=1, layers=2, activ=False)\n",
        "        self.down_4 = ConvDown(512, 512, 4, 2, padding=1, layers=3, activ=False)\n",
        "        self.down = ConvDown(768, 256, 1, 1)\n",
        "        self.fuse = ConvDown(512, 512, 1, 1)\n",
        "        self.up = ConvUp(512, 256, 1, 1)\n",
        "        self.up_128 = ConvUp(512, 64, 1, 1)\n",
        "        self.up_64 = ConvUp(512, 128, 1, 1)\n",
        "        self.up_32 = ConvUp(512, 256, 1, 1)\n",
        "        self.base= BASE(512)\n",
        "        seuqence_3 = []\n",
        "        seuqence_5 = []\n",
        "        seuqence_7 = []\n",
        "        for i in range(5):\n",
        "            seuqence_3 += [PCBActiv(256, 256, innorm=True)]\n",
        "            seuqence_5 += [PCBActiv(256, 256, sample='same-5', innorm=True)]\n",
        "            seuqence_7 += [PCBActiv(256, 256, sample='same-7', innorm=True)]\n",
        "\n",
        "        self.cov_3 = nn.Sequential(*seuqence_3)\n",
        "        self.cov_5 = nn.Sequential(*seuqence_5)\n",
        "        self.cov_7 = nn.Sequential(*seuqence_7)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        mask =  cal_feat_mask(mask, 3, 1)\n",
        "        # input[2]:256 32 32\n",
        "        b, c, h, w = input[2].size()\n",
        "        mask_1 = torch.add(torch.neg(mask.float()), 1)\n",
        "        mask_1 = mask_1.expand(b, c, h, w)\n",
        "\n",
        "        x_1 = self.activation(input[0])\n",
        "        x_2 = self.activation(input[1])\n",
        "        x_3 = self.activation(input[2])\n",
        "        x_4 = self.activation(input[3])\n",
        "        x_5 = self.activation(input[4])\n",
        "        x_6 = self.activation(input[5])\n",
        "        # Change the shape of each layer and intergrate low-level/high-level features\n",
        "        x_1 = self.down_128(x_1)\n",
        "        x_2 = self.down_64(x_2)\n",
        "        x_3 = self.down_32(x_3)\n",
        "        x_4 = self.up(x_4, (32, 32))\n",
        "        x_5 = self.up(x_5, (32, 32))\n",
        "        x_6 = self.up(x_6, (32, 32))\n",
        "\n",
        "        # The first three layers are Texture/detail\n",
        "        # The last three layers are Structure\n",
        "        x_DE = torch.cat([x_1, x_2, x_3], 1)\n",
        "        x_ST = torch.cat([x_4, x_5, x_6], 1)\n",
        "\n",
        "        x_ST = self.down(x_ST)\n",
        "        x_DE = self.down(x_DE)\n",
        "        x_ST = [x_ST, mask_1]\n",
        "        x_DE = [x_DE, mask_1]\n",
        "\n",
        "        # Multi Scale PConv fill the Details\n",
        "        x_DE_3 = self.cov_3(x_DE)\n",
        "        x_DE_5 = self.cov_5(x_DE)\n",
        "        x_DE_7 = self.cov_7(x_DE)\n",
        "        x_DE_fuse = torch.cat([x_DE_3[0], x_DE_5[0], x_DE_7[0]], 1)\n",
        "        x_DE_fi = self.down(x_DE_fuse)\n",
        "\n",
        "        # Multi Scale PConv fill the Structure\n",
        "        x_ST_3 = self.cov_3(x_ST)\n",
        "        x_ST_5 = self.cov_5(x_ST)\n",
        "        x_ST_7 = self.cov_7(x_ST)\n",
        "        x_ST_fuse = torch.cat([x_ST_3[0], x_ST_5[0], x_ST_7[0]], 1)\n",
        "        x_ST_fi = self.down(x_ST_fuse)\n",
        "\n",
        "        x_cat = torch.cat([x_ST_fi, x_DE_fi], 1)\n",
        "        x_cat_fuse = self.fuse(x_cat)\n",
        "\n",
        "        # Feature equalizations\n",
        "        x_final = self.base(x_cat_fuse)\n",
        "\n",
        "        # Add back to the input\n",
        "        x_ST = x_final\n",
        "        x_DE = x_final\n",
        "        x_1 = self.up_128(x_DE, (128, 128)) + input[0]\n",
        "        x_2 = self.up_64(x_DE, (64, 64)) + input[1]\n",
        "        x_3 = self.up_32(x_DE, (32, 32)) + input[2]\n",
        "        x_4 = self.down_16(x_ST) + input[3]\n",
        "        x_5 = self.down_8(x_ST) + input[4]\n",
        "        x_6 = self.down_4(x_ST) + input[5]\n",
        "\n",
        "        out = [x_1, x_2, x_3, x_4, x_5, x_6]\n",
        "        loss = [x_ST_fi, x_DE_fi]\n",
        "        out_final = [out, loss]\n",
        "        return out_final\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Define the resnet block\n",
        "class ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation=1):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=False),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# define the Encoder unit\n",
        "class UnetSkipConnectionEBlock(pl.LightningModule):\n",
        "    def __init__(self, outer_nc, inner_nc, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d,\n",
        "                 use_dropout=False):\n",
        "        super(UnetSkipConnectionEBlock, self).__init__()\n",
        "        downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1)\n",
        "\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        downnorm = norm_layer(inner_nc, affine=True)\n",
        "        if outermost:\n",
        "            down = [downconv]\n",
        "            model = down\n",
        "        elif innermost:\n",
        "            down = [downrelu, downconv]\n",
        "            model = down\n",
        "        else:\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            if use_dropout:\n",
        "                model = down + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class Encoder(pl.LightningModule):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, res_num=4, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # construct unet structure\n",
        "        Encoder_1 = UnetSkipConnectionEBlock(input_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, outermost=True)\n",
        "        Encoder_2 = UnetSkipConnectionEBlock(ngf, ngf * 2, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_3 = UnetSkipConnectionEBlock(ngf * 2, ngf * 4, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_4 = UnetSkipConnectionEBlock(ngf * 4, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_5 = UnetSkipConnectionEBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_6 = UnetSkipConnectionEBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout, innermost=True)\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(res_num):\n",
        "            block = ResnetBlock(ngf * 8, 2)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "        self.Encoder_1 = Encoder_1\n",
        "        self.Encoder_2 = Encoder_2\n",
        "        self.Encoder_3 = Encoder_3\n",
        "        self.Encoder_4 = Encoder_4\n",
        "        self.Encoder_5 = Encoder_5\n",
        "        self.Encoder_6 = Encoder_6\n",
        "\n",
        "    def forward(self, input):\n",
        "        y_1 = self.Encoder_1(input)\n",
        "        y_2 = self.Encoder_2(y_1)\n",
        "        y_3 = self.Encoder_3(y_2)\n",
        "        y_4 = self.Encoder_4(y_3)\n",
        "        y_5 = self.Encoder_5(y_4)\n",
        "        y_6 = self.Encoder_6(y_5)\n",
        "        y_7 = self.middle(y_6)\n",
        "\n",
        "        return y_1, y_2, y_3, y_4, y_5, y_7\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class UnetSkipConnectionDBlock(pl.LightningModule):\n",
        "    def __init__(self, inner_nc, outer_nc, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d,\n",
        "                 use_dropout=False):\n",
        "        super(UnetSkipConnectionDBlock, self).__init__()\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc, affine=True)\n",
        "        upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                    kernel_size=4, stride=2,\n",
        "                                    padding=1)\n",
        "        up = [uprelu, upconv, upnorm]\n",
        "\n",
        "        if outermost:\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = up\n",
        "        elif innermost:\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = up\n",
        "        else:\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class Decoder(pl.LightningModule):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # construct unet structure\n",
        "        Decoder_1 = UnetSkipConnectionDBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout,\n",
        "                                             innermost=True)\n",
        "        Decoder_2 = UnetSkipConnectionDBlock(ngf * 16, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_3 = UnetSkipConnectionDBlock(ngf * 16, ngf * 4, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_4 = UnetSkipConnectionDBlock(ngf * 8, ngf * 2, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_5 = UnetSkipConnectionDBlock(ngf * 4, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_6 = UnetSkipConnectionDBlock(ngf * 2, output_nc, norm_layer=norm_layer, use_dropout=use_dropout, outermost=True)\n",
        "\n",
        "        self.Decoder_1 = Decoder_1\n",
        "        self.Decoder_2 = Decoder_2\n",
        "        self.Decoder_3 = Decoder_3\n",
        "        self.Decoder_4 = Decoder_4\n",
        "        self.Decoder_5 = Decoder_5\n",
        "        self.Decoder_6 = Decoder_6\n",
        "\n",
        "    def forward(self, input_1, input_2, input_3, input_4, input_5, input_6):\n",
        "        y_1 = self.Decoder_1(input_6)\n",
        "        y_2 = self.Decoder_2(torch.cat([y_1, input_5], 1))\n",
        "        y_3 = self.Decoder_3(torch.cat([y_2, input_4], 1))\n",
        "        y_4 = self.Decoder_4(torch.cat([y_3, input_3], 1))\n",
        "        y_5 = self.Decoder_5(torch.cat([y_4, input_2], 1))\n",
        "        y_6 = self.Decoder_6(torch.cat([y_5, input_1], 1))\n",
        "        out = y_6\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PCblock(pl.LightningModule):\n",
        "    def __init__(self, stde_list):\n",
        "        super(PCblock, self).__init__()\n",
        "        self.pc_block = PCconv()\n",
        "        innerloss = InnerCos()\n",
        "        stde_list.append(innerloss)\n",
        "        loss = [innerloss]\n",
        "        self.loss=nn.Sequential(*loss)\n",
        "    def forward(self, input, mask):\n",
        "        out = self.pc_block(input, mask)\n",
        "        out = self.loss(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MEDFEGenerator(pl.LightningModule):\n",
        "    def __init__(self, input_nc=4, output_nc=3, ngf=64,  norm='batch', use_dropout=False, stde_list=[], norm_layer = nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "        self.netEN = Encoder(input_nc=input_nc, output_nc=output_nc, ngf=ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        self.netDE = Decoder(input_nc=input_nc, output_nc=output_nc, ngf=ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        self.netMEDFE = PCblock(stde_list)\n",
        "\n",
        "    def mask_process(self, mask):\n",
        "        mask = mask[0][0]\n",
        "        mask = torch.unsqueeze(mask, 0)\n",
        "        mask = torch.unsqueeze(mask, 1)\n",
        "        mask = mask.byte()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, images, masks):\n",
        "        #masks =torch.cat([masks,masks,masks],1)\n",
        "\n",
        "        fake_p_1, fake_p_2, fake_p_3, fake_p_4, fake_p_5, fake_p_6 = self.netEN(torch.cat([images, masks], 1))\n",
        "        x_out = self.netMEDFE([fake_p_1, fake_p_2, fake_p_3, fake_p_4, fake_p_5, fake_p_6], masks)\n",
        "        self.fake_out = self.netDE(x_out[0], x_out[1], x_out[2], x_out[3], x_out[4], x_out[5])\n",
        "\n",
        "        return self.fake_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIXC_Hs56XLJ",
        "cellView": "form"
      },
      "source": [
        "#@title [RFR_arch.py](https://github.com/jingyuanli001/RFR-Inpainting) (2020)\n",
        "\"\"\"\n",
        "RFRNet.py (18-12-20)\n",
        "https://github.com/jingyuanli001/RFR-Inpainting/blob/master/modules/RFRNet.py\n",
        "\n",
        "partialconv2d.py (18-12-20) # using their partconv2d to avoid dimension errors\n",
        "https://github.com/jingyuanli001/RFR-Inpainting/blob/master/modules/partialconv2d.py\n",
        "\n",
        "Attention.py (18-12-20)\n",
        "https://github.com/jingyuanli001/RFR-Inpainting/blob/master/modules/Attention.py\n",
        "\"\"\"\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class KnowledgeConsistentAttention(pl.LightningModule):\n",
        "    def __init__(self, patch_size = 3, propagate_size = 3, stride = 1):\n",
        "        super(KnowledgeConsistentAttention, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "        self.att_scores_prev = None\n",
        "        self.masks_prev = None\n",
        "        self.ratio = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        bz, nc, h, w = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background\n",
        "        conv_kernels_all = background.view(bz, nc, w * h, 1, 1)\n",
        "        conv_kernels_all = conv_kernels_all.permute(0, 2, 1, 3, 4)\n",
        "        output_tensor = []\n",
        "        att_score = []\n",
        "        for i in range(bz):\n",
        "            feature_map = foreground[i:i+1]\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels**2, [1, 2, 3], keepdim = True)**0.5\n",
        "            conv_kernels = conv_kernels/norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding = self.patch_size//2)\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.avg_pool2d(conv_result, 3, 1, padding = 1)*9\n",
        "            attention_scores = F.softmax(conv_result, dim = 1)\n",
        "            if self.att_scores_prev is not None:\n",
        "                attention_scores = (self.att_scores_prev[i:i+1]*self.masks_prev[i:i+1] + attention_scores * (torch.abs(self.ratio)+1e-7))/(self.masks_prev[i:i+1]+(torch.abs(self.ratio)+1e-7))\n",
        "            att_score.append(attention_scores)\n",
        "            feature_map = F.conv_transpose2d(attention_scores, conv_kernels, stride = 1, padding = self.patch_size//2)\n",
        "            final_output = feature_map\n",
        "            output_tensor.append(final_output)\n",
        "        self.att_scores_prev = torch.cat(att_score, dim = 0).view(bz, h*w, h, w)\n",
        "        self.masks_prev = masks.view(bz, 1, h, w)\n",
        "        return torch.cat(output_tensor, dim = 0)\n",
        "\n",
        "class AttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, inchannel, patch_size_list = [1], propagate_size_list = [3], stride_list = [1]):\n",
        "        assert isinstance(patch_size_list, list), \"patch_size should be a list containing scales, or you should use Contextual Attention to initialize your module\"\n",
        "        assert len(patch_size_list) == len(propagate_size_list) and len(propagate_size_list) == len(stride_list), \"the input_lists should have same lengths\"\n",
        "        super(AttentionModule, self).__init__()\n",
        "\n",
        "        self.att = KnowledgeConsistentAttention(patch_size_list[0], propagate_size_list[0], stride_list[0])\n",
        "        self.num_of_modules = len(patch_size_list)\n",
        "        self.combiner = nn.Conv2d(inchannel * 2, inchannel, kernel_size = 1)\n",
        "\n",
        "    def forward(self, foreground, mask):\n",
        "        outputs = self.att(foreground, mask)\n",
        "        outputs = torch.cat([outputs, foreground],dim = 1)\n",
        "        outputs = self.combiner(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False\n",
        "\n",
        "        self.return_mask = True\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "\n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "\n",
        "        if mask is not None or self.last_size != (input.data.shape[2], input.data.shape[3]):\n",
        "            self.last_size = (input.data.shape[2], input.data.shape[3])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "\n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "            self.update_mask.to(input)\n",
        "            self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class RFRModule(pl.LightningModule):\n",
        "    def __init__(self, layer_size=6, in_channel = 64):\n",
        "        super(RFRModule, self).__init__()\n",
        "        self.freeze_enc_bn = False\n",
        "        self.layer_size = layer_size\n",
        "        for i in range(3):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            out_channel = in_channel * 2\n",
        "            block = [nn.Conv2d(in_channel, out_channel, 3, 2, 1, bias = False),\n",
        "                     nn.BatchNorm2d(out_channel),\n",
        "                     nn.ReLU(inplace = True)]\n",
        "            in_channel = out_channel\n",
        "            setattr(self, name, nn.Sequential(*block))\n",
        "\n",
        "        for i in range(3, 6):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            block = [nn.Conv2d(in_channel, out_channel, 3, 1, 2, dilation = 2, bias = False),\n",
        "                     nn.BatchNorm2d(out_channel),\n",
        "                     nn.ReLU(inplace = True)]\n",
        "            setattr(self, name, nn.Sequential(*block))\n",
        "        self.att = AttentionModule(512)\n",
        "        for i in range(5, 3, -1):\n",
        "            name = 'dec_{:d}'.format(i)\n",
        "            block = [nn.Conv2d(in_channel + in_channel, in_channel, 3, 1, 2, dilation = 2, bias = False),\n",
        "                     nn.BatchNorm2d(in_channel),\n",
        "                     nn.LeakyReLU(0.2, inplace = True)]\n",
        "            setattr(self, name, nn.Sequential(*block))\n",
        "\n",
        "\n",
        "        block = [nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias = False),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.LeakyReLU(0.2, inplace = True)]\n",
        "        self.dec_3 = nn.Sequential(*block)\n",
        "\n",
        "        block = [nn.ConvTranspose2d(768, 256, 4, 2, 1, bias = False),\n",
        "                 nn.BatchNorm2d(256),\n",
        "                 nn.LeakyReLU(0.2, inplace = True)]\n",
        "        self.dec_2 = nn.Sequential(*block)\n",
        "\n",
        "        block = [nn.ConvTranspose2d(384, 64, 4, 2, 1, bias = False),\n",
        "                 nn.BatchNorm2d(64),\n",
        "                 nn.LeakyReLU(0.2, inplace = True)]\n",
        "        self.dec_1 = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "\n",
        "        h_dict['h_0']= input\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            h_dict[h_key] = getattr(self, l_key)(h_dict[h_key_prev])\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h = h_dict[h_key]\n",
        "        for i in range(self.layer_size - 1, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n",
        "            h = getattr(self, dec_l_key)(h)\n",
        "            if i == 3:\n",
        "                h = self.att(h, mask)\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "class RFRNet(pl.LightningModule):\n",
        "    def __init__(self, conv_type):\n",
        "        super(RFRNet, self).__init__()\n",
        "\n",
        "        self.conv_type = conv_type\n",
        "        if self.conv_type == 'partial':\n",
        "          self.conv1 = PartialConv2d(3, 64, 7, 2, 3, multi_channel = True, bias = False)\n",
        "          self.conv2 = PartialConv2d(64, 64, 7, 1, 3, multi_channel = True, bias = False)\n",
        "          self.conv21 = PartialConv2d(64, 64, 7, 1, 3, multi_channel = True, bias = False)\n",
        "          self.conv22 = PartialConv2d(64, 64, 7, 1, 3, multi_channel = True, bias = False)\n",
        "          self.tail1 = PartialConv2d(67, 32, 3, 1, 1, multi_channel = True, bias = False)\n",
        "          # original code uses conv2d\n",
        "          self.out = nn.Conv2d(64,3,3,1,1, bias = False)\n",
        "        elif self.conv_type == 'deform':\n",
        "          self.conv1 = DeformConv2d(3, 64, 7, 2, 3)\n",
        "          self.conv2 = DeformConv2d(64, 64, 7, 1, 3)\n",
        "          self.conv21 = DeformConv2d(64, 64, 7, 1, 3)\n",
        "          self.conv22 = DeformConv2d(64, 64, 7, 1, 3)\n",
        "          self.tail1 = DeformConv2d(67, 32, 3, 1, 1)\n",
        "          # original code uses conv2d\n",
        "          self.out = nn.Conv2d(64,3,3,1,1, bias = False)\n",
        "        else:\n",
        "          print(\"conv_type not found\")\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn20 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.RFRModule = RFRModule()\n",
        "        self.Tconv = nn.ConvTranspose2d(64, 64, 4, 2, 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.tail2 = Bottleneck(32,8)\n",
        "\n",
        "    def forward(self, in_image, mask):\n",
        "        #in_image = torch.cat((in_image, mask), dim=1)\n",
        "        mask =torch.cat([mask,mask,mask],1)\n",
        "        if self.conv_type == 'partial':\n",
        "          x1, m1 = self.conv1(in_image.type(torch.cuda.FloatTensor), mask.type(torch.cuda.FloatTensor))\n",
        "        elif self.conv_type == 'deform':\n",
        "          x1 = self.conv1(in_image)\n",
        "          m1 = self.conv1(mask)\n",
        "\n",
        "        x1 = F.relu(self.bn1(x1), inplace = True)\n",
        "\n",
        "        if self.conv_type == 'partial':\n",
        "          x1, m1 = self.conv2(x1, m1)\n",
        "        elif self.conv_type == 'deform':\n",
        "          x1 = self.conv2(x1)\n",
        "          m1 = self.conv2(m1)\n",
        "\n",
        "        x1 = F.relu(self.bn20(x1), inplace = True)\n",
        "        x2 = x1\n",
        "        x2, m2 = x1, m1\n",
        "        n, c, h, w = x2.size()\n",
        "        feature_group = [x2.view(n, c, 1, h, w)]\n",
        "        mask_group = [m2.view(n, c, 1, h, w)]\n",
        "        self.RFRModule.att.att.att_scores_prev = None\n",
        "        self.RFRModule.att.att.masks_prev = None\n",
        "\n",
        "        for i in range(6):\n",
        "            if self.conv_type == 'partial':\n",
        "              x2, m2 = self.conv21(x2, m2)\n",
        "              x2, m2 = self.conv22(x2, m2)\n",
        "            elif self.conv_type == 'deform':\n",
        "              x2 = self.conv21(x2)\n",
        "              m2 = self.conv21(m2)\n",
        "              x2 = self.conv22(x2)\n",
        "              m2 = self.conv22(m2)\n",
        "\n",
        "            x2 = F.leaky_relu(self.bn2(x2), inplace = True)\n",
        "            x2 = self.RFRModule(x2, m2[:,0:1,:,:])\n",
        "            x2 = x2 * m2\n",
        "            feature_group.append(x2.view(n, c, 1, h, w))\n",
        "            mask_group.append(m2.view(n, c, 1, h, w))\n",
        "        x3 = torch.cat(feature_group, dim = 2)\n",
        "        m3 = torch.cat(mask_group, dim = 2)\n",
        "        amp_vec = m3.mean(dim = 2)\n",
        "        x3 = (x3*m3).mean(dim = 2) /(amp_vec+1e-7)\n",
        "        x3 = x3.view(n, c, h, w)\n",
        "        m3 = m3[:,:,-1,:,:]\n",
        "        x4 = self.Tconv(x3)\n",
        "        x4 = F.leaky_relu(self.bn3(x4), inplace = True)\n",
        "        m4 = F.interpolate(m3, scale_factor = 2)\n",
        "        x5 = torch.cat([in_image, x4], dim = 1)\n",
        "        m5 = torch.cat([mask, m4], dim = 1)\n",
        "\n",
        "        if self.conv_type == 'partial':\n",
        "          x5, _ = self.tail1(x5, m5)\n",
        "        elif self.conv_type == 'deform':\n",
        "          x5 = self.tail1(x5)\n",
        "\n",
        "        x5 = F.leaky_relu(x5, inplace = True)\n",
        "        x6 = self.tail2(x5)\n",
        "        x6 = torch.cat([x5,x6], dim = 1)\n",
        "        output = self.out(x6)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pri4vUtP7V-Y",
        "cellView": "form"
      },
      "source": [
        "#@title [DMFN_arch.py](https://github.com/Zheng222/DMFN) (2020)\n",
        "\"\"\"\n",
        "block.py (18-12-20)\n",
        "https://github.com/Zheng222/DMFN/blob/master/models/block.py\n",
        "\n",
        "architecture.py (18-12-20)\n",
        "https://github.com/Zheng222/DMFN/blob/master/models/architecture.py\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1):\n",
        "    padding = int((kernel_size - 1) / 2) * dilation\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=True, dilation=dilation,\n",
        "                     groups=groups)\n",
        "\n",
        "\n",
        "def _norm(norm_type, nc):\n",
        "    norm_type = norm_type.lower()\n",
        "    if norm_type == 'bn':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "    elif norm_type == 'in':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def _activation(act_type, inplace=True, neg_slope=0.2, n_prelu=1):\n",
        "    act_type = act_type.lower()\n",
        "    if act_type == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act_type == 'lrelu':\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act_type == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "class conv_block(pl.LightningModule):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,\n",
        "                 padding=0, norm='in', activation='relu', pad_type='zero'):\n",
        "        super(conv_block, self).__init__()\n",
        "        if pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        elif pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        if norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(out_nc, affine=False)\n",
        "        elif norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(out_nc, affine=True)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported norm type: {}\".format(norm)\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_nc, out_nc, kernel_size, stride, 0, dilation, groups, bias)  # padding=0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(self.pad(x))\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class upconv_block(pl.LightningModule):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size=3, stride=1, bias=True,\n",
        "                 padding=0, pad_type='zero', norm='none', activation='relu'):\n",
        "        super(upconv_block, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_nc, out_nc, 4, 2, 1)\n",
        "        self.act = _activation('relu')\n",
        "        self.norm = _norm('in', out_nc)\n",
        "\n",
        "        self.conv = conv_block(out_nc, out_nc, kernel_size, stride, bias=bias, padding=padding, pad_type=pad_type,\n",
        "                               norm=norm, activation=activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.norm(self.deconv(x)))\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResBlock_new(pl.LightningModule):\n",
        "    def __init__(self, nc):\n",
        "        super(ResBlock_new, self).__init__()\n",
        "        self.c1 = conv_layer(nc, nc // 4, 3, 1)\n",
        "        self.d1 = conv_layer(nc // 4, nc // 4, 3, 1, 1)  # rate = 1\n",
        "        self.d2 = conv_layer(nc // 4, nc // 4, 3, 1, 2)  # rate = 2\n",
        "        self.d3 = conv_layer(nc // 4, nc // 4, 3, 1, 4)  # rate = 4\n",
        "        self.d4 = conv_layer(nc // 4, nc // 4, 3, 1, 8)  # rate = 8\n",
        "        self.act = _activation('relu')\n",
        "        self.norm = _norm('in', nc)\n",
        "        self.c2 = conv_layer(nc, nc, 3, 1)  # fusion\n",
        "\n",
        "    def forward(self, x):\n",
        "        output1 = self.act(self.norm(self.c1(x)))\n",
        "        d1 = self.d1(output1)\n",
        "        d2 = self.d2(output1)\n",
        "        d3 = self.d3(output1)\n",
        "        d4 = self.d4(output1)\n",
        "\n",
        "        add1 = d1 + d2\n",
        "        add2 = add1 + d3\n",
        "        add3 = add2 + d4\n",
        "        combine = torch.cat([d1, add1, add2, add3], 1)\n",
        "        output2 = self.c2(self.act(self.norm(combine)))\n",
        "        output = x + self.norm(output2)\n",
        "        return output\n",
        "\n",
        "import torch.nn as nn\n",
        "#from . import block as B\n",
        "\n",
        "class InpaintingGenerator(pl.LightningModule):\n",
        "    def __init__(self, in_nc=4, out_nc=3, nf=64, n_res=8, norm='in', activation='relu'):\n",
        "        super(InpaintingGenerator, self).__init__()\n",
        "        self.encoder = nn.Sequential(  # input: [4, 256, 256]\n",
        "            conv_block(in_nc, nf, 5, stride=1, padding=2, norm='none', activation=activation),  # [64, 256, 256]\n",
        "            conv_block(nf, 2 * nf, 3, stride=2, padding=1, norm=norm, activation=activation),  # [128, 128, 128]\n",
        "            conv_block(2 * nf, 2 * nf, 3, stride=1, padding=1, norm=norm, activation=activation),  # [128, 128, 128]\n",
        "            conv_block(2 * nf, 4 * nf, 3, stride=2, padding=1, norm=norm, activation=activation)  # [256, 64, 64]\n",
        "        )\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(n_res):\n",
        "            block = ResBlock_new(4 * nf)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            conv_block(4 * nf, 4 * nf, 3, stride=1, padding=1, norm=norm, activation=activation),  # [256, 64, 64]\n",
        "            upconv_block(4 * nf, 2 * nf, kernel_size=3, stride=1, padding=1, norm=norm, activation='relu'),\n",
        "            # [128, 128, 128]\n",
        "            upconv_block(2 * nf, nf, kernel_size=3, stride=1, padding=1, norm=norm, activation='relu'),\n",
        "            # [64, 256, 256]\n",
        "            conv_block(nf, out_nc, 3, stride=1, padding=1, norm='none', activation='tanh')  # [3, 256, 256]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4eBTBk7otF",
        "cellView": "form"
      },
      "source": [
        "#@title [RN_arch.py](https://github.com/geekyutao/RN/) (2020)\n",
        "\"\"\"\n",
        "networks.py (13-12-20)\n",
        "https://github.com/geekyutao/RN/blob/a3cf1fccc08f22fcf4b336503a8853748720fd67/networks.py\n",
        "\n",
        "rn.py (13-12-20)\n",
        "https://github.com/geekyutao/RN/blob/a3cf1fccc08f22fcf4b336503a8853748720fd67/rn.py\n",
        "\n",
        "module_util.py (15-12-20)\n",
        "https://github.com/geekyutao/RN/blob/a3cf1fccc08f22fcf4b336503a8853748720fd67/module_util.py\n",
        "\"\"\"\n",
        "\n",
        "from torchvision.transforms import *\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "logger = logging.getLogger('base')\n",
        "\n",
        "def rn_initialize_weights(net_l, scale=1):\n",
        "    if not isinstance(net_l, list):\n",
        "        net_l = [net_l]\n",
        "    for net in net_l:\n",
        "        for m in net.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None:\n",
        "                    init.normal_(m.bias, 0.0001)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    init.normal_(m.bias, 0.0001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                try:\n",
        "                    init.constant_(m.weight, 1)\n",
        "                    init.normal_(m.bias, 0.0001)\n",
        "                except:\n",
        "                    print('This layer has no BN parameters:', m)\n",
        "    logger.info('RN Initialization method [kaiming]')\n",
        "\n",
        "\n",
        "\n",
        "class RN_binarylabel(pl.LightningModule):\n",
        "    def __init__(self, feature_channels):\n",
        "        super(RN_binarylabel, self).__init__()\n",
        "        self.bn_norm = nn.BatchNorm2d(feature_channels, affine=False, track_running_stats=False)\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        '''\n",
        "        input:  x: (B,C,M,N), features\n",
        "                label: (B,1,M,N), 1 for foreground regions, 0 for background regions\n",
        "        output: _x: (B,C,M,N)\n",
        "        '''\n",
        "        label = label.detach()\n",
        "\n",
        "        rn_foreground_region = self.rn(x * label, label)\n",
        "\n",
        "        rn_background_region = self.rn(x * (1 - label), 1 - label)\n",
        "\n",
        "        return rn_foreground_region + rn_background_region\n",
        "\n",
        "    def rn(self, region, mask):\n",
        "        '''\n",
        "        input:  region: (B,C,M,N), 0 for surroundings\n",
        "                mask: (B,1,M,N), 1 for target region, 0 for surroundings\n",
        "        output: rn_region: (B,C,M,N)\n",
        "        '''\n",
        "        shape = region.size()\n",
        "\n",
        "        sum = torch.sum(region, dim=[0,2,3])  # (B, C) -> (C)\n",
        "        Sr = torch.sum(mask, dim=[0,2,3])    # (B, 1) -> (1)\n",
        "        Sr[Sr==0] = 1\n",
        "        mu = (sum / Sr)     # (B, C) -> (C)\n",
        "\n",
        "        return self.bn_norm(region + (1 - mask) * mu[None,:,None,None]) * \\\n",
        "        (torch.sqrt(Sr / (shape[0] * shape[2] * shape[3])))[None,:,None,None]\n",
        "\n",
        "class RN_B(pl.LightningModule):\n",
        "    def __init__(self, feature_channels):\n",
        "        super(RN_B, self).__init__()\n",
        "        '''\n",
        "        input: tensor(features) x: (B,C,M,N)\n",
        "               condition Mask: (B,1,H,W): 0 for background, 1 for foreground\n",
        "        return: tensor RN_B(x): (N,C,M,N)\n",
        "        ---------------------------------------\n",
        "        args:\n",
        "            feature_channels: C\n",
        "        '''\n",
        "        # RN\n",
        "        self.rn = RN_binarylabel(feature_channels)    # need no external parameters\n",
        "\n",
        "        # gamma and beta\n",
        "        self.foreground_gamma = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "        self.foreground_beta = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "        self.background_gamma = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "        self.background_beta = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # mask = F.adaptive_max_pool2d(mask, output_size=x.size()[2:])\n",
        "        mask = F.interpolate(mask, size=x.size()[2:], mode='nearest')   # after down-sampling, there can be all-zero mask\n",
        "\n",
        "        rn_x = self.rn(x, mask)\n",
        "\n",
        "        rn_x_foreground = (rn_x * mask) * (1 + self.foreground_gamma[None,:,None,None]) + self.foreground_beta[None,:,None,None]\n",
        "        rn_x_background = (rn_x * (1 - mask)) * (1 + self.background_gamma[None,:,None,None]) + self.background_beta[None,:,None,None]\n",
        "\n",
        "        return rn_x_foreground + rn_x_background\n",
        "\n",
        "class SelfAware_Affine(pl.LightningModule):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SelfAware_Affine, self).__init__()\n",
        "\n",
        "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.gamma_conv = nn.Conv2d(1, 1, kernel_size, padding=padding)\n",
        "        self.beta_conv = nn.Conv2d(1, 1, kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        importance_map = self.sigmoid(x)\n",
        "\n",
        "        gamma = self.gamma_conv(importance_map)\n",
        "        beta = self.beta_conv(importance_map)\n",
        "\n",
        "        return importance_map, gamma, beta\n",
        "\n",
        "class RN_L(pl.LightningModule):\n",
        "    def __init__(self, feature_channels, threshold=0.8):\n",
        "        super(RN_L, self).__init__()\n",
        "        '''\n",
        "        input: tensor(features) x: (B,C,M,N)\n",
        "        return: tensor RN_L(x): (B,C,M,N)\n",
        "        ---------------------------------------\n",
        "        args:\n",
        "            feature_channels: C\n",
        "        '''\n",
        "        # SelfAware_Affine\n",
        "        self.sa = SelfAware_Affine()\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # RN\n",
        "        self.rn = RN_binarylabel(feature_channels)    # need no external parameters\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        sa_map, gamma, beta = self.sa(x)     # (B,1,M,N)\n",
        "\n",
        "        # m = sa_map.detach()\n",
        "        if x.is_cuda:\n",
        "            mask = torch.zeros_like(sa_map).cuda()\n",
        "        else:\n",
        "            mask = torch.zeros_like(sa_map)\n",
        "        mask[sa_map.detach() >= self.threshold] = 1\n",
        "\n",
        "        rn_x = self.rn(x, mask.expand(x.size()))\n",
        "\n",
        "        rn_x = rn_x * (1 + gamma) + beta\n",
        "\n",
        "        return rn_x\n",
        "\n",
        "\n",
        "class G_Net(pl.LightningModule):\n",
        "    def __init__(self, input_channels=3, residual_blocks=8, threshold=0.8):\n",
        "        super(G_Net, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_prePad = nn.ReflectionPad2d(3)\n",
        "        self.encoder_conv1 = nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=7, padding=0)\n",
        "        self.encoder_in1 = RN_B(feature_channels=64)\n",
        "        self.encoder_relu1 = nn.ReLU(True)\n",
        "        self.encoder_conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
        "        self.encoder_in2 = RN_B(feature_channels=128)\n",
        "        self.encoder_relu2 = nn.ReLU(True)\n",
        "        self.encoder_conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
        "        self.encoder_in3 = RN_B(feature_channels=256)\n",
        "        self.encoder_relu3 = nn.ReLU(True)\n",
        "\n",
        "\n",
        "        # Middle\n",
        "        blocks = []\n",
        "        for _ in range(residual_blocks):\n",
        "            # block = ResnetBlock(256, 2, use_spectral_norm=False)\n",
        "            block = saRN_ResnetBlock(256, dilation=2, threshold=threshold, use_spectral_norm=False)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 128*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            RN_L(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(128, 64*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            RN_L(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=input_channels, kernel_size=7, padding=0)\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def encoder(self, x, mask):\n",
        "        # float\n",
        "        x = x.type(torch.cuda.FloatTensor)\n",
        "        mask = mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        # half precision (not working?)\n",
        "        #x = x.type(torch.cuda.HalfTensor)\n",
        "        #mask = mask.type(torch.cuda.HalfTensor)\n",
        "\n",
        "        x = self.encoder_prePad(x)\n",
        "\n",
        "        x = self.encoder_conv1(x)\n",
        "        x = self.encoder_in1(x, mask)\n",
        "        x = self.encoder_relu2(x)\n",
        "\n",
        "        x = self.encoder_conv2(x)\n",
        "        x = self.encoder_in2(x, mask)\n",
        "        x = self.encoder_relu2(x)\n",
        "\n",
        "        x = self.encoder_conv3(x)\n",
        "        x = self.encoder_in3(x, mask)\n",
        "        x = self.encoder_relu3(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #gt = x\n",
        "        #x = (x * (1 - mask).float()) + mask\n",
        "        # input mask: 1 for hole, 0 for valid\n",
        "        x = self.encoder(x, mask)\n",
        "\n",
        "        x = self.middle(x)\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        x = (torch.tanh(x) + 1) / 2\n",
        "        # x = x*mask+gt*(1-mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation=1, use_spectral_norm=True):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=256, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "\n",
        "        # Remove ReLU at the end of the residual block\n",
        "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
        "\n",
        "        return out\n",
        "\n",
        "class saRN_ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation, threshold, use_spectral_norm=True):\n",
        "        super(saRN_ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=256, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            # nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "            RN_L(feature_channels=256, threshold=threshold),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            # nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "            RN_L(feature_channels=dim, threshold=threshold),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        # skimage.io.imsave('block.png', out[0].detach().permute(1,2,0).cpu().numpy()[:,:,0])\n",
        "\n",
        "        # Remove ReLU at the end of the residual block\n",
        "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
        "\n",
        "        return out\n",
        "\n",
        "def spectral_norm(module, mode=True):\n",
        "    if mode:\n",
        "        return nn.utils.spectral_norm(module)\n",
        "\n",
        "    return module\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfjYt7dyOcvQ",
        "cellView": "form"
      },
      "source": [
        "#@title [DFNet_arch.py](https://github.com/hughplay/DFNet) (2019) [no SWA]\n",
        "# https://github.com/hughplay/DFNet\n",
        "# https://github.com/Yukariin/DFNet\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "#from .convolutions import partialconv2d\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "\n",
        "def resize_like(x, target, mode='bilinear'):\n",
        "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
        "\n",
        "\n",
        "def get_norm(name, out_channels):\n",
        "    if name == 'batch':\n",
        "        norm = nn.BatchNorm2d(out_channels)\n",
        "    elif name == 'instance':\n",
        "        norm = nn.InstanceNorm2d(out_channels)\n",
        "    else:\n",
        "        norm = None\n",
        "    return norm\n",
        "\n",
        "\n",
        "def get_activation(name):\n",
        "    if name == 'relu':\n",
        "        activation = nn.ReLU()\n",
        "    elif name == 'elu':\n",
        "        activation == nn.ELU()\n",
        "    elif name == 'leaky_relu':\n",
        "        activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "    elif name == 'tanh':\n",
        "        activation = nn.Tanh()\n",
        "    elif name == 'sigmoid':\n",
        "        activation = nn.Sigmoid()\n",
        "    else:\n",
        "        activation = None\n",
        "    return activation\n",
        "\n",
        "\n",
        "class Conv2dSame(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        padding = self.conv_same_pad(kernel_size, stride)\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          # original\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = nn.Conv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  nn.Conv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "        elif conv_type == 'partial':\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = PartialConv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  PartialConv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "\n",
        "        elif conv_type == 'deform':\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = PartialConv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  DeformConv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "\n",
        "    def conv_same_pad(self, ksize, stride):\n",
        "        if (ksize - stride) % 2 == 0:\n",
        "            return (ksize - stride) // 2\n",
        "        else:\n",
        "            left = (ksize - stride) // 2\n",
        "            right = left + 1\n",
        "            return left, right\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ConvTranspose2dSame(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        padding, output_padding = self.deconv_same_pad(kernel_size, stride)\n",
        "        self.trans_conv = nn.ConvTranspose2d(\n",
        "            in_channels, out_channels, kernel_size, stride,\n",
        "            padding, output_padding)\n",
        "\n",
        "    def deconv_same_pad(self, ksize, stride):\n",
        "        pad = (ksize - stride + 1) // 2\n",
        "        outpad = 2 * pad + stride - ksize\n",
        "        return pad, outpad\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.trans_conv(x)\n",
        "\n",
        "\n",
        "class UpBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, mode='nearest', scale=2, channel=None, kernel_size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        if mode == 'deconv':\n",
        "            self.up = ConvTranspose2dSame(\n",
        "                channel, channel, kernel_size, stride=scale)\n",
        "        else:\n",
        "            def upsample(x):\n",
        "                return F.interpolate(x, scale_factor=scale, mode=mode)\n",
        "            self.up = upsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "\n",
        "class EncodeBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, conv_type, kernel_size, stride,\n",
        "            normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_in = in_channels\n",
        "        self.c_out = out_channels\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Conv2dSame(self.c_in, self.c_out, conv_type, kernel_size, stride))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, self.c_out))\n",
        "        if activation:\n",
        "            layers.append(get_activation(activation))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class DecodeBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, c_from_up, c_from_down, conv_type, c_out, mode='nearest',\n",
        "            kernel_size=4, scale=2, normalization='batch', activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_from_up = c_from_up\n",
        "        self.c_from_down = c_from_down\n",
        "        self.c_in = c_from_up + c_from_down\n",
        "        self.c_out = c_out\n",
        "\n",
        "        self.up = UpBlock(mode, scale, c_from_up, kernel_size=scale)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Conv2dSame(self.c_in, self.c_out, conv_type, kernel_size, stride=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, self.c_out))\n",
        "        if activation:\n",
        "            layers.append(get_activation(activation))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, concat=None):\n",
        "        out = self.up(x)\n",
        "        if self.c_from_down > 0:\n",
        "            out = torch.cat([out, concat], dim=1)\n",
        "        out = self.decode(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BlendBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, c_in, c_out, conv_type, ksize_mid=3, norm='batch', act='leaky_relu'):\n",
        "        super().__init__()\n",
        "        c_mid = max(c_in // 2, 32)\n",
        "        self.blend = nn.Sequential(\n",
        "            Conv2dSame(c_in, c_mid, conv_type, 1, 1),\n",
        "            get_norm(norm, c_mid),\n",
        "            get_activation(act),\n",
        "            Conv2dSame(c_mid, c_out, conv_type, ksize_mid, 1),\n",
        "            get_norm(norm, c_out),\n",
        "            get_activation(act),\n",
        "            Conv2dSame(c_out, c_out, conv_type, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blend(x)\n",
        "\n",
        "\n",
        "class FusionBlock(pl.LightningModule):\n",
        "    def __init__(self, c_feat, conv_type, c_alpha=1):\n",
        "        super().__init__()\n",
        "        c_img = 3\n",
        "        self.map2img = nn.Sequential(\n",
        "            Conv2dSame(c_feat, c_img, conv_type, 1, 1),\n",
        "            nn.Sigmoid())\n",
        "        self.blend = BlendBlock(c_img*2, c_alpha, conv_type)\n",
        "\n",
        "    def forward(self, img_miss, feat_de):\n",
        "        img_miss = resize_like(img_miss, feat_de)\n",
        "        raw = self.map2img(feat_de)\n",
        "        alpha = self.blend(torch.cat([img_miss, raw], dim=1))\n",
        "        result = alpha * raw + (1 - alpha) * img_miss\n",
        "        return result, alpha, raw\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class DFNet(pl.LightningModule):\n",
        "    def __init__(\n",
        "            self, c_img=3, c_mask=1, c_alpha=3,\n",
        "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3]*8,\n",
        "            blend_layers=[0, 1, 2, 3, 4, 5], conv_type = 'normal'):\n",
        "        super().__init__()\n",
        "\n",
        "        c_init = c_img + c_mask\n",
        "\n",
        "        self.n_en = len(en_ksize)\n",
        "        self.n_de = len(de_ksize)\n",
        "        assert self.n_en == self.n_de, (\n",
        "            'The number layer of Encoder and Decoder must be equal.')\n",
        "        assert self.n_en >= 1, (\n",
        "            'The number layer of Encoder and Decoder must be greater than 1.')\n",
        "\n",
        "        assert 0 in blend_layers, 'Layer 0 must be blended.'\n",
        "\n",
        "        self.en = []\n",
        "        c_in = c_init\n",
        "        self.en.append(\n",
        "            EncodeBlock(c_in, 64, conv_type, en_ksize[0], 2, None, None))\n",
        "        for k_en in en_ksize[1:]:\n",
        "            c_in = self.en[-1].c_out\n",
        "            c_out = min(c_in*2, 512)\n",
        "            self.en.append(EncodeBlock(\n",
        "                c_in, c_out, conv_type, k_en, stride=2,\n",
        "                normalization=norm, activation=act_en))\n",
        "\n",
        "        # register parameters\n",
        "        for i, en in enumerate(self.en):\n",
        "            self.__setattr__('en_{}'.format(i), en)\n",
        "\n",
        "        self.de = []\n",
        "        self.fuse = []\n",
        "        for i, k_de in enumerate(de_ksize):\n",
        "\n",
        "            c_from_up = self.en[-1].c_out if i == 0 else self.de[-1].c_out\n",
        "            c_out = c_from_down = self.en[-i-1].c_in\n",
        "            layer_idx = self.n_de - i - 1\n",
        "\n",
        "            self.de.append(DecodeBlock(\n",
        "                c_from_up, c_from_down, conv_type, c_out, mode, k_de, scale=2,\n",
        "                normalization=norm, activation=act_de))\n",
        "            if layer_idx in blend_layers:\n",
        "                self.fuse.append(FusionBlock(c_out, conv_type, c_alpha))\n",
        "            else:\n",
        "                self.fuse.append(None)\n",
        "\n",
        "        # register parameters\n",
        "        for i, de in enumerate(self.de[::-1]):\n",
        "            self.__setattr__('de_{}'.format(i), de)\n",
        "        for i, fuse in enumerate(self.fuse[::-1]):\n",
        "            if fuse:\n",
        "                self.__setattr__('fuse_{}'.format(i), fuse)\n",
        "\n",
        "    def forward(self, img_miss, mask):\n",
        "\n",
        "        out = torch.cat([img_miss, mask], dim=1)\n",
        "        out_en = [out]\n",
        "\n",
        "        for encode in self.en:\n",
        "            out = encode(out)\n",
        "            out_en.append(out)\n",
        "\n",
        "        results = []\n",
        "        for i, (decode, fuse) in enumerate(zip(self.de, self.fuse)):\n",
        "            out = decode(out, out_en[-i-2])\n",
        "            if fuse:\n",
        "                result, alpha, raw = fuse(img_miss, out)\n",
        "                results.append(result)\n",
        "        return results[::-1][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj9OiPPf6enW",
        "cellView": "form"
      },
      "source": [
        "#@title [LBAM_arch.py](https://github.com/Vious/LBAM_Pytorch) (2019)\n",
        "\"\"\"\n",
        "LBAMModel.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/master/models/LBAMModel.py\n",
        "\n",
        "forwardAttentionLayer.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/98c2ae70486f4ba3ab86d4345e586e7841cfe343/models/forwardAttentionLayer.py\n",
        "\n",
        "reverseAttentionLayer.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/98c2ae70486f4ba3ab86d4345e586e7841cfe343/models/reverseAttentionLayer.py\n",
        "\n",
        "ActivationFunction.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/98c2ae70486f4ba3ab86d4345e586e7841cfe343/models/ActivationFunction.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# asymmetric gaussian shaped activation function g_A\n",
        "class GaussActivation(pl.LightningModule):\n",
        "    def __init__(self, a, mu, sigma1, sigma2):\n",
        "        super(GaussActivation, self).__init__()\n",
        "\n",
        "        self.a = Parameter(torch.tensor(a, dtype=torch.float32))\n",
        "        self.mu = Parameter(torch.tensor(mu, dtype=torch.float32))\n",
        "        self.sigma1 = Parameter(torch.tensor(sigma1, dtype=torch.float32))\n",
        "        self.sigma2 = Parameter(torch.tensor(sigma2, dtype=torch.float32))\n",
        "\n",
        "\n",
        "    def forward(self, inputFeatures):\n",
        "\n",
        "        self.a.data = torch.clamp(self.a.data, 1.01, 6.0)\n",
        "        self.mu.data = torch.clamp(self.mu.data, 0.1, 3.0)\n",
        "        self.sigma1.data = torch.clamp(self.sigma1.data, 0.5, 2.0)\n",
        "        self.sigma2.data = torch.clamp(self.sigma2.data, 0.5, 2.0)\n",
        "\n",
        "        lowerThanMu = inputFeatures < self.mu\n",
        "        largerThanMu = inputFeatures >= self.mu\n",
        "\n",
        "        leftValuesActiv = self.a * torch.exp(- self.sigma1 * ( (inputFeatures - self.mu) ** 2 ) )\n",
        "        leftValuesActiv.masked_fill_(largerThanMu, 0.0)\n",
        "\n",
        "        rightValueActiv = 1 + (self.a - 1) * torch.exp(- self.sigma2 * ( (inputFeatures - self.mu) ** 2 ) )\n",
        "        rightValueActiv.masked_fill_(lowerThanMu, 0.0)\n",
        "\n",
        "        output = leftValuesActiv + rightValueActiv\n",
        "\n",
        "        return output\n",
        "\n",
        "# mask updating functions, we recommand using alpha that is larger than 0 and lower than 1.0\n",
        "class MaskUpdate(pl.LightningModule):\n",
        "    def __init__(self, alpha):\n",
        "        super(MaskUpdate, self).__init__()\n",
        "\n",
        "        self.updateFunc = nn.ReLU(True)\n",
        "        #self.alpha = Parameter(torch.tensor(alpha, dtype=torch.float32))\n",
        "        self.alpha = alpha\n",
        "    def forward(self, inputMaskMap):\n",
        "        \"\"\" self.alpha.data = torch.clamp(self.alpha.data, 0.6, 0.8)\n",
        "        print(self.alpha) \"\"\"\n",
        "\n",
        "        return torch.pow(self.updateFunc(inputMaskMap), self.alpha)\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "#from models.ActivationFunction import GaussActivation, MaskUpdate\n",
        "#from models.weightInitial import weights_init\n",
        "\n",
        "\n",
        "# learnable reverse attention conv\n",
        "class ReverseMaskConv(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, kernelSize=4, stride=2,\n",
        "        padding=1, dilation=1, groups=1, convBias=False):\n",
        "        super(ReverseMaskConv, self).__init__()\n",
        "\n",
        "        self.reverseMaskConv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, \\\n",
        "            dilation, groups, bias=convBias)\n",
        "\n",
        "        #self.reverseMaskConv.apply(weights_init())\n",
        "\n",
        "        self.activationFuncG_A = GaussActivation(1.1, 1.0, 0.5, 0.5)\n",
        "        self.updateMask = MaskUpdate(0.8)\n",
        "\n",
        "    def forward(self, inputMasks):\n",
        "        maskFeatures = self.reverseMaskConv(inputMasks)\n",
        "\n",
        "        maskActiv = self.activationFuncG_A(maskFeatures)\n",
        "\n",
        "        maskUpdate = self.updateMask(maskFeatures)\n",
        "\n",
        "        return maskActiv, maskUpdate\n",
        "\n",
        "# learnable reverse attention layer, including features activation and batchnorm\n",
        "class ReverseAttention(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, bn=False, activ='leaky', \\\n",
        "        kernelSize=4, stride=2, padding=1, outPadding=0,dilation=1, groups=1,convBias=False, bnChannels=512):\n",
        "        super(ReverseAttention, self).__init__()\n",
        "\n",
        "        self.conv = nn.ConvTranspose2d(inputChannels, outputChannels, kernel_size=kernelSize, \\\n",
        "            stride=stride, padding=padding, output_padding=outPadding, dilation=dilation, groups=groups,bias=convBias)\n",
        "\n",
        "        #self.conv.apply(weights_init())\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(bnChannels)\n",
        "\n",
        "        if activ == 'leaky':\n",
        "            self.activ = nn.LeakyReLU(0.2, False)\n",
        "        elif activ == 'relu':\n",
        "            self.activ = nn.ReLU()\n",
        "        elif activ == 'sigmoid':\n",
        "            self.activ = nn.Sigmoid()\n",
        "        elif activ == 'tanh':\n",
        "            self.activ = nn.Tanh()\n",
        "        elif activ == 'prelu':\n",
        "            self.activ = nn.PReLU()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, ecFeaturesSkip, dcFeatures, maskFeaturesForAttention):\n",
        "        nextDcFeatures = self.conv(dcFeatures)\n",
        "\n",
        "        # note that encoder features are ahead, it's important tor make forward attention map ahead\n",
        "        # of reverse attention map when concatenate, we do it in the LBAM model forward function\n",
        "        concatFeatures = torch.cat((ecFeaturesSkip, nextDcFeatures), 1)\n",
        "\n",
        "        outputFeatures = concatFeatures * maskFeaturesForAttention\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            outputFeatures = self.bn(outputFeatures)\n",
        "        if hasattr(self, 'activ'):\n",
        "            outputFeatures = self.activ(outputFeatures)\n",
        "\n",
        "        return outputFeatures\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "#from models.ActivationFunction import GaussActivation, MaskUpdate\n",
        "#from models.weightInitial import weights_init\n",
        "\n",
        "# learnable forward attention conv layer\n",
        "class ForwardAttentionLayer(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, kernelSize, stride,\n",
        "        padding, dilation=1, groups=1, bias=False):\n",
        "        super(ForwardAttentionLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, dilation, \\\n",
        "            groups, bias)\n",
        "\n",
        "        if inputChannels == 4:\n",
        "            self.maskConv = nn.Conv2d(3, outputChannels, kernelSize, stride, padding, dilation, \\\n",
        "                groups, bias)\n",
        "        else:\n",
        "            self.maskConv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, \\\n",
        "                dilation, groups, bias)\n",
        "\n",
        "        #self.conv.apply(weights_init())\n",
        "        #self.maskConv.apply(weights_init())\n",
        "\n",
        "        self.activationFuncG_A = GaussActivation(1.1, 2.0, 1.0, 1.0)\n",
        "        self.updateMask = MaskUpdate(0.8)\n",
        "\n",
        "    def forward(self, inputFeatures, inputMasks):\n",
        "        convFeatures = self.conv(inputFeatures)\n",
        "        maskFeatures = self.maskConv(inputMasks)\n",
        "        #convFeatures_skip = convFeatures.clone()\n",
        "\n",
        "        maskActiv = self.activationFuncG_A(maskFeatures)\n",
        "        convOut = convFeatures * maskActiv\n",
        "\n",
        "        maskUpdate = self.updateMask(maskFeatures)\n",
        "\n",
        "        return convOut, maskUpdate, convFeatures, maskActiv\n",
        "\n",
        "# forward attention gather feature activation and batchnorm\n",
        "class ForwardAttention(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, bn=False, sample='down-4', \\\n",
        "        activ='leaky', convBias=False):\n",
        "        super(ForwardAttention, self).__init__()\n",
        "\n",
        "        if sample == 'down-4':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 4, 2, 1, bias=convBias)\n",
        "        elif sample == 'down-5':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 5, 2, 2, bias=convBias)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 7, 2, 3, bias=convBias)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 3, 2, 1, bias=convBias)\n",
        "        else:\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 3, 1, 1, bias=convBias)\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(outputChannels)\n",
        "\n",
        "        if activ == 'leaky':\n",
        "            self.activ = nn.LeakyReLU(0.2, False)\n",
        "        elif activ == 'relu':\n",
        "            self.activ = nn.ReLU()\n",
        "        elif activ == 'sigmoid':\n",
        "            self.activ = nn.Sigmoid()\n",
        "        elif activ == 'tanh':\n",
        "            self.activ = nn.Tanh()\n",
        "        elif activ == 'prelu':\n",
        "            self.activ = nn.PReLU()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, inputFeatures, inputMasks):\n",
        "        features, maskUpdated, convPreF, maskActiv = self.conv(inputFeatures, inputMasks)\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            features = self.bn(features)\n",
        "        if hasattr(self, 'activ'):\n",
        "            features = self.activ(features)\n",
        "\n",
        "        return features, maskUpdated, convPreF, maskActiv\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "#from models.forwardAttentionLayer import ForwardAttention\n",
        "#from models.reverseAttentionLayer import ReverseAttention, ReverseMaskConv\n",
        "#from models.weightInitial import weights_init\n",
        "\n",
        "#VGG16 feature extract\n",
        "class VGG16FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(VGG16FeatureExtractor, self).__init__()\n",
        "        vgg16 = models.vgg16(pretrained=False)\n",
        "        vgg16.load_state_dict(torch.load('./vgg16-397923af.pth'))\n",
        "        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(3):\n",
        "            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        results = [image]\n",
        "        for i in range(3):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "class LBAMModel(pl.LightningModule):\n",
        "    def __init__(self, inputChannels=4, outputChannels=3):\n",
        "        super(LBAMModel, self).__init__()\n",
        "\n",
        "        # default kernel is of size 4X4, stride 2, padding 1,\n",
        "        # and the use of biases are set false in default ReverseAttention class.\n",
        "        self.ec1 = ForwardAttention(inputChannels, 64, bn=False)\n",
        "        self.ec2 = ForwardAttention(64, 128)\n",
        "        self.ec3 = ForwardAttention(128, 256)\n",
        "        self.ec4 = ForwardAttention(256, 512)\n",
        "\n",
        "        for i in range(5, 8):\n",
        "            name = 'ec{:d}'.format(i)\n",
        "            setattr(self, name, ForwardAttention(512, 512))\n",
        "\n",
        "        # reverse mask conv\n",
        "        self.reverseConv1 = ReverseMaskConv(3, 64)\n",
        "        self.reverseConv2 = ReverseMaskConv(64, 128)\n",
        "        self.reverseConv3 = ReverseMaskConv(128, 256)\n",
        "        self.reverseConv4 = ReverseMaskConv(256, 512)\n",
        "        self.reverseConv5 = ReverseMaskConv(512, 512)\n",
        "        self.reverseConv6 = ReverseMaskConv(512, 512)\n",
        "\n",
        "        self.dc1 = ReverseAttention(512, 512, bnChannels=1024)\n",
        "        self.dc2 = ReverseAttention(512 * 2, 512, bnChannels=1024)\n",
        "        self.dc3 = ReverseAttention(512 * 2, 512, bnChannels=1024)\n",
        "        self.dc4 = ReverseAttention(512 * 2, 256, bnChannels=512)\n",
        "        self.dc5 = ReverseAttention(256 * 2, 128, bnChannels=256)\n",
        "        self.dc6 = ReverseAttention(128 * 2, 64, bnChannels=128)\n",
        "        self.dc7 = nn.ConvTranspose2d(64 * 2, outputChannels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, inputImgs, masks):\n",
        "        inputImgs = torch.cat((inputImgs, masks), 1).type(torch.cuda.FloatTensor)\n",
        "        masks = torch.cat([masks,masks,masks],1).type(torch.cuda.FloatTensor)\n",
        "\n",
        "        ef1, mu1, skipConnect1, forwardMap1 = self.ec1(inputImgs, masks)\n",
        "        ef2, mu2, skipConnect2, forwardMap2 = self.ec2(ef1, mu1)\n",
        "        ef3, mu3, skipConnect3, forwardMap3 = self.ec3(ef2, mu2)\n",
        "        ef4, mu4, skipConnect4, forwardMap4 = self.ec4(ef3, mu3)\n",
        "        ef5, mu5, skipConnect5, forwardMap5 = self.ec5(ef4, mu4)\n",
        "        ef6, mu6, skipConnect6, forwardMap6 = self.ec6(ef5, mu5)\n",
        "        ef7, _, _, _ = self.ec7(ef6, mu6)\n",
        "\n",
        "\n",
        "        reverseMap1, revMu1 = self.reverseConv1(1 - masks)\n",
        "        reverseMap2, revMu2 = self.reverseConv2(revMu1)\n",
        "        reverseMap3, revMu3 = self.reverseConv3(revMu2)\n",
        "        reverseMap4, revMu4 = self.reverseConv4(revMu3)\n",
        "        reverseMap5, revMu5 = self.reverseConv5(revMu4)\n",
        "        reverseMap6, _ = self.reverseConv6(revMu5)\n",
        "\n",
        "        concatMap6 = torch.cat((forwardMap6, reverseMap6), 1)\n",
        "        dcFeatures1 = self.dc1(skipConnect6, ef7, concatMap6)\n",
        "\n",
        "        concatMap5 = torch.cat((forwardMap5, reverseMap5), 1)\n",
        "        dcFeatures2 = self.dc2(skipConnect5, dcFeatures1, concatMap5)\n",
        "\n",
        "        concatMap4 = torch.cat((forwardMap4, reverseMap4), 1)\n",
        "        dcFeatures3 = self.dc3(skipConnect4, dcFeatures2, concatMap4)\n",
        "\n",
        "        concatMap3 = torch.cat((forwardMap3, reverseMap3), 1)\n",
        "        dcFeatures4 = self.dc4(skipConnect3, dcFeatures3, concatMap3)\n",
        "\n",
        "        concatMap2 = torch.cat((forwardMap2, reverseMap2), 1)\n",
        "        dcFeatures5 = self.dc5(skipConnect2, dcFeatures4, concatMap2)\n",
        "\n",
        "        concatMap1 = torch.cat((forwardMap1, reverseMap1), 1)\n",
        "        dcFeatures6 = self.dc6(skipConnect1, dcFeatures5, concatMap1)\n",
        "\n",
        "        dcFeatures7 = self.dc7(dcFeatures6)\n",
        "\n",
        "        output = (self.tanh(dcFeatures7) + 1) / 2\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LZk4iSggfC7e"
      },
      "source": [
        "#@title [Adaptive_arch.py](https://github.com/GuardSkill/AdaptiveGAN) (2019)\n",
        "\"\"\"\n",
        "blocks.py (13-12-20)\n",
        "https://github.com/GuardSkill/AdaptiveGAN/blob/429311f6d22948904429ff1c19b0d953bc26ba81/src/blocks.py\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class hswish(pl.LightningModule):\n",
        "    def forward(self, x):\n",
        "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class hsigmoid(pl.LightningModule):\n",
        "    def forward(self, x):\n",
        "        out = F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeModule(pl.LightningModule):\n",
        "    def __init__(self, in_size, reduction=4):\n",
        "        super(SeModule, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(in_size // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(in_size),\n",
        "            hsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "\n",
        "# class BottleneckBlock(pl.LightningModule):\n",
        "#     expansion = 2\n",
        "#\n",
        "#     def __init__(self, in_channels, out_channels, stride=1, dilation=1, use_spectral_norm=False, downsample=None):\n",
        "#         super(BottleneckBlock, self).__init__()\n",
        "#         self.downsample = downsample\n",
        "#         self.stride = stride\n",
        "#         self.conv_block = nn.Sequential(\n",
        "#             nn.ZeroPad2d(dilation),\n",
        "#             spectral_norm(\n",
        "#                 nn.Conv2d(in_channels=in_channels, out_channels=out_channels * 2, kernel_size=3, stride=stride,\n",
        "#                           padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "#             # nn.LeakyReLU(0.2, inplace=False),\n",
        "#             nn.Tanh(),\n",
        "#             nn.ZeroPad2d(1),\n",
        "#             spectral_norm(\n",
        "#                 nn.Conv2d(in_channels=out_channels * 2, out_channels=out_channels, kernel_size=3, stride=stride,\n",
        "#                           padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "#         )\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         residual = x\n",
        "#         if self.downsample is not None:\n",
        "#             residual = self.downsample(x)\n",
        "#         out = self.conv_block(x) + residual\n",
        "#         # out = nn.LeakyReLU(0.2, inplace=False)(out)\n",
        "#         out = nn.Tanh()(out)\n",
        "#         return out\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "    '''expand + depthwise + pointwise'''\n",
        "\n",
        "    def __init__(self, kernel_size, in_size, expand_size, out_size, stride, dilation=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.se = SeModule(out_size)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.nolinear1 = nn.Tanh()\n",
        "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride,\n",
        "                               padding=(kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2, dilation=dilation,\n",
        "                               groups=expand_size, bias=False)\n",
        "        self.nolinear2 = nn.Tanh()\n",
        "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_size != out_size:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.nolinear1(self.conv1(x))\n",
        "        out = self.nolinear2(self.conv2(out))\n",
        "        out = self.conv3(out)\n",
        "        if self.se != None:\n",
        "            out = self.se(out)\n",
        "        out = out + self.shortcut(x) if self.stride == 1 else out\n",
        "        return out\n",
        "\n",
        "\n",
        "class LinkNet(pl.LightningModule):\n",
        "    def __init__(self, in_channels=3, residual_blocks=1, init_weights=True):\n",
        "        super(LinkNet, self).__init__()\n",
        "        self.conv1 =    nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        # self.conv1 =Block(3, 3, 8, 16, 2)\n",
        "        self.block1 = nn.Sequential(\n",
        "            *[Block(3, 16, 16, 16, 1) for i in range(residual_blocks)]\n",
        "            # residual_blocks1   kernel  input  expand output strike dilation\n",
        "        )\n",
        "        #  out 16\n",
        "        self.conv2 = Block(3, 16, 64, 24, 2)\n",
        "        self.block2 = nn.Sequential(\n",
        "            *[Block(3, 24, 72, 24, 1) for _ in range(residual_blocks * 2)]  # residual_blocks1\n",
        "        )\n",
        "        #  out 24\n",
        "\n",
        "        self.conv3 = Block(5, 24, 72, 40, 2)\n",
        "        self.block3 = nn.Sequential(\n",
        "            *[Block(5, 40, 120, 40, 1) for _ in range(residual_blocks * 3)]  # residual_blocks2\n",
        "        )\n",
        "        #  out 40\n",
        "\n",
        "        self.conv4 = Block(3, 40, 240, 80, 2, 1)\n",
        "        self.block4 = nn.Sequential(\n",
        "            *[Block(3, 80, 200, 80, 1, 4) for _ in range(residual_blocks * 4)]  # residual_blocks3\n",
        "        )\n",
        "        #  out 80\n",
        "\n",
        "        self.conv5 = Block(5, 80, 480, 160, 2)\n",
        "        self.block5 = nn.Sequential(\n",
        "            *[Block(5, 160, 672, 160, 1, 4) for _ in range(residual_blocks * 4)]\n",
        "        )\n",
        "        #  out 160\n",
        "\n",
        "        # self.up1 = nn.Sequential(\n",
        "        #     nn.Conv2d(16, 16, 1, 1, 0, bias=True),\n",
        "        #     nn.Tanh(),\n",
        "        #     # nn.Upsample(scale_factor=2 << 0, mode='bilinear')\n",
        "        # )\n",
        "\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Conv2d(24, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 0, mode='bilinear')\n",
        "        )\n",
        "\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Conv2d(40, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 1, mode='bilinear')\n",
        "        )\n",
        "        self.up4 = nn.Sequential(\n",
        "            nn.Conv2d(80, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 2, mode='bilinear')\n",
        "        )\n",
        "\n",
        "        self.up5 = nn.Sequential(\n",
        "            nn.Conv2d(160, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 3, mode='bilinear')\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            *[Block(5, 80, 160, 80, 1) for _ in range(residual_blocks * 4)],  # 3x3 original:residual_blocks*2\n",
        "            Block(5, 80, 48, 32, 1)\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.block_fusion = nn.Sequential(\n",
        "            *[Block(5, 32, 48, 32, 1) for _ in range(residual_blocks * 4)]  # 3x3 original:residual_blocks*2\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "        self.final = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # self.final =Block(5, 32, 48, 3,1)      #kernel_size, in_size, expand_size, out_size, stride\n",
        "        #  out 160\n",
        "\n",
        "    #     self.init_params()\n",
        "    #\n",
        "    # def init_params(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Conv2d):\n",
        "    #             init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.BatchNorm2d):\n",
        "    #             init.constant_(m.weight, 1)\n",
        "    #             init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.Linear):\n",
        "    #             init.normal_(m.weight, std=0.001)\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block1(self.conv1(x))\n",
        "        x2 = self.block2(self.conv2(x1))\n",
        "        x3 = self.block3(self.conv3(x2))\n",
        "        x4 = self.block4(self.conv4(x3))\n",
        "        x5 = self.block5(self.conv5(x4))\n",
        "        # x=x1+self.up2(x2)+self.up3(x3)+self.up4(x4)+self.up5(x5)\n",
        "        x = torch.cat([x1, self.up2(x2), self.up3(x3), self.up4(x4), self.up5(x5)], 1)\n",
        "        x = self.block_fusion(self.fusion(x))\n",
        "        x = self.final(x)\n",
        "        out = (torch.tanh(x) + 1) / 2\n",
        "        return out\n",
        "\n",
        "\n",
        "class PyramidNet(pl.LightningModule):\n",
        "    def __init__(self, in_channels=3, residual_blocks=1, init_weights=True):\n",
        "        super(PyramidNet, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            *[Block(3, 16, 16, 16, 1) for i in range(residual_blocks)]\n",
        "            # residual_blocks1   kernel  input  expand output strike dilation\n",
        "        )\n",
        "        #  out 16\n",
        "        self.conv2 = Block(3, 16, 64, 24, 2)\n",
        "        self.block2 = nn.Sequential(\n",
        "            *[Block(3, 24, 72, 24, 1) for _ in range(residual_blocks * 2)]  # residual_blocks1\n",
        "        )\n",
        "        #  out 24\n",
        "\n",
        "        self.conv3 = Block(5, 24, 72, 40, 2)\n",
        "        self.block3 = nn.Sequential(\n",
        "            *[Block(5, 40, 120, 40, 1) for _ in range(residual_blocks * 3)]  # residual_blocks2\n",
        "        )\n",
        "        #  out 40\n",
        "\n",
        "        self.conv4 = Block(3, 40, 240, 80, 2, 2)\n",
        "        self.block4 = nn.Sequential(\n",
        "            *[Block(3, 80, 200, 80, 1, 4) for _ in range(residual_blocks * 4)]  # residual_blocks3\n",
        "        )\n",
        "        #  out 80\n",
        "\n",
        "        self.conv5 = Block(5, 80, 480, 160, 2)\n",
        "        self.block5 = nn.Sequential(\n",
        "            *[Block(5, 160, 672, 160, 1, 4) for _ in range(residual_blocks * 4)]\n",
        "        )\n",
        "        #  out 160\n",
        "\n",
        "        # self.up1 = nn.Sequential(\n",
        "        #     nn.Conv2d(16, 16, 1, 1, 0, bias=True),\n",
        "        #     nn.Tanh(),\n",
        "        #     # nn.Upsample(scale_factor=2 << 0, mode='bilinear')\n",
        "        # )\n",
        "\n",
        "        self.channel_reduce4 = nn.Sequential(\n",
        "            nn.Conv2d(160, 80, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.channel_reduce3 = nn.Sequential(\n",
        "            nn.Conv2d(80, 40, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.channel_reduce2 = nn.Sequential(\n",
        "            nn.Conv2d(40, 24, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.channel_reduce1 = nn.Sequential(\n",
        "            nn.Conv2d(24, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.smooth4 = nn.Conv2d(80, 80, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth3 = nn.Conv2d(40, 40, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth2 = nn.Conv2d(24, 24, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth1 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            *[Block(5, 80, 160, 80, 1) for _ in range(residual_blocks * 4)],  # 3x3 original:residual_blocks*2\n",
        "            Block(5, 80, 48, 32, 1)\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.block_fusion = nn.Sequential(\n",
        "            *[Block(5, 32, 48, 32, 1) for _ in range(residual_blocks * 4)]  # 3x3 original:residual_blocks*2\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "        self.final = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # self.final =Block(5, 32, 48, 3,1)      #kernel_size, in_size, expand_size, out_size, stride\n",
        "        #  out 160\n",
        "\n",
        "    #     self.init_params()\n",
        "    #\n",
        "    # def init_params(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Conv2d):\n",
        "    #             init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.BatchNorm2d):\n",
        "    #             init.constant_(m.weight, 1)\n",
        "    #             init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.Linear):\n",
        "    #             init.normal_(m.weight, std=0.001)\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, images, masks):\n",
        "        x = torch.cat((images, masks), dim=1)\n",
        "        x1 = self.block1(self.conv1(x))\n",
        "        x2 = self.block2(self.conv2(x1))\n",
        "        x3 = self.block3(self.conv3(x2))\n",
        "        x4 = self.block4(self.conv4(x3))\n",
        "        x5 = self.block5(self.conv5(x4))\n",
        "        c4=self.smooth4(self._upsample_add( self.channel_reduce4(x5),x4))\n",
        "        c3=self.smooth3(self._upsample_add( self.channel_reduce3(c4),x3))\n",
        "        c2=self.smooth2(self._upsample_add( self.channel_reduce2(c3),x2))\n",
        "        c1=self.smooth1(self._upsample_add( self.channel_reduce1(c2),x1))\n",
        "        x = self.final(c1)\n",
        "        out = (torch.tanh(x) + 1) / 2\n",
        "        return out\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        '''Upsample and add two feature maps.\n",
        "        Args:\n",
        "          x: (Variable) top feature map to be upsampled.\n",
        "          y: (Variable) lateral feature map.\n",
        "        Returns:\n",
        "          (Variable) added feature map.\n",
        "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
        "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
        "        maybe not equal to the lateral feature map size.\n",
        "        e.g.\n",
        "        original input size: [N,_,15,15] ->\n",
        "        conv2d feature map size: [N,_,8,8] ->\n",
        "        upsampled feature map size: [N,_,16,16]\n",
        "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
        "        '''\n",
        "        _, _, H, W = y.size()\n",
        "        return F.upsample(x, size=(H, W), mode='bilinear') + y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxdARh5M7fHN",
        "cellView": "form"
      },
      "source": [
        "#@title [partial_arch.py](https://github.com/jacobaustin123/pytorch-inpainting-partial-conv) (2018)\n",
        "\"\"\"\n",
        "model.py (24-12-20)\n",
        "https://github.com/jacobaustin123/pytorch-inpainting-partial-conv/blob/master/model.py\n",
        "\"\"\"\n",
        "\n",
        "from torch import nn, cuda\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from .convolutions import partialconv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class PartialLayer(pl.LightningModule):\n",
        "    def __init__(self, in_size, out_size, kernel_size, stride, non_linearity=\"relu\", bn=True, multi_channel=False):\n",
        "        super(PartialLayer, self).__init__()\n",
        "\n",
        "        self.conv = PartialConv2d(in_size, out_size, kernel_size, stride, return_mask=True, padding=(kernel_size - 1) // 2, multi_channel=multi_channel, bias=not bn)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_size) if bn else None\n",
        "\n",
        "        if non_linearity == \"relu\":\n",
        "            self.non_linearity = nn.ReLU()\n",
        "        elif non_linearity == \"leaky\":\n",
        "           self.non_linearity = nn.LeakyReLU(negative_slope=0.2)\n",
        "        elif non_linearity == 'sigmoid':\n",
        "            self.non_linearity = nn.Sigmoid()\n",
        "        elif non_linearity == 'tanh':\n",
        "            self.non_linearity = nn.Tanh()\n",
        "        elif non_linearity is None:\n",
        "            self.non_linearity = None\n",
        "        else:\n",
        "            raise ValueError(\"unexpected value for non_linearity\")\n",
        "\n",
        "    def forward(self, x, mask_in=None, return_mask=True):\n",
        "        x, mask = self.conv(x, mask_in=mask_in)\n",
        "\n",
        "        if self.bn:\n",
        "            x = self.bn(x)\n",
        "\n",
        "        if self.non_linearity:\n",
        "            x = self.non_linearity(x)\n",
        "\n",
        "        if return_mask:\n",
        "            return x, mask\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "class Model(pl.LightningModule):\n",
        "    def __init__(self, freeze_bn=False):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.freeze_bn = freeze_bn # freeze bn layers for fine tuning\n",
        "\n",
        "        self.conv1 = PartialLayer(3, 64, 7, 2) # encoder for UNET,  use relu for encoder\n",
        "        self.conv2 = PartialLayer(64, 128, 5, 2)\n",
        "        self.conv3 = PartialLayer(128, 256, 5, 2)\n",
        "        self.conv4 = PartialLayer(256, 512, 3, 2)\n",
        "        self.conv5 = PartialLayer(512, 512, 3, 2)\n",
        "        self.conv6 = PartialLayer(512, 512, 3, 2)\n",
        "        self.conv7 = PartialLayer(512, 512, 3, 2)\n",
        "        self.conv8 = PartialLayer(512, 512, 3, 2)\n",
        "\n",
        "        self.conv9 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True) # decoder for UNET\n",
        "        self.conv10 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv11 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv12 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv13 = PartialLayer(512 + 256, 256, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv14 = PartialLayer(256 + 128, 128, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv15 = PartialLayer(128 + 64, 64, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv16 = PartialLayer(64 + 3, 3, 3, 1, non_linearity=\"tanh\", bn=False, multi_channel=True)\n",
        "    def concat(self, input, prev):\n",
        "        return torch.cat([F.interpolate(input, scale_factor=2), prev], dim=1)\n",
        "\n",
        "    def repeat(self, mask, size1, size2):\n",
        "        return torch.cat([mask[:,0].unsqueeze(1).repeat(1, size1, 1, 1), mask[:,1].unsqueeze(1).repeat(1, size2, 1, 1)], dim=1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x1, mask1 = self.conv1(x.type(torch.cuda.FloatTensor), mask_in=mask.type(torch.cuda.FloatTensor))\n",
        "        x2, mask2 = self.conv2(x1, mask_in=mask1)\n",
        "        x3, mask3 = self.conv3(x2, mask_in=mask2)\n",
        "        x4, mask4 = self.conv4(x3, mask_in=mask3)\n",
        "        x5, mask5 = self.conv5(x4, mask_in=mask4)\n",
        "        x6, mask6 = self.conv6(x5, mask_in=mask5)\n",
        "        x7, mask7 = self.conv7(x6, mask_in=mask6)\n",
        "        x8, mask8 = self.conv8(x7, mask_in=mask7)\n",
        "\n",
        "        x9, mask9 = self.conv9(self.concat(x8, x7), mask_in=self.repeat(self.concat(mask8, mask7), 512, 512))\n",
        "        x10, mask10 = self.conv10(self.concat(x9, x6), mask_in=self.repeat(self.concat(mask9, mask6), 512, 512))\n",
        "        x11, mask11 = self.conv11(self.concat(x10, x5), mask_in=self.repeat(self.concat(mask10, mask5), 512, 512))\n",
        "        x12, mask12 = self.conv12(self.concat(x11, x4), mask_in=self.repeat(self.concat(mask11, mask4), 512, 512))\n",
        "        x13, mask13 = self.conv13(self.concat(x12, x3), mask_in=self.repeat(self.concat(mask12, mask3), 512, 256))\n",
        "        x14, mask14 = self.conv14(self.concat(x13, x2), mask_in=self.repeat(self.concat(mask13, mask2), 256, 128))\n",
        "        x15, mask15 = self.conv15(self.concat(x14, x1), mask_in=self.repeat(self.concat(mask14, mask1), 128, 64))\n",
        "        out, mask16 = self.conv16(self.concat(x15, x), mask_in=self.repeat(self.concat(mask15, mask), 64, 3))\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fosumm9E7xkK"
      },
      "source": [
        "Here are some generators with two outputs. If you decide to use one of them, then make sure you calculate the first stage loss inside ``CustomTrainClass``. Custom combinations are possible, but currently there is mostly just l1 for the first stage / other image.\n",
        "\n",
        "With two outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls5OpYV08KL-",
        "cellView": "form"
      },
      "source": [
        "#@title [deepfillv2_arch.py](https://github.com/zhaoyuzhi/deepfillv2) (2019)\n",
        "\"\"\"\n",
        "network.py (15-12-20)\n",
        "https://github.com/zhaoyuzhi/deepfillv2/blob/master/deepfillv2/network.py\n",
        "\n",
        "network_module.py (15-12-20)\n",
        "https://github.com/zhaoyuzhi/deepfillv2/blob/master/deepfillv2/network_module.py\n",
        "\"\"\"\n",
        "\n",
        "#from network_module import *\n",
        "#from .convolutions import partialconv2d\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import functional as F\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "logger = logging.getLogger('base')\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                Normal ConvBlock\n",
        "#-----------------------------------------------\n",
        "class Conv2dLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):\n",
        "        super(Conv2dLayer, self).__init__()\n",
        "        # Initialize the padding scheme\n",
        "        if pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        elif pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        # Initialize the normalization type\n",
        "        if norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        elif norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        elif norm == 'ln':\n",
        "            self.norm = LayerNorm(out_channels)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
        "\n",
        "        # Initialize the activation funtion\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2, inplace = True)\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU(inplace = True)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        # Initialize the convolution layers\n",
        "        if sn:\n",
        "            print(\"sn\")\n",
        "            self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
        "        else:\n",
        "            if conv_type == 'normal':\n",
        "              self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            elif conv_type == 'partial':\n",
        "              self.conv2d = PartialConv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            else:\n",
        "              print(\"conv_type not implemented\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pad(x)\n",
        "        x = self.conv2d(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class TransposeConv2dLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False, scale_factor = 2):\n",
        "        super(TransposeConv2dLayer, self).__init__()\n",
        "        # Initialize the conv scheme\n",
        "        self.scale_factor = scale_factor\n",
        "        self.conv2d = Conv2dLayer(in_channels, out_channels, conv_type, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')\n",
        "        x = self.conv2d(x)\n",
        "        return x\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                Gated ConvBlock\n",
        "#-----------------------------------------------\n",
        "class GatedConv2d(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'reflect', activation = 'lrelu', norm = 'none', sn = False):\n",
        "        super(GatedConv2d, self).__init__()\n",
        "        # Initialize the padding scheme\n",
        "        if pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        elif pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        # Initialize the normalization type\n",
        "        if norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        elif norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        elif norm == 'ln':\n",
        "            self.norm = LayerNorm(out_channels)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
        "\n",
        "        # Initialize the activation funtion\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2, inplace = True)\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU(inplace = True)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        # Initialize the convolution layers\n",
        "        if sn:\n",
        "            self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
        "            self.mask_conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
        "        else:\n",
        "            if conv_type == 'normal':\n",
        "              self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "              self.mask_conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            elif conv_type == 'partial':\n",
        "              self.conv2d = PartialConv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "              self.mask_conv2d = PartialConv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            else:\n",
        "              print(\"conv_type not implemented\")\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pad(x)\n",
        "        conv = self.conv2d(x)\n",
        "        mask = self.mask_conv2d(x)\n",
        "        gated_mask = self.sigmoid(mask)\n",
        "        x = conv * gated_mask\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class TransposeGatedConv2d(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = True, scale_factor = 2):\n",
        "        super(TransposeGatedConv2d, self).__init__()\n",
        "        # Initialize the conv scheme\n",
        "        self.scale_factor = scale_factor\n",
        "        self.gated_conv2d = GatedConv2d(in_channels, out_channels, conv_type, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')\n",
        "        x = self.gated_conv2d(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------\n",
        "#               Layer Norm\n",
        "# ----------------------------------------\n",
        "class LayerNorm(pl.LightningModule):\n",
        "    def __init__(self, num_features, eps = 1e-8, affine = True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.affine = affine\n",
        "        self.eps = eps\n",
        "\n",
        "        if self.affine:\n",
        "            self.gamma = Parameter(torch.Tensor(num_features).uniform_())\n",
        "            self.beta = Parameter(torch.zeros(num_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # layer norm\n",
        "        shape = [-1] + [1] * (x.dim() - 1)                                  # for 4d input: [-1, 1, 1, 1]\n",
        "        if x.size(0) == 1:\n",
        "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
        "            mean = x.view(-1).mean().view(*shape)\n",
        "            std = x.view(-1).std().view(*shape)\n",
        "        else:\n",
        "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
        "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
        "        x = (x - mean) / (std + self.eps)\n",
        "        # if it is learnable\n",
        "        if self.affine:\n",
        "            shape = [1, -1] + [1] * (x.dim() - 2)                          # for 4d input: [1, -1, 1, 1]\n",
        "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
        "        return x\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                  SpectralNorm\n",
        "#-----------------------------------------------\n",
        "def l2normalize(v, eps = 1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "def deepfillv2_weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    logger.info('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                   Generator\n",
        "#-----------------------------------------------\n",
        "# Input: masked image + mask\n",
        "# Output: filled image\n",
        "\n",
        "#https://github.com/zhaoyuzhi/deepfillv2/blob/62dad2c601400e14d79f4d1e090c2effcb9bf3eb/deepfillv2/train.py\n",
        "class GatedGenerator(pl.LightningModule):\n",
        "    def __init__(self, in_channels = 4, out_channels = 3, latent_channels = 64, pad_type = 'zero', activation = 'lrelu', norm = 'in', conv_type = 'normal'):\n",
        "        super(GatedGenerator, self).__init__()\n",
        "\n",
        "        self.coarse = nn.Sequential(\n",
        "            # encoder\n",
        "            GatedConv2d(in_channels, latent_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = activation, norm = 'none'),\n",
        "            GatedConv2d(latent_channels, latent_channels * 2, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # Bottleneck\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 2, dilation = 2, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 4, dilation = 4, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 8, dilation = 8, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 16, dilation = 16, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # decoder\n",
        "            TransposeGatedConv2d(latent_channels * 4, latent_channels * 2, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 2, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            TransposeGatedConv2d(latent_channels * 2, latent_channels, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels, out_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = 'tanh', norm = 'none')\n",
        "        )\n",
        "        self.refinement = nn.Sequential(\n",
        "            # encoder\n",
        "            GatedConv2d(in_channels, latent_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = activation, norm = 'none'),\n",
        "            GatedConv2d(latent_channels, latent_channels * 2, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # Bottleneck\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 2, dilation = 2, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 4, dilation = 4, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 8, dilation = 8, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 16, dilation = 16, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # decoder\n",
        "            TransposeGatedConv2d(latent_channels * 4, latent_channels * 2, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 2, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            TransposeGatedConv2d(latent_channels * 2, latent_channels, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels, out_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = 'tanh', norm = 'none')\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img, mask):\n",
        "        # img: entire img\n",
        "        # mask: 1 for mask region; 0 for unmask region\n",
        "        # 1 - mask: unmask\n",
        "        # img * (1 - mask): ground truth unmask region\n",
        "        # Coarse\n",
        "        #print(img.shape, mask.shape)\n",
        "        first_masked_img = img * (1 - mask) + mask\n",
        "        first_in = torch.cat((first_masked_img, mask), 1)       # in: [B, 4, H, W]\n",
        "        first_out = self.coarse(first_in)                       # out: [B, 3, H, W]\n",
        "        # Refinement\n",
        "        second_masked_img = img * (1 - mask) + first_out * mask\n",
        "        second_in = torch.cat((second_masked_img, mask), 1)     # in: [B, 4, H, W]\n",
        "        second_out = self.refinement(second_in)                 # out: [B, 3, H, W]\n",
        "        #return first_out, second_out\n",
        "        #return second_out\n",
        "        return second_out, first_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T9ZSdpP8EJL",
        "cellView": "form"
      },
      "source": [
        "#@title [deepfillv1_arch.py](https://github.com/avalonstrel/GatedConvolution_pytorch) (2018)\n",
        "\"\"\"\n",
        "networks.py (12-12-20)\n",
        "https://github.com/avalonstrel/GatedConvolution_pytorch/blob/master/models/networks.py\n",
        "\n",
        "sa_gan.py (13-12-20)\n",
        "https://github.com/avalonstrel/GatedConvolution_pytorch/blob/master/models/sa_gan.py\n",
        "\n",
        "spectral.py (13-12-20)\n",
        "https://github.com/avalonstrel/GatedConvolution_pytorch/blob/master/models/spectral.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def get_pad(in_,  ksize, stride, atrous=1):\n",
        "    out_ = np.ceil(float(in_)/stride)\n",
        "    return int(((out_ - 1) * stride + atrous*(ksize-1) + 1 - in_)/2)\n",
        "\n",
        "\n",
        "class GatedConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated Convlution layer with activation (default activation:LeakyReLU)\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(GatedConv2dWithActivation, self).__init__()\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def gated(self, mask):\n",
        "        #return torch.clamp(mask, -1, 1)\n",
        "        return self.sigmoid(mask)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        mask = self.mask_conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) * self.gated(mask)\n",
        "        else:\n",
        "            x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class GatedDeConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True,activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(GatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = GatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)\n",
        "\n",
        "class SNGatedConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated Convolution with spetral normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.mask_conv2d = torch.nn.utils.spectral_norm(self.mask_conv2d)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def gated(self, mask):\n",
        "        return self.sigmoid(mask)\n",
        "        #return torch.clamp(mask, -1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        mask = self.mask_conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) * self.gated(mask)\n",
        "        else:\n",
        "            x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x\n",
        "class SNGatedDeConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = SNGatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)\n",
        "\n",
        "class SNConvWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    SN convolution for spetral normalization conv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNConvWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.activation = activation\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "#from .spectral import SpectralNorm\n",
        "#from .networks import GatedConv2dWithActivation, GatedDeConv2dWithActivation, SNConvWithActivation, get_pad\n",
        "class Self_Attn(pl.LightningModule):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation,with_attn=False):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "        self.with_attn = with_attn\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature\n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        if self.with_attn:\n",
        "            return out ,attention\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "class SAGenerator(pl.LightningModule):\n",
        "    \"\"\"Generator.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, image_size=64, z_dim=100, conv_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        repeat_num = int(np.log2(self.imsize)) - 3\n",
        "        mult = 2 ** repeat_num # 8\n",
        "        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n",
        "        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n",
        "        layer1.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = conv_dim * mult\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer2.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer3.append(nn.ReLU())\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "            layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "            layer4.append(nn.ReLU())\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n",
        "        last.append(nn.Tanh())\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn( 128, 'relu')\n",
        "        self.attn2 = Self_Attn( 64,  'relu')\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        out=self.l1(z)\n",
        "        out=self.l2(out)\n",
        "        out=self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out, p1, p2\n",
        "\n",
        "class InpaintSANet(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Inpaint generator, input should be 5*256*256, where 3*256*256 is the masked image, 1*256*256 for mask, 1*256*256 is the guidence\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in_channel=5):\n",
        "        super(InpaintSANet, self).__init__()\n",
        "        cnum = 32\n",
        "        self.coarse_net = nn.Sequential(\n",
        "            #input is 5*256*256, but it is full convolution network, so it can be larger than 256\n",
        "            GatedConv2dWithActivation(n_in_channel, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n",
        "            # downsample 128\n",
        "            GatedConv2dWithActivation(cnum, 2*cnum, 4, 2, padding=get_pad(256, 4, 2)),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            #downsample to 64\n",
        "            GatedConv2dWithActivation(2*cnum, 4*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            # atrous convlution\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            #Self_Attn(4*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            # upsample\n",
        "            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            #Self_Attn(2*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "            #Self_Attn(cnum//2, 'relu'),\n",
        "            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(128, 3, 1), activation=None)\n",
        "        )\n",
        "\n",
        "        self.refine_conv_net = nn.Sequential(\n",
        "            # input is 5*256*256\n",
        "            GatedConv2dWithActivation(n_in_channel, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n",
        "            # downsample\n",
        "            GatedConv2dWithActivation(cnum, cnum, 4, 2, padding=get_pad(256, 4, 2)),\n",
        "            GatedConv2dWithActivation(cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            # downsample\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n",
        "            GatedConv2dWithActivation(2*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n",
        "            #Self_Attn(4*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n",
        "\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16))\n",
        "        )\n",
        "        self.refine_attn = Self_Attn(4*cnum, 'relu', with_attn=False)\n",
        "        self.refine_upsample_net = nn.Sequential(\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "            #Self_Attn(cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(256, 3, 1), activation=None),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, imgs, masks, img_exs=None):\n",
        "        # Coarse\n",
        "        #masked_imgs =  imgs * (1 - masks) + masks\n",
        "\n",
        "        if img_exs == None:\n",
        "            input_imgs = torch.cat([imgs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        else:\n",
        "            input_imgs = torch.cat([imgs, img_exs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        #print(input_imgs.size(), imgs.size(), masks.size())\n",
        "        x = self.coarse_net(input_imgs)\n",
        "        x = torch.clamp(x, -1., 1.)\n",
        "        coarse_x = x\n",
        "        # Refine\n",
        "        masked_imgs = imgs * (1 - masks) + coarse_x * masks\n",
        "        if img_exs is None:\n",
        "            input_imgs = torch.cat([masked_imgs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        else:\n",
        "            input_imgs = torch.cat([masked_imgs, img_exs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        x = self.refine_conv_net(input_imgs)\n",
        "        x= self.refine_attn(x)\n",
        "        #print(x.size(), attention.size())\n",
        "        x = self.refine_upsample_net(x)\n",
        "        x = torch.clamp(x, -1., 1.)\n",
        "\n",
        "        return x, coarse_x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ceB_mrdmaVs"
      },
      "source": [
        "Special:\n",
        "\n",
        "Needs more attention because of amount of outputs, custom inputs or custom loss calculation. Assumes you can configure everything correctly inside ``CustomTrainClass``. Example usage is [here](https://github.com/styler00dollar/Colab-BasicSR/blob/master/codes/models/inpaint_model.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPsYK_bnGZU",
        "cellView": "form"
      },
      "source": [
        "#@title [Pluralistic_arch.py](https://github.com/lyndonzheng/Pluralistic-Inpainting) (should work, but ``CustomTrainClass`` is not ready) (2019)\n",
        "\"\"\"\n",
        "network.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/network.py\n",
        "\n",
        "external_function.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/external_function.py\n",
        "\n",
        "base_function.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/base_function.py\n",
        "\n",
        "external_function.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/external_function.py\n",
        "\n",
        "task.py (16-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/util/task.py\n",
        "\"\"\"\n",
        "\n",
        "from PIL import Image\n",
        "from random import randint\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "import cv2\n",
        "import functools\n",
        "import logging\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "logger = logging.getLogger('base')\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "###################################################################\n",
        "# multi scale for image generation\n",
        "###################################################################\n",
        "\n",
        "\n",
        "def scale_img(img, size):\n",
        "    scaled_img = F.interpolate(img, size=size, mode='bilinear', align_corners=True)\n",
        "    return scaled_img\n",
        "\n",
        "\n",
        "def scale_pyramid(img, num_scales):\n",
        "    scaled_imgs = [img]\n",
        "\n",
        "    s = img.size()\n",
        "\n",
        "    h = s[2]\n",
        "    w = s[3]\n",
        "\n",
        "    for i in range(1, num_scales):\n",
        "        ratio = 2**i\n",
        "        nh = h // ratio\n",
        "        nw = w // ratio\n",
        "        scaled_img = scale_img(img, size=[nh, nw])\n",
        "        scaled_imgs.append(scaled_img)\n",
        "\n",
        "    scaled_imgs.reverse()\n",
        "    return scaled_imgs\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# spectral normalization layer to decouple the magnitude of a weight tensor\n",
        "####################################################################################################\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    spectral normalization\n",
        "    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'\n",
        "    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n",
        "    \"\"\"\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# neural style transform loss from neural_style_tutorial of pytorch\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def GramMatrix(input):\n",
        "    s = input.size()\n",
        "    features = input.view(s[0], s[1], s[2]*s[3])\n",
        "    features_t = torch.transpose(features, 1, 2)\n",
        "    G = torch.bmm(features, features_t).div(s[1]*s[2]*s[3])\n",
        "    return G\n",
        "\n",
        "\n",
        "def img_crop(input, size=224):\n",
        "    input_cropped = F.upsample(input, size=(size, size), mode='bilinear', align_corners=True)\n",
        "    return input_cropped\n",
        "\n",
        "\n",
        "class Normalization(pl.LightningModule):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = mean.view(-1, 1, 1)\n",
        "        self.std = std.view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (input-self.mean) / self.std\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "# base function for network structure\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "def pluralistic_init_weights(net, init_type='normal', gain=0.02):\n",
        "    \"\"\"Get different initial method for the network weights\"\"\"\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv')!=-1 or classname.find('Linear')!=-1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    #print('initialize network with %s' % init_type)\n",
        "    logger.info('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type='batch'):\n",
        "    \"\"\"Get the normalization layer for the networks\"\"\"\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, momentum=0.1, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=True)\n",
        "    elif norm_type == 'none':\n",
        "        norm_layer = None\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "\n",
        "def get_nonlinearity_layer(activation_type='PReLU'):\n",
        "    \"\"\"Get the activation layer for the networks\"\"\"\n",
        "    if activation_type == 'ReLU':\n",
        "        nonlinearity_layer = nn.ReLU()\n",
        "    elif activation_type == 'SELU':\n",
        "        nonlinearity_layer = nn.SELU()\n",
        "    elif activation_type == 'LeakyReLU':\n",
        "        nonlinearity_layer = nn.LeakyReLU(0.1)\n",
        "    elif activation_type == 'PReLU':\n",
        "        nonlinearity_layer = nn.PReLU()\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [%s] is not found' % activation_type)\n",
        "    return nonlinearity_layer\n",
        "\n",
        "def print_network(net):\n",
        "    \"\"\"print the network\"\"\"\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    print(net)\n",
        "    print('total number of parameters: %.3f M' % (num_params/1e6))\n",
        "\n",
        "\n",
        "def init_net(net, init_type='normal', activation='relu', gpu_ids=[]):\n",
        "    \"\"\"print the network structure and initial the network\"\"\"\n",
        "    print_network(net)\n",
        "\n",
        "    if len(gpu_ids) > 0:\n",
        "        assert(torch.cuda.is_available())\n",
        "        net.cuda()\n",
        "        net = torch.nn.DataParallel(net, gpu_ids)\n",
        "    init_weights(net, init_type)\n",
        "    return net\n",
        "\n",
        "\n",
        "def _freeze(*args):\n",
        "    \"\"\"freeze the network for forward process\"\"\"\n",
        "    for module in args:\n",
        "        if module:\n",
        "            for p in module.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "\n",
        "def _unfreeze(*args):\n",
        "    \"\"\" unfreeze the network for parameter update\"\"\"\n",
        "    for module in args:\n",
        "        if module:\n",
        "            for p in module.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "\n",
        "def spectral_norm(module, use_spect=True):\n",
        "    \"\"\"use spectral normal layer to stable the training process\"\"\"\n",
        "    if use_spect:\n",
        "        return SpectralNorm(module)\n",
        "    else:\n",
        "        return module\n",
        "\n",
        "\n",
        "def coord_conv(input_nc, output_nc, use_spect=False, use_coord=False, with_r=False, **kwargs):\n",
        "    \"\"\"use coord convolution layer to add position information\"\"\"\n",
        "    if use_coord:\n",
        "        return CoordConv(input_nc, output_nc, with_r, use_spect, **kwargs)\n",
        "    else:\n",
        "        return spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "# Network basic function\n",
        "######################################################################################\n",
        "class AddCoords(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Add Coords to a tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, with_r=False):\n",
        "        super(AddCoords, self).__init__()\n",
        "        self.with_r = with_r\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: shape (batch, channel, x_dim, y_dim)\n",
        "        :return: shape (batch, channel+2, x_dim, y_dim)\n",
        "        \"\"\"\n",
        "        B, _, x_dim, y_dim = x.size()\n",
        "\n",
        "        # coord calculate\n",
        "        xx_channel = torch.arange(x_dim).repeat(B, 1, y_dim, 1).type_as(x)\n",
        "        yy_cahnnel = torch.arange(y_dim).repeat(B, 1, x_dim, 1).permute(0, 1, 3, 2).type_as(x)\n",
        "        # normalization\n",
        "        xx_channel = xx_channel.float() / (x_dim-1)\n",
        "        yy_cahnnel = yy_cahnnel.float() / (y_dim-1)\n",
        "        xx_channel = xx_channel * 2 - 1\n",
        "        yy_cahnnel = yy_cahnnel * 2 - 1\n",
        "\n",
        "        ret = torch.cat([x, xx_channel, yy_cahnnel], dim=1)\n",
        "\n",
        "        if self.with_r:\n",
        "            rr = torch.sqrt(xx_channel ** 2 + yy_cahnnel ** 2)\n",
        "            ret = torch.cat([ret, rr], dim=1)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class CoordConv(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    CoordConv operation\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, with_r=False, use_spect=False, **kwargs):\n",
        "        super(CoordConv, self).__init__()\n",
        "        self.addcoords = AddCoords(with_r=with_r)\n",
        "        input_nc = input_nc + 2\n",
        "        if with_r:\n",
        "            input_nc = input_nc + 1\n",
        "        self.conv = spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ret = self.addcoords(x)\n",
        "        ret = self.conv(ret)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class ResBlock(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define an Residual block for different types\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, hidden_nc=None, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
        "                 sample_type='none', use_spect=False, use_coord=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        hidden_nc = output_nc if hidden_nc is None else hidden_nc\n",
        "        self.sample = True\n",
        "        if sample_type == 'none':\n",
        "            self.sample = False\n",
        "        elif sample_type == 'up':\n",
        "            output_nc = output_nc * 4\n",
        "            self.pool = nn.PixelShuffle(upscale_factor=2)\n",
        "        elif sample_type == 'down':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        else:\n",
        "            raise NotImplementedError('sample type [%s] is not found' % sample_type)\n",
        "\n",
        "        kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
        "        kwargs_short = {'kernel_size': 1, 'stride': 1, 'padding': 0}\n",
        "\n",
        "        self.conv1 = coord_conv(input_nc, hidden_nc, use_spect, use_coord, **kwargs)\n",
        "        self.conv2 = coord_conv(hidden_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "        self.bypass = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs_short)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(nonlinearity, self.conv1, nonlinearity, self.conv2,)\n",
        "        else:\n",
        "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, self.conv1, norm_layer(hidden_nc), nonlinearity, self.conv2,)\n",
        "\n",
        "        self.shortcut = nn.Sequential(self.bypass,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.sample:\n",
        "            out = self.pool(self.model(x)) + self.pool(self.shortcut(x))\n",
        "        else:\n",
        "            out = self.model(x) + self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResBlockEncoderOptimized(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define an Encoder block for the first layer of the discriminator and representation network\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(), use_spect=False, use_coord=False):\n",
        "        super(ResBlockEncoderOptimized, self).__init__()\n",
        "\n",
        "        kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
        "        kwargs_short = {'kernel_size': 1, 'stride': 1, 'padding': 0}\n",
        "\n",
        "        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "        self.conv2 = coord_conv(output_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "        self.bypass = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs_short)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(self.conv1, nonlinearity, self.conv2, nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "        else:\n",
        "            self.model = nn.Sequential(self.conv1, norm_layer(output_nc), nonlinearity, self.conv2, nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.shortcut = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2), self.bypass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x) + self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResBlockDecoder(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define a decoder block\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, hidden_nc=None, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
        "                 use_spect=False, use_coord=False):\n",
        "        super(ResBlockDecoder, self).__init__()\n",
        "\n",
        "        hidden_nc = output_nc if hidden_nc is None else hidden_nc\n",
        "\n",
        "        self.conv1 = spectral_norm(nn.Conv2d(input_nc, hidden_nc, kernel_size=3, stride=1, padding=1), use_spect)\n",
        "        self.conv2 = spectral_norm(nn.ConvTranspose2d(hidden_nc, output_nc, kernel_size=3, stride=2, padding=1, output_padding=1), use_spect)\n",
        "        self.bypass = spectral_norm(nn.ConvTranspose2d(input_nc, output_nc, kernel_size=3, stride=2, padding=1, output_padding=1), use_spect)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(nonlinearity, self.conv1, nonlinearity, self.conv2,)\n",
        "        else:\n",
        "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, self.conv1, norm_layer(hidden_nc), nonlinearity, self.conv2,)\n",
        "\n",
        "        self.shortcut = nn.Sequential(self.bypass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x) + self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Output(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define the output layer\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, kernel_size = 3, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
        "                 use_spect=False, use_coord=False):\n",
        "        super(Output, self).__init__()\n",
        "\n",
        "        kwargs = {'kernel_size': kernel_size, 'padding':0, 'bias': True}\n",
        "\n",
        "        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(nonlinearity, nn.ReflectionPad2d(int(kernel_size/2)), self.conv1, nn.Tanh())\n",
        "        else:\n",
        "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, nn.ReflectionPad2d(int(kernel_size / 2)), self.conv1, nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Auto_Attn(pl.LightningModule):\n",
        "    \"\"\" Short+Long attention Layer\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, norm_layer=nn.BatchNorm2d):\n",
        "        super(Auto_Attn, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "\n",
        "        self.query_conv = nn.Conv2d(input_nc, input_nc // 4, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.alpha = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.model = ResBlock(int(input_nc*2), input_nc, input_nc, norm_layer=norm_layer, use_spect=True)\n",
        "\n",
        "    def forward(self, x, pre=None, mask=None):\n",
        "        \"\"\"\n",
        "        inputs :\n",
        "            x : input feature maps( B X C X W X H)\n",
        "        returns :\n",
        "            out : self attention value + input feature\n",
        "            attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        B, C, W, H = x.size()\n",
        "        proj_query = self.query_conv(x).view(B, -1, W * H)  # B X (N)X C\n",
        "        proj_key = proj_query  # B X C x (N)\n",
        "\n",
        "        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)  # transpose check\n",
        "        attention = self.softmax(energy)  # BX (N) X (N)\n",
        "        proj_value = x.view(B, -1, W * H)  # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(B, C, W, H)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "\n",
        "        if type(pre) != type(None):\n",
        "            # using long distance attention layer to copy information from valid regions\n",
        "            context_flow = torch.bmm(pre.view(B, -1, W*H), attention.permute(0, 2, 1)).view(B, -1, W, H)\n",
        "            context_flow = self.alpha * (1-mask) * context_flow + (mask) * pre\n",
        "            out = self.model(torch.cat([out, context_flow], dim=1))\n",
        "\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# spectral normalization layer to decouple the magnitude of a weight tensor\n",
        "####################################################################################################\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    spectral normalization\n",
        "    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'\n",
        "    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n",
        "    \"\"\"\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# adversarial loss for different gan mode\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "class GANLoss(pl.LightningModule):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "    The GANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
        "        \"\"\" Initialize the GANLoss class.\n",
        "        Parameters:\n",
        "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
        "            target_real_label (bool) - - label for a real image\n",
        "            target_fake_label (bool) - - label of a fake image\n",
        "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
        "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
        "        \"\"\"\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        elif gan_mode == 'wgangp':\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real, is_disc=False):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
        "            labels = (self.real_label if target_is_real else self.fake_label).expand_as(prediction).type_as(prediction)\n",
        "            loss = self.loss(prediction, labels)\n",
        "        elif self.gan_mode in ['hinge', 'wgangp']:\n",
        "            if is_disc:\n",
        "                if target_is_real:\n",
        "                    prediction = -prediction\n",
        "                if self.gan_mode == 'hinge':\n",
        "                    loss = self.loss(1 + prediction).mean()\n",
        "                elif self.gan_mode == 'wgangp':\n",
        "                    loss = prediction.mean()\n",
        "            else:\n",
        "                loss = -prediction.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def cal_gradient_penalty(netD, real_data, fake_data, type='mixed', constant=1.0, lambda_gp=10.0):\n",
        "    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n",
        "    Arguments:\n",
        "        netD (network)              -- discriminator network\n",
        "        real_data (tensor array)    -- real images\n",
        "        fake_data (tensor array)    -- generated images from the generator\n",
        "        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n",
        "        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n",
        "        lambda_gp (float)           -- weight for this loss\n",
        "    Returns the gradient penalty loss\n",
        "    \"\"\"\n",
        "    if lambda_gp > 0.0:\n",
        "        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.\n",
        "            interpolatesv = real_data\n",
        "        elif type == 'fake':\n",
        "            interpolatesv = fake_data\n",
        "        elif type == 'mixed':\n",
        "            alpha = torch.rand(real_data.shape[0], 1)\n",
        "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
        "            alpha = alpha.type_as(real_data)\n",
        "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "        else:\n",
        "            raise NotImplementedError('{} not implemented'.format(type))\n",
        "        interpolatesv.requires_grad_(True)\n",
        "        disc_interpolates = netD(interpolatesv)\n",
        "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
        "                                        grad_outputs=torch.ones(disc_interpolates.size()).type_as(real_data),\n",
        "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
        "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
        "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n",
        "        return gradient_penalty, gradients\n",
        "    else:\n",
        "        return 0.0, None\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# neural style transform loss from neural_style_tutorial of pytorch\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def ContentLoss(input, target):\n",
        "    target = target.detach()\n",
        "    loss = F.l1_loss(input, target)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def GramMatrix(input):\n",
        "    s = input.size()\n",
        "    features = input.view(s[0], s[1], s[2]*s[3])\n",
        "    features_t = torch.transpose(features, 1, 2)\n",
        "    G = torch.bmm(features, features_t).div(s[1]*s[2]*s[3])\n",
        "    return G\n",
        "\n",
        "\n",
        "def StyleLoss(input, target):\n",
        "    target = GramMatrix(target).detach()\n",
        "    input = GramMatrix(input)\n",
        "    loss = F.l1_loss(input, target)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def img_crop(input, size=224):\n",
        "    input_cropped = F.upsample(input, size=(size, size), mode='bilinear', align_corners=True)\n",
        "    return input_cropped\n",
        "\n",
        "\n",
        "class Normalization(pl.LightningModule):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = mean.view(-1, 1, 1)\n",
        "        self.std = std.view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (input-self.mean) / self.std\n",
        "\n",
        "\n",
        "class get_features(pl.LightningModule):\n",
        "    def __init__(self, cnn):\n",
        "        super(get_features, self).__init__()\n",
        "\n",
        "        vgg = copy.deepcopy(cnn)\n",
        "\n",
        "        self.conv1 = nn.Sequential(vgg[0], vgg[1], vgg[2], vgg[3], vgg[4])\n",
        "        self.conv2 = nn.Sequential(vgg[5], vgg[6], vgg[7], vgg[8], vgg[9])\n",
        "        self.conv3 = nn.Sequential(vgg[10], vgg[11], vgg[12], vgg[13], vgg[14], vgg[15], vgg[16])\n",
        "        self.conv4 = nn.Sequential(vgg[17], vgg[18], vgg[19], vgg[20], vgg[21], vgg[22], vgg[23])\n",
        "        self.conv5 = nn.Sequential(vgg[24], vgg[25], vgg[26], vgg[27], vgg[28], vgg[29], vgg[30])\n",
        "\n",
        "    def forward(self, input, layers):\n",
        "        input = img_crop(input)\n",
        "        output = []\n",
        "        for i in range(1, layers):\n",
        "            layer = getattr(self, 'conv'+str(i))\n",
        "            input = layer(input)\n",
        "            output.append(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "##############################################################################################################\n",
        "# Network function\n",
        "##############################################################################################################\n",
        "def define_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
        "             use_coord=False, init_type='orthogonal', gpu_ids=[]):\n",
        "\n",
        "    net = ResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord)\n",
        "\n",
        "    return init_net(net, init_type, activation, gpu_ids)\n",
        "\n",
        "\n",
        "def define_g(output_nc=3, ngf=64, z_nc=512, img_f=512, L=1, layers=5, norm='instance', activation='ReLU', output_scale=1,\n",
        "             use_spect=True, use_coord=False, use_attn=True, init_type='orthogonal', gpu_ids=[]):\n",
        "\n",
        "    net = ResGenerator(output_nc, ngf, z_nc, img_f, L, layers, norm, activation, output_scale, use_spect, use_coord, use_attn)\n",
        "\n",
        "    return init_net(net, init_type, activation, gpu_ids)\n",
        "\n",
        "\n",
        "def define_d(input_nc=3, ndf=64, img_f=512, layers=6, norm='none', activation='LeakyReLU', use_spect=True, use_coord=False,\n",
        "             use_attn=True,  model_type='ResDis', init_type='orthogonal', gpu_ids=[]):\n",
        "\n",
        "    if model_type == 'ResDis':\n",
        "        net = ResDiscriminator(input_nc, ndf, img_f, layers, norm, activation, use_spect, use_coord, use_attn)\n",
        "    elif model_type == 'PatchDis':\n",
        "        net = PatchDiscriminator(input_nc, ndf, img_f, layers, norm, activation, use_spect, use_coord, use_attn)\n",
        "\n",
        "    return init_net(net, init_type, activation, gpu_ids)\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "# Network structure\n",
        "#############################################################################################################\n",
        "class ResEncoder(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    ResNet Encoder Network\n",
        "    :param input_nc: number of channels in input\n",
        "    :param ngf: base filter channel\n",
        "    :param z_nc: latent channels\n",
        "    :param img_f: the largest feature channels\n",
        "    :param L: Number of refinements of density\n",
        "    :param layers: down and up sample layers\n",
        "    :param norm: normalization function 'instance, batch, group'\n",
        "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc=3, ngf=64, z_nc=128, img_f=1024, L=6, layers=6, norm='none', activation='ReLU',\n",
        "                 use_spect=True, use_coord=False):\n",
        "        super(ResEncoder, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.z_nc = z_nc\n",
        "        self.L = L\n",
        "\n",
        "        norm_layer = get_norm_layer(norm_type=norm)\n",
        "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
        "        # encoder part\n",
        "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
        "\n",
        "        mult = 1\n",
        "        for i in range(layers-1):\n",
        "            mult_prev = mult\n",
        "            mult = min(2 ** (i + 1), img_f // ngf)\n",
        "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
        "            setattr(self, 'encoder' + str(i), block)\n",
        "\n",
        "        # inference part\n",
        "        for i in range(self.L):\n",
        "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
        "            setattr(self, 'infer_prior' + str(i), block)\n",
        "\n",
        "        self.posterior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
        "        self.prior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
        "\n",
        "    def forward(self, img_m, img_c=None):\n",
        "        \"\"\"\n",
        "        :param img_m: image with mask regions I_m\n",
        "        :param img_c: complement of I_m, the mask regions\n",
        "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
        "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
        "        \"\"\"\n",
        "\n",
        "        if type(img_c) != type(None):\n",
        "            img = torch.cat([img_m, img_c], dim=0)\n",
        "        else:\n",
        "            img = img_m\n",
        "\n",
        "        # encoder part\n",
        "        out = self.block0(img)\n",
        "        feature = [out]\n",
        "        for i in range(self.layers-1):\n",
        "            model = getattr(self, 'encoder' + str(i))\n",
        "            out = model(out)\n",
        "            feature.append(out)\n",
        "\n",
        "        # infer part\n",
        "        # during the training, we have two paths, during the testing, we only have one paths\n",
        "        if type(img_c) != type(None):\n",
        "            distribution = self.two_paths(out)\n",
        "            return distribution, feature\n",
        "        else:\n",
        "            distribution = self.one_path(out)\n",
        "            return distribution, feature\n",
        "\n",
        "    def one_path(self, f_in):\n",
        "        \"\"\"one path for baseline training or testing\"\"\"\n",
        "        f_m = f_in\n",
        "        distribution = []\n",
        "\n",
        "        # infer state\n",
        "        for i in range(self.L):\n",
        "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
        "            f_m = infer_prior(f_m)\n",
        "\n",
        "        # get distribution\n",
        "        o = self.prior(f_m)\n",
        "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
        "        distribution.append([q_mu, F.softplus(q_std)])\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    def two_paths(self, f_in):\n",
        "        \"\"\"two paths for the training\"\"\"\n",
        "        f_m, f_c = f_in.chunk(2)\n",
        "        distributions = []\n",
        "\n",
        "        # get distribution\n",
        "        o = self.posterior(f_c)\n",
        "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
        "        distribution = self.one_path(f_m)\n",
        "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
        "\n",
        "        return distributions\n",
        "\n",
        "\n",
        "class ResGenerator(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    ResNet Generator Network\n",
        "    :param output_nc: number of channels in output\n",
        "    :param ngf: base filter channel\n",
        "    :param z_nc: latent channels\n",
        "    :param img_f: the largest feature channels\n",
        "    :param L: Number of refinements of density\n",
        "    :param layers: down and up sample layers\n",
        "    :param norm: normalization function 'instance, batch, group'\n",
        "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
        "    :param output_scale: Different output scales\n",
        "    \"\"\"\n",
        "    def __init__(self, output_nc=3, ngf=64, z_nc=128, img_f=1024, L=1, layers=6, norm='batch', activation='ReLU',\n",
        "                 output_scale=1, use_spect=True, use_coord=False, use_attn=True):\n",
        "        super(ResGenerator, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.L = L\n",
        "        self.output_scale = output_scale\n",
        "        self.use_attn = use_attn\n",
        "\n",
        "        norm_layer = get_norm_layer(norm_type=norm)\n",
        "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
        "        # latent z to feature\n",
        "        mult = min(2 ** (layers-1), img_f // ngf)\n",
        "        self.generator = ResBlock(z_nc, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
        "\n",
        "        # transform\n",
        "        for i in range(self.L):\n",
        "            block = ResBlock(ngf * mult, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
        "            setattr(self, 'generator' + str(i), block)\n",
        "\n",
        "        # decoder part\n",
        "        for i in range(layers):\n",
        "            mult_prev = mult\n",
        "            mult = min(2 ** (layers - i - 1), img_f // ngf)\n",
        "            if i > layers - output_scale:\n",
        "                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
        "                upconv = ResBlockDecoder(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
        "            else:\n",
        "                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
        "                upconv = ResBlockDecoder(ngf * mult_prev , ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
        "            setattr(self, 'decoder' + str(i), upconv)\n",
        "            # output part\n",
        "            if i > layers - output_scale - 1:\n",
        "                outconv = Output(ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord)\n",
        "                setattr(self, 'out' + str(i), outconv)\n",
        "            # short+long term attention part\n",
        "            if i == 1 and use_attn:\n",
        "                attn = Auto_Attn(ngf*mult, None)\n",
        "                setattr(self, 'attn' + str(i), attn)\n",
        "\n",
        "    def forward(self, z, f_m=None, f_e=None, mask=None):\n",
        "        \"\"\"\n",
        "        ResNet Generator Network\n",
        "        :param z: latent vector\n",
        "        :param f_m: feature of valid regions for conditional VAG-GAN\n",
        "        :param f_e: previous encoder feature for short+long term attention layer\n",
        "        :return results: different scale generation outputs\n",
        "        \"\"\"\n",
        "\n",
        "        f = self.generator(z)\n",
        "        for i in range(self.L):\n",
        "             generator = getattr(self, 'generator' + str(i))\n",
        "             f = generator(f)\n",
        "\n",
        "        # the features come from mask regions and valid regions, we directly add them together\n",
        "        out = f_m + f\n",
        "        results= []\n",
        "        attn = 0\n",
        "        for i in range(self.layers):\n",
        "            model = getattr(self, 'decoder' + str(i))\n",
        "            out = model(out)\n",
        "            if i == 1 and self.use_attn:\n",
        "                # auto attention\n",
        "                model = getattr(self, 'attn' + str(i))\n",
        "                out, attn = model(out, f_e, mask)\n",
        "            if i > self.layers - self.output_scale - 1:\n",
        "                model = getattr(self, 'out' + str(i))\n",
        "                output = model(out)\n",
        "                results.append(output)\n",
        "                out = torch.cat([out, output], dim=1)\n",
        "\n",
        "        return results, attn\n",
        "\n",
        "# https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/model/pluralistic_model.py\n",
        "# https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/util/task.py\n",
        "class PluralisticGenerator(pl.LightningModule):\n",
        "    def __init__(self, ngf_E=32, z_nc_E=128, img_f_E=128, layers_E=5, norm_E='none', activation_E='LeakyReLU',\n",
        "                 ngf_G=32, z_nc_G=128, img_f_G=128, L_G=0, output_scale_G=1, norm_G='instance', activation_G='LeakyReLU', train_paths='two'):\n",
        "        super().__init__()\n",
        "        self.net_E = ResEncoder(ngf=ngf_E, z_nc=z_nc_E, img_f=img_f_E, layers=layers_E, norm=norm_E, activation=activation_E)\n",
        "        self.net_G = ResGenerator(ngf=ngf_G, z_nc=z_nc_G, img_f=img_f_G, L=L_G, layers=5, output_scale=output_scale_G,\n",
        "                                      norm=norm_G, activation=activation_G)\n",
        "        self.train_paths = train_paths\n",
        "    def get_distribution(self, distributions, mask):\n",
        "        \"\"\"Calculate encoder distribution for img_m, img_c\"\"\"\n",
        "        # get distribution\n",
        "        sum_valid = (torch.mean(mask.view(mask.size(0), -1), dim=1) - 1e-5).view(-1, 1, 1, 1)\n",
        "        m_sigma = 1 / (1 + ((sum_valid - 0.8) * 8).exp_())\n",
        "        p_distribution, q_distribution, kl_rec, kl_g = 0, 0, 0, 0\n",
        "        self.distribution = []\n",
        "        for distribution in distributions:\n",
        "            p_mu, p_sigma, q_mu, q_sigma = distribution\n",
        "            # the assumption distribution for different mask regions\n",
        "            m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma))\n",
        "            # m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), torch.ones_like(p_sigma))\n",
        "            # the post distribution from mask regions\n",
        "            p_distribution = torch.distributions.Normal(p_mu, p_sigma)\n",
        "            p_distribution_fix = torch.distributions.Normal(p_mu.detach(), p_sigma.detach())\n",
        "            # the prior distribution from valid region\n",
        "            q_distribution = torch.distributions.Normal(q_mu, q_sigma)\n",
        "\n",
        "            # kl divergence\n",
        "            kl_rec += torch.distributions.kl_divergence(m_distribution, p_distribution)\n",
        "            if self.train_paths == \"one\":\n",
        "                kl_g += torch.distributions.kl_divergence(m_distribution, q_distribution)\n",
        "            elif self.train_paths == \"two\":\n",
        "                kl_g += torch.distributions.kl_divergence(p_distribution_fix, q_distribution)\n",
        "            self.distribution.append([torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma), p_mu, p_sigma, q_mu, q_sigma])\n",
        "\n",
        "        return p_distribution, q_distribution, kl_rec, kl_g\n",
        "\n",
        "    def get_G_inputs(self, p_distribution, q_distribution, f, mask):\n",
        "        \"\"\"Process the encoder feature and distributions for generation network\"\"\"\n",
        "        f_m = torch.cat([f[-1].chunk(2)[0], f[-1].chunk(2)[0]], dim=0)\n",
        "        f_e = torch.cat([f[2].chunk(2)[0], f[2].chunk(2)[0]], dim=0)\n",
        "        scale_mask = scale_img(mask, size=[f_e.size(2), f_e.size(3)])\n",
        "        mask = torch.cat([scale_mask.chunk(3, dim=1)[0], scale_mask.chunk(3, dim=1)[0]], dim=0)\n",
        "        z_p = p_distribution.rsample()\n",
        "        z_q = q_distribution.rsample()\n",
        "        z = torch.cat([z_p, z_q], dim=0)\n",
        "        return z, f_m, f_e, mask\n",
        "\n",
        "    def forward(self, images, img_inverted, masks):\n",
        "      distributions, f = self.net_E(images, img_inverted)\n",
        "      p_distribution, q_distribution, kl_rec, kl_g = self.get_distribution(distributions, masks)\n",
        "      z, f_m, f_e, mask = self.get_G_inputs(p_distribution, q_distribution, f, masks)\n",
        "      results, attn = self.net_G(z, f_m, f_e, mask)\n",
        "\n",
        "      self.img_rec = []\n",
        "      self.img_g = []\n",
        "      for result in results:\n",
        "          img_rec, img_g = result.chunk(2)\n",
        "          self.img_rec.append(img_rec)\n",
        "          self.img_g.append(img_g)\n",
        "\n",
        "      return self.img_g[-1].detach(), kl_rec, kl_g\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBy2VzOxnUCh",
        "cellView": "form"
      },
      "source": [
        "#@title [EdgeConnect_arch.py](https://github.com/knazeri/edge-connect) (2019)\n",
        "\"\"\"\n",
        "edge_connect.py (12-12-20)\n",
        "https://github.com/knazeri/edge-connect/blob/master/src/edge_connect.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch.optim as optim\n",
        "\n",
        "#from models.modules.architectures.convolutions.partialconv2d import PartialConv2d\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class InpaintGenerator(pl.LightningModule):\n",
        "    def __init__(self, residual_blocks=8, init_weights=True, conv_type='deform'):\n",
        "        super(InpaintGenerator, self).__init__()\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              nn.Conv2d(in_channels=4, out_channels=64, kernel_size=7, padding=0),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'partial':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              PartialConv2d(in_channels=4, out_channels=64, kernel_size=7, padding=0),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              PartialConv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              PartialConv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'deform':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              DeformConv2d(in_nc=4, out_nc=64, kernel_size=7, padding=0),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              DeformConv2d(in_nc=64, out_nc=128, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              DeformConv2d(in_nc=128, out_nc=256, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(residual_blocks):\n",
        "            block = ResnetBlock(256, 2)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, padding=0),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        x = (torch.tanh(x) + 1) / 2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EdgeGenerator(pl.LightningModule):\n",
        "    def __init__(self, residual_blocks=8, use_spectral_norm=True, init_weights=True, conv_type='normal'):\n",
        "        super(EdgeGenerator, self).__init__()\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              spectral_norm(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=0), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'partial':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              spectral_norm(PartialConv2d(in_channels=3, out_channels=64, kernel_size=7, padding=0), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(PartialConv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(PartialConv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'deform':\n",
        "            # without spectral_norm\n",
        "            self.encoder = nn.Sequential(\n",
        "                nn.ReflectionPad2d(3),\n",
        "                DeformConv2d(in_nc=3, out_nc=64, kernel_size=7, padding=0),\n",
        "                nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                DeformConv2d(in_nc=64, out_nc=128, kernel_size=4, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                DeformConv2d(in_nc=128, out_nc=256, kernel_size=4, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "                nn.ReLU(True)\n",
        "            )\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(residual_blocks):\n",
        "            block = ResnetBlock(256, 2, use_spectral_norm=use_spectral_norm)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            spectral_norm(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            spectral_norm(nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=7, padding=0),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation=1, use_spectral_norm=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "\n",
        "        # Remove ReLU at the end of the residual block\n",
        "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def spectral_norm(module, mode=True):\n",
        "    if mode:\n",
        "        return nn.utils.spectral_norm(module)\n",
        "\n",
        "    return module\n",
        "\n",
        "class EdgeConnectModel(pl.LightningModule):\n",
        "    def __init__(self, residual_blocks_edge=8, residual_blocks_inpaint=8, use_spectral_norm=True, conv_type_edge='normal', conv_type_inpaint='normal'):\n",
        "        super().__init__()\n",
        "        self.EdgeGenerator = EdgeGenerator(residual_blocks=residual_blocks_edge, use_spectral_norm=use_spectral_norm, conv_type=conv_type_edge)\n",
        "        self.InpaintGenerator = InpaintGenerator(residual_blocks=residual_blocks_inpaint, conv_type=conv_type_inpaint)\n",
        "\n",
        "    def forward(self, images, edges, grayscale, masks):\n",
        "        images = images.type(torch.cuda.FloatTensor)\n",
        "        edges = edges.type(torch.cuda.FloatTensor)\n",
        "        grayscale = grayscale.type(torch.cuda.FloatTensor)\n",
        "        masks = masks.type(torch.cuda.FloatTensor)\n",
        "        \n",
        "\n",
        "        # edge\n",
        "        edges_masked = (edges * masks)\n",
        "        grayscale_masked = grayscale * masks\n",
        "\n",
        "        inputs = torch.cat((grayscale_masked, edges_masked, masks), dim=1)\n",
        "        outputs_edge = self.EdgeGenerator(inputs)                                      # in: [grayscale(1) + edge(1) + mask(1)]\n",
        "\n",
        "        # inpaint\n",
        "        images_masked = (images * masks).float() + (1-masks)\n",
        "        inputs = torch.cat((images_masked, outputs_edge), dim=1)\n",
        "        outputs = self.InpaintGenerator(inputs)                                    # in: [rgb(3) + edge(1)]\n",
        "        return outputs, outputs_edge\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcKSIOD_naJV",
        "cellView": "form"
      },
      "source": [
        "#@title [FRRN_arch.py](https://github.com/ZongyuGuo/Inpainting_FRRN/) (2019)\n",
        "\"\"\"\n",
        "networks.py (18-12-20)\n",
        "https://github.com/ZongyuGuo/Inpainting_FRRN/blob/master/src/networks.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from .convolutions import partialconv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class FRRNet(pl.LightningModule):\n",
        "    def __init__(self, block_num=16):\n",
        "        super(FRRNet, self).__init__()\n",
        "        self.block_num = block_num\n",
        "        self.dilation_num = block_num // 2\n",
        "        blocks = []\n",
        "        for _ in range(self.block_num):\n",
        "            blocks.append(FRRBlock())\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = x.type(torch.cuda.FloatTensor)\n",
        "        mask = mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        mid_x = []\n",
        "        mid_m = []\n",
        "\n",
        "        mask_new = mask\n",
        "        for index in range(self.dilation_num):\n",
        "            x, _ = self.blocks[index * 2](x, mask_new, mask)\n",
        "            x, mask_new = self.blocks[index * 2 + 1](x, mask_new, mask)\n",
        "            mid_x.append(x)\n",
        "            mid_m.append(mask_new)\n",
        "\n",
        "        return x, mid_x, mid_m\n",
        "\n",
        "\n",
        "class FRRBlock(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(FRRBlock, self).__init__()\n",
        "        self.full_conv1 = PConvLayer(3,  32, kernel_size=5, stride=1, padding=2, use_norm=False)\n",
        "        self.full_conv2 = PConvLayer(32, 32, kernel_size=5, stride=1, padding=2, use_norm=False)\n",
        "        self.full_conv3 = PConvLayer(32, 3,  kernel_size=5, stride=1, padding=2, use_norm=False)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.branch_conv1 = PConvLayer(3,   64,  kernel_size=3, stride=2, padding=1, use_norm=False)\n",
        "        self.branch_conv2 = PConvLayer(64,  96,  kernel_size=3, stride=2, padding=1)\n",
        "        self.branch_conv3 = PConvLayer(96,  128, kernel_size=3, stride=2, padding=1)\n",
        "        self.branch_conv4 = PConvLayer(128, 96,  kernel_size=3, stride=1, padding=1, act='LeakyReLU')\n",
        "        self.branch_conv5 = PConvLayer(96,  64,  kernel_size=3, stride=1, padding=1, act='LeakyReLU')\n",
        "        self.branch_conv6 = PConvLayer(64,  3,   kernel_size=3, stride=1, padding=1, act='Tanh')\n",
        "\n",
        "    def forward(self, input, mask, mask_ori):\n",
        "        x = input\n",
        "        out_f, mask_f = self.full_conv1(x, mask)\n",
        "        out_f, mask_f = self.full_conv2(out_f, mask_f)\n",
        "        out_f, mask_f = self.full_conv3(out_f, mask_f)\n",
        "\n",
        "        out_b, mask_b = self.branch_conv1(x, mask)\n",
        "        out_b, mask_b = self.branch_conv2(out_b, mask_b)\n",
        "        out_b, mask_b = self.branch_conv3(out_b, mask_b)\n",
        "\n",
        "        out_b = self.upsample(out_b)\n",
        "        mask_b = self.upsample(mask_b)\n",
        "        out_b, mask_b = self.branch_conv4(out_b, mask_b)\n",
        "        out_b = self.upsample(out_b)\n",
        "        mask_b = self.upsample(mask_b)\n",
        "        out_b, mask_b = self.branch_conv5(out_b, mask_b)\n",
        "        out_b = self.upsample(out_b)\n",
        "        mask_b = self.upsample(mask_b)\n",
        "        out_b, mask_b = self.branch_conv6(out_b, mask_b)\n",
        "\n",
        "        mask_new = mask_f * mask_b\n",
        "        out = (out_f * mask_new + out_b * mask_new) / 2 * (1 - mask_ori) + input\n",
        "        #out = (out_f * mask_new + out_b * mask_new) / 2 * (1 - mask_ori) + input * mask_ori\n",
        "        return out, mask_new\n",
        "\n",
        "\n",
        "\n",
        "class PConvLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, act='ReLU', use_norm=True):\n",
        "        super(PConvLayer, self).__init__()\n",
        "        self.conv = PartialConv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride, padding=padding, return_mask=True)\n",
        "        self.norm = nn.InstanceNorm2d(out_channels, track_running_stats=False)\n",
        "        self.use_norm = use_norm\n",
        "        if act == 'ReLU':\n",
        "            self.act = nn.ReLU(True)\n",
        "        elif act == 'LeakyReLU':\n",
        "            self.act = nn.LeakyReLU(0.2, True)\n",
        "        elif act == 'Tanh':\n",
        "            self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        x, mask_update = self.conv(x, mask)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        x = self.act(x)\n",
        "        return x, mask_update\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDxfzr73nfuC",
        "cellView": "form"
      },
      "source": [
        "#@title [PRVS_arch.py](https://github.com/jingyuanli001/PRVS-Image-Inpainting) (2019)\n",
        "\"\"\"\n",
        "model.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/model.py\n",
        "\n",
        "PRVSNet.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/PRVSNet.py\n",
        "\n",
        "partialconv2d.py (18-12-20) # using their partconv2d to avoid dimension errors\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/partialconv2d.py\n",
        "\n",
        "PConvLayer.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/PConvLayer.py\n",
        "\n",
        "VSRLayer.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/VSRLayer.py\n",
        "\n",
        "Attention.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/Attention.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torchvision import models\n",
        "#from .convolutions import partialconv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "###############################################################################\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False\n",
        "\n",
        "        self.return_mask = True\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "\n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "\n",
        "        if mask is not None or self.last_size != (input.data.shape[2], input.data.shape[3]):\n",
        "            self.last_size = (input.data.shape[2], input.data.shape[3])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "\n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "            self.update_mask.to(input)\n",
        "            self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "PartialConv = PartialConv2d\n",
        "\n",
        "class AttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, patch_size = 3, propagate_size = 3, stride = 1):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        ###assume the masked area has value 1\n",
        "        bz, nc, w, h = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background * masks\n",
        "        background = F.pad(background, [self.patch_size//2, self.patch_size//2, self.patch_size//2, self.patch_size//2])\n",
        "        conv_kernels_all = background.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size, self.stride).contiguous().view(bz, nc, -1, self.patch_size, self.patch_size)\n",
        "        conv_kernels_all = conv_kernels_all.transpose(2, 1)\n",
        "        output_tensor = []\n",
        "        for i in range(bz):\n",
        "            mask = masks[i:i+1]\n",
        "            feature_map = foreground[i:i+1]\n",
        "            #form convolutional kernels\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels**2, [1, 2, 3], keepdim = True)**0.5\n",
        "            conv_kernels = conv_kernels/norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding = self.patch_size//2)\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.conv2d(conv_result, self.prop_kernels, stride = 1, padding = 1, groups = conv_result.size(1))\n",
        "            attention_scores = F.softmax(conv_result, dim = 1)\n",
        "            ##propagate the scores\n",
        "            recovered_foreground = F.conv_transpose2d(attention_scores, conv_kernels, stride = 1, padding = self.patch_size//2)\n",
        "            #average the recovered value, at the same time make non-masked area 0\n",
        "            recovered_foreground = (recovered_foreground * (1 - mask))/(self.patch_size ** 2)\n",
        "            #recover the image\n",
        "            final_output = recovered_foreground + feature_map * mask\n",
        "            output_tensor.append(final_output)\n",
        "        return torch.cat(output_tensor, dim = 0)\n",
        "\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class EdgeGenerator(pl.LightningModule):\n",
        "    def __init__(self, in_channels_feature, kernel_s = 3, add_last_edge = True):\n",
        "        super(EdgeGenerator, self).__init__()\n",
        "\n",
        "        self.p_conv = PartialConv2d(in_channels_feature + 1, 64, kernel_size = kernel_s, stride = 1, padding = kernel_s // 2, multi_channel = True, bias = False)\n",
        "\n",
        "        self.edge_resolver = Bottleneck(64, 16)\n",
        "        self.out_layer = nn.Conv2d(64, 1, 1, bias = False)\n",
        "\n",
        "    def forward(self, in_x, mask):\n",
        "        x, mask_updated = self.p_conv(in_x, mask)\n",
        "        x = self.edge_resolver(x)\n",
        "        edge_out = self.out_layer(x)\n",
        "        return edge_out, mask_updated\n",
        "\n",
        "class VSRLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channel, out_channel, stride = 2, kernel_size = 3, batch_norm = True, activation = \"ReLU\", deconv = False):\n",
        "        super(VSRLayer, self).__init__()\n",
        "        self.edge_generator = EdgeGenerator(in_channel, kernel_s = kernel_size)\n",
        "        self.feat_rec = PartialConv(in_channel+1, out_channel, stride = stride, kernel_size = kernel_size, padding = kernel_size//2, multi_channel = True)\n",
        "        if deconv:\n",
        "            self.deconv = nn.ConvTranspose2d(out_channel, out_channel, 4, 2, 1)\n",
        "        else:\n",
        "            self.deconv = lambda x:x\n",
        "\n",
        "        if batch_norm:\n",
        "            self.batchnorm = nn.BatchNorm2d(out_channel)\n",
        "        else:\n",
        "            self.batchnorm = lambda x:x\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        if activation == \"ReLU\":\n",
        "            self.activation = nn.ReLU(True)\n",
        "        elif activation == \"Leaky\":\n",
        "            self.activation = nn.LeakyReLU(0.2, True)\n",
        "        else:\n",
        "            self.activation = lambda x:x\n",
        "\n",
        "    def forward(self, feat_in, mask_in, edge_in):\n",
        "        edge_in = F.interpolate(edge_in, size = feat_in.size()[2:])\n",
        "        edge_updated, mask_updated = self.edge_generator(torch.cat([feat_in, edge_in], dim = 1), torch.cat([mask_in, mask_in[:,:1,:,:]], dim = 1))\n",
        "        edge_reconstructed = edge_in * mask_in[:,:1,:,:] + edge_updated * (mask_updated[:,:1,:,:] - mask_in[:,:1,:,:])\n",
        "        feat_out, feat_mask = self.feat_rec(torch.cat([feat_in, edge_reconstructed], dim = 1), torch.cat([mask_in, mask_updated[:,:1,:,:]], dim = 1))\n",
        "        feat_out = self.deconv(feat_out)\n",
        "        feat_out = self.batchnorm(feat_out)\n",
        "        feat_out = self.activation(feat_out)\n",
        "        mask_updated = F.interpolate(mask_updated, size = feat_out.size()[2:])\n",
        "        feat_mask = F.interpolate(feat_mask, size = feat_out.size()[2:])\n",
        "        return feat_out, feat_mask*mask_updated[:,0:1,:,:], edge_reconstructed\n",
        "\n",
        "\n",
        "class PConvLayer(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=True, sample='none-3', activ='relu',\n",
        "                 conv_bias=False, deconv = False):\n",
        "        super().__init__()\n",
        "        if sample == 'down-5':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 5, 2, 2, bias=conv_bias, multi_channel = True)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 7, 2, 3, bias=conv_bias, multi_channel = True)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias, multi_channel = True)\n",
        "        else:\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias, multi_channel = True)\n",
        "        if deconv:\n",
        "            self.deconv = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1, bias = conv_bias)\n",
        "        else:\n",
        "            self.deconv = None\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(out_ch)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input, input_mask):\n",
        "        h, h_mask = self.conv(input, input_mask)\n",
        "        if self.deconv is not None:\n",
        "            h = self.deconv(h)\n",
        "        if hasattr(self, 'bn'):\n",
        "            h = self.bn(h)\n",
        "        if hasattr(self, 'activation'):\n",
        "            h = self.activation(h)\n",
        "        h_mask = F.interpolate(h_mask, size = h.size()[2:])\n",
        "        return h, h_mask\n",
        "\n",
        "class VGG16FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(3):\n",
        "            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        results = [image]\n",
        "        for i in range(3):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class PRVSNet(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, att = False):\n",
        "        super().__init__()\n",
        "        self.layer_size = layer_size\n",
        "        self.enc_1 = VSRLayer(3, 64, kernel_size = 7)\n",
        "        self.enc_2 = VSRLayer(64, 128, kernel_size = 5)\n",
        "        self.enc_3 = PConvLayer(128, 256, sample='down-5')\n",
        "        self.enc_4 = PConvLayer(256, 512, sample='down-3')\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            setattr(self, name, PConvLayer(512, 512, sample='down-3'))\n",
        "        self.deconv = nn.ConvTranspose2d(512, 512, 4, 2, 1)\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'dec_{:d}'.format(i + 1)\n",
        "            setattr(self, name, PConvLayer(512 + 512, 512, activ='leaky', deconv = True))\n",
        "        self.dec_4 = PConvLayer(512 + 256, 256, activ='leaky', deconv = True)\n",
        "        if att:\n",
        "            self.att = Attention.AttentionModule()\n",
        "        else:\n",
        "            self.att = lambda x:x\n",
        "        self.dec_3 = PConvLayer(256 + 128, 128, activ='leaky', deconv = True)\n",
        "        self.dec_2 = VSRLayer(128 + 64, 64, stride = 1, activation='leaky', deconv = True)\n",
        "        self.dec_1 = VSRLayer(64 + input_channels, 64, stride = 1, activation = None, batch_norm = False)\n",
        "        self.resolver = Bottleneck(64,16)\n",
        "        self.output = nn.Conv2d(128, 3, 1)\n",
        "\n",
        "    def forward(self, input, input_mask, input_edge):\n",
        "        input = input.type(torch.cuda.FloatTensor)\n",
        "        input_mask = input_mask.type(torch.cuda.FloatTensor)\n",
        "        input_edge = input_edge.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        input = input * input_mask[:,0:1,:,:]\n",
        "        input_edge = input_edge * input_mask[:,0:1,:,:]\n",
        "        input_mask = torch.cat([input_mask]*3, dim = 1)\n",
        "        input_mask = input_mask[:,:3,:,:]\n",
        "\n",
        "\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "        h_mask_dict = {}  # for the output of enc_N\n",
        "        h_edge_list = []\n",
        "        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask\n",
        "        edge = input_edge\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            if i not in [1, 2]:\n",
        "                h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(h_dict[h_key_prev], h_mask_dict[h_key_prev])\n",
        "            else:\n",
        "                h_dict[h_key], h_mask_dict[h_key], edge = getattr(self, l_key)(h_dict[h_key_prev], h_mask_dict[h_key_prev], edge)\n",
        "                h_edge_list.append(edge)\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h_key = 'h_{:d}'.format(self.layer_size)\n",
        "        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n",
        "        h = self.deconv(h)\n",
        "        h_mask = F.interpolate(h_mask, scale_factor = 2)\n",
        "\n",
        "        for i in range(self.layer_size, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i - 1)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim = 1)\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim = 1)\n",
        "            if i not in [2, 1]:\n",
        "                h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n",
        "            else:\n",
        "                edge = h_edge_list[i-1]\n",
        "                h, h_mask, edge = getattr(self, dec_l_key)(h, h_mask, edge)\n",
        "                h_edge_list.append(edge)\n",
        "            if i == 4:\n",
        "                h = self.att(h)\n",
        "        h_out = self.resolver(h)\n",
        "        h_out = torch.cat([h_out, h], dim = 1)\n",
        "        h_out = self.output(h_out)\n",
        "        return h_out, h_mask, h_edge_list[-2], h_edge_list[-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAFa5pvrntKK",
        "cellView": "form"
      },
      "source": [
        "#@title [CSA_arch.py](https://github.com/Yukariin/CSA_pytorch) (2019)\n",
        "\"\"\"\n",
        "model.py (13-12-20)\n",
        "https://github.com/Yukariin/CSA_pytorch/blob/master/model.py\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def get_norm(name, out_channels):\n",
        "    if name == 'batch':\n",
        "        norm = nn.BatchNorm2d(out_channels)\n",
        "    elif name == 'instance':\n",
        "        norm = nn.InstanceNorm2d(out_channels)\n",
        "    else:\n",
        "        norm = None\n",
        "    return norm\n",
        "\n",
        "\n",
        "def get_act(name):\n",
        "    if name == 'relu':\n",
        "        activation = nn.ReLU(inplace=True)\n",
        "    elif name == 'elu':\n",
        "        activation == nn.ELU(inplace=True)\n",
        "    elif name == 'leaky_relu':\n",
        "        activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "    elif name == 'tanh':\n",
        "        activation = nn.Tanh()\n",
        "    elif name == 'sigmoid':\n",
        "        activation = nn.Sigmoid()\n",
        "    else:\n",
        "        activation = None\n",
        "    return activation\n",
        "\n",
        "\n",
        "class CoarseEncodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class CoarseDecodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "\n",
        "\n",
        "class CoarseNet(pl.LightningModule):\n",
        "    def __init__(self, c_img=3,\n",
        "                 norm='instance', act_en='leaky_relu', act_de='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        cnum = 64\n",
        "\n",
        "        self.en_1 = nn.Conv2d(c_img, cnum, 4, 2, padding=1)\n",
        "        self.en_2 = CoarseEncodeBlock(cnum, cnum*2, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_3 = CoarseEncodeBlock(cnum*2, cnum*4, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_4 = CoarseEncodeBlock(cnum*4, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_5 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_6 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_7 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_8 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, activation=act_en)\n",
        "\n",
        "        self.de_8 = CoarseDecodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_7 = CoarseDecodeBlock(cnum*8*2, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_6 = CoarseDecodeBlock(cnum*8*2, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_5 = CoarseDecodeBlock(cnum*8*2, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_4 = CoarseDecodeBlock(cnum*8*2, cnum*4, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_3 = CoarseDecodeBlock(cnum*4*2, cnum*2, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_2 = CoarseDecodeBlock(cnum*2*2, cnum, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_1 = nn.Sequential(\n",
        "            get_act(act_de),\n",
        "            nn.ConvTranspose2d(cnum*2, c_img, 4, 2, padding=1),\n",
        "            get_act('tanh'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1 = self.en_1(x)\n",
        "        out_2 = self.en_2(out_1)\n",
        "        out_3 = self.en_3(out_2)\n",
        "        out_4 = self.en_4(out_3)\n",
        "        out_5 = self.en_5(out_4)\n",
        "        out_6 = self.en_6(out_5)\n",
        "        out_7 = self.en_7(out_6)\n",
        "        out_8 = self.en_8(out_7)\n",
        "\n",
        "        dout_8 = self.de_8(out_8)\n",
        "        dout_8_out_7 = torch.cat([dout_8, out_7], 1)\n",
        "        dout_7 = self.de_7(dout_8_out_7)\n",
        "        dout_7_out_6 = torch.cat([dout_7, out_6], 1)\n",
        "        dout_6 = self.de_6(dout_7_out_6)\n",
        "        dout_6_out_5 = torch.cat([dout_6, out_5], 1)\n",
        "        dout_5 = self.de_5(dout_6_out_5)\n",
        "        dout_5_out_4 = torch.cat([dout_5, out_4], 1)\n",
        "        dout_4 = self.de_4(dout_5_out_4)\n",
        "        dout_4_out_3 = torch.cat([dout_4, out_3], 1)\n",
        "        dout_3 = self.de_3(dout_4_out_3)\n",
        "        dout_3_out_2 = torch.cat([dout_3, out_2], 1)\n",
        "        dout_2 = self.de_2(dout_3_out_2)\n",
        "        dout_2_out_1 = torch.cat([dout_2, out_1], 1)\n",
        "        dout_1 = self.de_1(dout_2_out_1)\n",
        "\n",
        "        return dout_1\n",
        "\n",
        "\n",
        "class RefineEncodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, in_channels, 4, 2, dilation=2, padding=3))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class RefineDecodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 3, 1, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(out_channels, out_channels, 4, 2, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "\n",
        "\n",
        "class RefineNet(pl.LightningModule):\n",
        "    def __init__(self, c_img=3,\n",
        "                 norm='instance', act_en='leaky_relu', act_de='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        c_in = c_img + c_img\n",
        "        cnum = 64\n",
        "\n",
        "        self.en_1 = nn.Conv2d(c_in, cnum, 3, 1, padding=1)\n",
        "        self.en_2 = RefineEncodeBlock(cnum, cnum*2, normalization=norm, activation=act_en)\n",
        "        self.en_3 = RefineEncodeBlock(cnum*2, cnum*4, normalization=norm, activation=act_en)\n",
        "        self.en_4 = RefineEncodeBlock(cnum*4, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_5 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_6 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_7 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_8 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_9 = nn.Sequential(\n",
        "            get_act(act_en),\n",
        "            nn.Conv2d(cnum*8, cnum*8, 4, 2, padding=1))\n",
        "\n",
        "        self.de_9 = nn.Sequential(\n",
        "            get_act(act_de),\n",
        "            nn.ConvTranspose2d(cnum*8, cnum*8, 4, 2, padding=1),\n",
        "            get_norm(norm, cnum*8))\n",
        "        self.de_8 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_7 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_6 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_5 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_4 = RefineDecodeBlock(cnum*8*2, cnum*4, normalization=norm, activation=act_de)\n",
        "        self.de_3 = RefineDecodeBlock(cnum*4*2, cnum*2, normalization=norm, activation=act_de)\n",
        "        self.de_2 = RefineDecodeBlock(cnum*2*2, cnum, normalization=norm, activation=act_de)\n",
        "\n",
        "        self.de_1 = nn.Sequential(\n",
        "            get_act(act_de),\n",
        "            nn.ConvTranspose2d(cnum*2, c_img, 3, 1, padding=1))\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], 1)\n",
        "        out_1 = self.en_1(x)\n",
        "        out_2 = self.en_2(out_1)\n",
        "        out_3 = self.en_3(out_2)\n",
        "        out_4 = self.en_4(out_3)\n",
        "        out_5 = self.en_5(out_4)\n",
        "        out_6 = self.en_6(out_5)\n",
        "        out_7 = self.en_7(out_6)\n",
        "        out_8 = self.en_8(out_7)\n",
        "        out_9 = self.en_9(out_8)\n",
        "\n",
        "        dout_9 = self.de_9(out_9)\n",
        "        dout_9_out_8 = torch.cat([dout_9, out_8], 1)\n",
        "        dout_8 = self.de_8(dout_9_out_8)\n",
        "        dout_8_out_7 = torch.cat([dout_8, out_7], 1)\n",
        "        dout_7 = self.de_7(dout_8_out_7)\n",
        "        dout_7_out_6 = torch.cat([dout_7, out_6], 1)\n",
        "        dout_6 = self.de_6(dout_7_out_6)\n",
        "        dout_6_out_5 = torch.cat([dout_6, out_5], 1)\n",
        "        dout_5 = self.de_5(dout_6_out_5)\n",
        "        dout_5_out_4 = torch.cat([dout_5, out_4], 1)\n",
        "        dout_4 = self.de_4(dout_5_out_4)\n",
        "        dout_4_out_3 = torch.cat([dout_4, out_3], 1)\n",
        "        dout_3 = self.de_3(dout_4_out_3)\n",
        "        dout_3_out_2 = torch.cat([dout_3, out_2], 1)\n",
        "        dout_2 = self.de_2(dout_3_out_2)\n",
        "        dout_2_out_1 = torch.cat([dout_2, out_1], 1)\n",
        "        dout_1 = self.de_1(dout_2_out_1)\n",
        "\n",
        "        return dout_1, out_4, dout_5\n",
        "\n",
        "\n",
        "class CSA(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        return x\n",
        "\n",
        "\n",
        "class InpaintNet(pl.LightningModule):\n",
        "    def __init__(self, c_img=3, norm='instance', act_en='leaky_relu', act_de='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.coarse = CoarseNet(c_img=c_img, norm=norm, act_en=act_en, act_de=act_de)\n",
        "        self.refine = RefineNet(c_img=c_img, norm=norm, act_en=act_en, act_de=act_de)\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        out_c = self.coarse(image)\n",
        "        out_c = image * (1. - mask) + out_c * mask\n",
        "        out_r, csa, csa_d = self.refine(out_c, image)\n",
        "        return out_c, out_r, csa, csa_d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy958bvZqGqs",
        "cellView": "form"
      },
      "source": [
        "#@title CSA_loss.py\n",
        "\"\"\"\n",
        "# needs to be merged with loss.py\n",
        "loss.py (16-12-20)\n",
        "https://github.com/Yukariin/CSA_pytorch/blob/master/loss.py\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2 # [-1,1] -> [0,1]\n",
        "    return out.clamp_(0, 1)\n",
        "\n",
        "\n",
        "class VGG16FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "        self.enc_4 = nn.Sequential(*vgg16.features[17:23])\n",
        "\n",
        "        #print(self.enc_1)\n",
        "        #print(self.enc_2)\n",
        "        #print(self.enc_3)\n",
        "        #print(self.enc_4)\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(4):\n",
        "            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        results = [image]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "\n",
        "class ConsistencyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "        self.vgg = VGG16FeatureExtractor()\n",
        "        self.vgg.cuda()\n",
        "\n",
        "        self.l2 = nn.MSELoss()\n",
        "\n",
        "    def forward(self, csa, csa_d, target, mask):\n",
        "        # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "        # Pre-trained VGG16 model expect input images normalized in the same way.\n",
        "        # The images have to be loaded in to a range of [0, 1]\n",
        "        # and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "        t = denorm(target) # [-1,1] -> [0,1]\n",
        "        t = self.normalize(t[0]) # BxCxHxW -> CxHxW -> normalize\n",
        "        t = t.unsqueeze(0) # CxHxW -> BxCxHxW\n",
        "\n",
        "        vgg_gt = self.vgg(t)\n",
        "        vgg_gt = vgg_gt[-1]\n",
        "\n",
        "        mask_r = F.interpolate(mask, size=csa.size()[2:])\n",
        "\n",
        "        lossvalue = self.l2(csa*mask_r, vgg_gt*mask_r) + self.l2(csa_d*mask_r, vgg_gt*mask_r)\n",
        "        return lossvalue\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ybOcG4cN-57"
      },
      "source": [
        "Broken generators:\n",
        "\n",
        "Generators that are not included here since I can't seem to make them work properly:\n",
        "\n",
        "PenNet [no AMP] (2019): [researchmm/PEN-Net-for-Inpainting](https://github.com/researchmm/PEN-Net-for-Inpainting/)\n",
        "- Always outputs white for some reason.\n",
        "\n",
        "CRA [no AMP] (2019): [wangyx240/High-Resolution-Image-Inpainting-GAN](https://github.com/wangyx240/High-Resolution-Image-Inpainting-GAN)\n",
        "- Likes to create the color pink.\n",
        "\n",
        "Global [no AMP] (2020): [SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting](https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting)\n",
        "- Always outputs white for some reason.\n",
        "\n",
        "crfill (2020): [zengxianyu/crfill](https://github.com/zengxianyu/crfill)\n",
        "- No clear instructions/code result in broken results. Unreleased training code makes a correct implementation harder.\n",
        "\n",
        "---------------------------\n",
        "\n",
        "Non-Pytorch generators:\n",
        "\n",
        "co-mod-gan (2021): [zsyzzsoft/co-mod-gan](https://github.com/zsyzzsoft/co-mod-gan)\n",
        "- Has a web demo and (a broken link to a) docker. Relies on Tensorflow / StyleGAN2 code.\n",
        "\n",
        "Diverse-Structure-Inpainting (2021): [USTC-JialunPeng/Diverse-Structure-Inpainting](https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting)\n",
        "- Tensorflow 1\n",
        "\n",
        "R-MNet (2021): [Jireh-Jam/R-MNet-Inpainting-keras](https://github.com/Jireh-Jam/R-MNet-Inpainting-keras)\n",
        "- Not sure if there is much new and interesting stuff.\n",
        "\n",
        "\n",
        "Hypergraphs (2021): [GouravWadhwa/Hypergraphs-Image-Inpainting](https://github.com/GouravWadhwa/Hypergraphs-Image-Inpainting)\n",
        "- Uses custom conv layer (that is implemented with tensorflow). It sounds interesting, but I got errors when I tried to port it to pytorch.\n",
        "\n",
        "PEPSI (2019): [Forty-lock/PEPSI-Fast_image_inpainting_with_parallel_decoding_network](https://github.com/Forty-lock/PEPSI-Fast_image_inpainting_with_parallel_decoding_network)\n",
        "- The net dcpV2 uses.\n",
        "\n",
        "Region (2019): [vickyFox/Region-wise-Inpainting](https://github.com/vickyFox/Region-wise-Inpainting)\n",
        "\n",
        "---------------------------\n",
        "\n",
        "Pytorch generators that I never tested:\n",
        "\n",
        "VCNET (2020): [birdortyedi/vcnet-blind-image-inpainting](https://github.com/birdortyedi/vcnet-blind-image-inpainting)\n",
        "- Blind image inpainting without masks.\n",
        "\n",
        "DFMA (2020): [mprzewie/dmfa_inpainting](https://github.com/mprzewie/dmfa_inpainting)\n",
        "\n",
        "\n",
        "GIN (2020): [rlct1/gin-sg](https://github.com/rlct1/gin-sg) and [rlct1/gin](https://github.com/rlct1/gin)\n",
        "\n",
        "\n",
        "StructureFlow (2019): [RenYurui/StructureFlow](https://github.com/RenYurui/StructureFlow)\n",
        "- Needs special files.\n",
        "\n",
        "GMCNN (2018): [shepnerd/inpainting_gmcnn](https://github.com/shepnerd/inpainting_gmcnn)\n",
        "- The net dcpV1 used iirc.\n",
        "\n",
        "ShiftNet (2018): [Zhaoyi-Yan/Shift-Net_pytorch](https://github.com/Zhaoyi-Yan/Shift-Net_pytorch)\n",
        "---------------------------\n",
        "\n",
        "No training code:\n",
        "\n",
        "SC-FEGAN (2019): [run-youngjoo/SC-FEGAN](https://github.com/run-youngjoo/SC-FEGAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEj2gEWfvyqM"
      },
      "source": [
        "# Upscale Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM7P_Ojbv6NC",
        "cellView": "form"
      },
      "source": [
        "#@title ESRGAN_arch.py (dataloader not implemented)\n",
        "\"\"\"\n",
        "RRDBNet_arch.py (12-2-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/architectures/RRDBNet_arch.py\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torchvision\n",
        "#from . import block as B\n",
        "import functools\n",
        "#from . import spectral_norm as SN\n",
        "\n",
        "\n",
        "####################\n",
        "# RRDBNet Generator (original architecture)\n",
        "####################\n",
        "\n",
        "class RRDBNet(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, nf, nb, nr=3, gc=32, upscale=4, norm_type=None, \\\n",
        "            act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D', \\\n",
        "            finalact=None, gaussian_noise=False, plus=False):\n",
        "        super(RRDBNet, self).__init__()\n",
        "        n_upscale = int(math.log(upscale, 2))\n",
        "        if upscale == 3:\n",
        "            n_upscale = 1\n",
        "\n",
        "        fea_conv = conv_block(in_nc, nf, kernel_size=3, norm_type=None, act_type=None, convtype=convtype)\n",
        "        rb_blocks = [RRDB(nf, nr, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero', \\\n",
        "            norm_type=norm_type, act_type=act_type, mode='CNA', convtype=convtype, \\\n",
        "            gaussian_noise=gaussian_noise, plus=plus) for _ in range(nb)]\n",
        "        LR_conv = conv_block(nf, nf, kernel_size=3, norm_type=norm_type, act_type=None, mode=mode, convtype=convtype)\n",
        "\n",
        "        if upsample_mode == 'upconv':\n",
        "            upsample_block = upconv_block\n",
        "        elif upsample_mode == 'pixelshuffle':\n",
        "            upsample_block = pixelshuffle_block\n",
        "        else:\n",
        "            raise NotImplementedError('upsample mode [{:s}] is not found'.format(upsample_mode))\n",
        "        if upscale == 3:\n",
        "            upsampler = upsample_block(nf, nf, 3, act_type=act_type, convtype=convtype)\n",
        "        else:\n",
        "            upsampler = [upsample_block(nf, nf, act_type=act_type, convtype=convtype) for _ in range(n_upscale)]\n",
        "        HR_conv0 = conv_block(nf, nf, kernel_size=3, norm_type=None, act_type=act_type, convtype=convtype)\n",
        "        HR_conv1 = conv_block(nf, out_nc, kernel_size=3, norm_type=None, act_type=None, convtype=convtype)\n",
        "\n",
        "        # Note: this option adds new parameters to the architecture, another option is to use \"outm\" in the forward\n",
        "        outact = act(finalact) if finalact else None\n",
        "        \n",
        "        self.model = sequential(fea_conv, ShortcutBlock(sequential(*rb_blocks, LR_conv)),\\\n",
        "            *upsampler, HR_conv0, HR_conv1, outact)\n",
        "\n",
        "    def forward(self, x, outm=None):\n",
        "        x = self.model(x)\n",
        "        \n",
        "        if outm=='scaltanh': # limit output range to [-1,1] range with tanh and rescale to [0,1] Idea from: https://github.com/goldhuang/SRGAN-PyTorch/blob/master/model.py\n",
        "            return(torch.tanh(x) + 1.0) / 2.0\n",
        "        elif outm=='tanh': # limit output to [-1,1] range\n",
        "            return torch.tanh(x)\n",
        "        elif outm=='sigmoid': # limit output to [0,1] range\n",
        "            return torch.sigmoid(x)\n",
        "        elif outm=='clamp':\n",
        "            return torch.clamp(x, min=0.0, max=1.0)\n",
        "        else: #Default, no cap for the output\n",
        "            return x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    '''\n",
        "    Residual in Residual Dense Block\n",
        "    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nf, nr=3, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero', \\\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(RRDB, self).__init__()\n",
        "        # This is for backwards compatibility with existing models\n",
        "        if nr == 3:\n",
        "            self.RDB1 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "            self.RDB2 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "            self.RDB3 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "        else:\n",
        "            RDB_list = [ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type,\n",
        "                                              norm_type, act_type, mode, convtype, spectral_norm=spectral_norm,\n",
        "                                              gaussian_noise=gaussian_noise, plus=plus) for _ in range(nr)]\n",
        "            self.RDBs = nn.Sequential(*RDB_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'RDB1'):\n",
        "            out = self.RDB1(x)\n",
        "            out = self.RDB2(out)\n",
        "            out = self.RDB3(out)\n",
        "        else:\n",
        "            out = self.RDBs(x)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class ResidualDenseBlock_5C(nn.Module):\n",
        "    '''\n",
        "    Residual Dense Block\n",
        "    style: 5 convs\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    Modified options that can be used:\n",
        "        - \"Partial Convolution based Padding\" arXiv:1811.11718\n",
        "        - \"Spectral normalization\" arXiv:1802.05957\n",
        "        - \"ICASSP 2020 - ESRGAN+ : Further Improving ESRGAN\" N. C. \n",
        "            {Rakotonirina} and A. {Rasoanaivo}\n",
        "    \n",
        "    Args:\n",
        "        nf (int): Channel number of intermediate features (num_feat).\n",
        "        gc (int): Channels for each growth (num_grow_ch: growth channel, \n",
        "            i.e. intermediate channels).\n",
        "        convtype (str): the type of convolution to use. Default: 'Conv2D'\n",
        "        gaussian_noise (bool): enable the ESRGAN+ gaussian noise (no new \n",
        "            trainable parameters)\n",
        "        plus (bool): enable the additional residual paths from ESRGAN+ \n",
        "            (adds trainable parameters)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nf=64, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero', \\\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(ResidualDenseBlock_5C, self).__init__()\n",
        "        \n",
        "        ## +\n",
        "        self.noise = GaussianNoise() if gaussian_noise else None\n",
        "        self.conv1x1 = conv1x1(nf, gc) if plus else None\n",
        "        ## +\n",
        "\n",
        "        self.conv1 = conv_block(nf, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv_block(nf+gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv_block(nf+2*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv_block(nf+3*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        if mode == 'CNA':\n",
        "            last_act = None\n",
        "        else:\n",
        "            last_act = act_type\n",
        "        self.conv5 = conv_block(nf+4*gc, nf, 3, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=last_act, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
        "        if self.conv1x1:\n",
        "            x2 = x2 + self.conv1x1(x) #+\n",
        "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
        "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
        "        if self.conv1x1:\n",
        "            x4 = x4 + x2 #+\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        if self.noise:\n",
        "            return self.noise(x5.mul(0.2) + x)\n",
        "        else:\n",
        "            return x5 * 0.2 + x\n",
        "\n",
        "\n",
        "####################\n",
        "# RRDBNet Generator (modified/\"new\" architecture)\n",
        "####################\n",
        "\n",
        "\n",
        "class MRRDBNet(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, nf, nb, gc=32):\n",
        "        super(MRRDBNet, self).__init__()\n",
        "        RRDB_block_f = functools.partial(RRDBM, nf=nf, gc=gc)\n",
        "\n",
        "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
        "        self.RRDB_trunk = make_layer(RRDB_block_f, nb)\n",
        "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        #### upsampling\n",
        "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fea = self.conv_first(x)\n",
        "        trunk = self.trunk_conv(self.RRDB_trunk(fea))\n",
        "        fea = fea + trunk\n",
        "\n",
        "        fea = self.lrelu(self.upconv1(torch.nn.functional.interpolate(fea, scale_factor=2, mode='nearest')))\n",
        "        fea = self.lrelu(self.upconv2(torch.nn.functional.interpolate(fea, scale_factor=2, mode='nearest')))\n",
        "        out = self.conv_last(self.lrelu(self.HRconv(fea)))\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResidualDenseBlock_5CM(nn.Module):\n",
        "    '''\n",
        "    Residual Dense Block\n",
        "    '''\n",
        "    def __init__(self, nf=64, gc=32, bias=True):\n",
        "        super(ResidualDenseBlock_5CM, self).__init__()\n",
        "        # gc: growth channel, i.e. intermediate channels\n",
        "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        # initialization\n",
        "        default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], scale=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "class RRDBM(nn.Module):\n",
        "    '''Residual in Residual Dense Block'''\n",
        "\n",
        "    def __init__(self, nf, gc=32):\n",
        "        super(RRDBM, self).__init__()\n",
        "        self.RDB1 = ResidualDenseBlock_5CM(nf, gc)\n",
        "        self.RDB2 = ResidualDenseBlock_5CM(nf, gc)\n",
        "        self.RDB3 = ResidualDenseBlock_5CM(nf, gc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.RDB1(x)\n",
        "        out = self.RDB2(out)\n",
        "        out = self.RDB3(out)\n",
        "        return out * 0.2 + x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msE-RbkL0LQ9"
      },
      "source": [
        "# Other Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "WOZN9Qjd4-Xe"
      },
      "source": [
        "#@title [Deoldify](https://github.com/alfagao/DeOldify) (2018) (dataloader not implemented)\n",
        "\n",
        "\"\"\"\n",
        "torch_imports.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fastai/torch_imports.py\n",
        "\n",
        "conv_learner.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fastai/conv_learner.py\n",
        "\n",
        "model.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fastai/model.py\n",
        "\n",
        "modules.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fasterai/modules.py\n",
        "\n",
        "generators.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fasterai/generators.py\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.spectral_norm import spectral_norm\n",
        "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.models import vgg16_bn, vgg19_bn\n",
        "\n",
        "def vgg16(pre): return children(vgg16_bn(pre))[0]\n",
        "def vgg19(pre): return children(vgg19_bn(pre))[0]\n",
        "\n",
        "def cut_model(m, cut):\n",
        "    return list(m.children())[:cut] if cut else [m]\n",
        "\"\"\"\n",
        "model_meta = {\n",
        "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
        "    vgg16:[0,22], vgg19:[0,22],\n",
        "    resnext50:[8,6], resnext101:[8,6], resnext101_64:[8,6],\n",
        "    wrn:[8,6], inceptionresnet_2:[-2,9], inception_4:[-1,9],\n",
        "    dn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "model_meta = {\n",
        "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
        "    vgg16:[0,22], vgg19:[0,22],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class ConvBlock(pl.LightningModule):\n",
        "    def __init__(self, ni:int, no:int, ks:int=3, stride:int=1, pad:int=None, actn:bool=True, \n",
        "            bn:bool=True, bias:bool=True, sn:bool=False, leakyReLu:bool=False, self_attention:bool=False,\n",
        "            inplace_relu:bool=True):\n",
        "        super().__init__()   \n",
        "        if pad is None: pad = ks//2//stride\n",
        "\n",
        "        if sn:\n",
        "            layers = [spectral_norm(nn.Conv2d(ni, no, ks, stride, padding=pad, bias=bias))]\n",
        "        else:\n",
        "            layers = [nn.Conv2d(ni, no, ks, stride, padding=pad, bias=bias)]\n",
        "        if actn:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=inplace_relu)) if leakyReLu else layers.append(nn.ReLU(inplace=inplace_relu)) \n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(no))\n",
        "        if self_attention:\n",
        "            layers.append(SelfAttention(no, 1))\n",
        "\n",
        "        self.seq = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class UpSampleBlock(pl.LightningModule):\n",
        "    @staticmethod\n",
        "    def _conv(ni:int, nf:int, ks:int=3, bn:bool=True, sn:bool=False, leakyReLu:bool=False):\n",
        "        layers = [ConvBlock(ni, nf, ks=ks, sn=sn, bn=bn, actn=False, leakyReLu=leakyReLu)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    @staticmethod\n",
        "    def _icnr(x:torch.Tensor, scale:int=2):\n",
        "        init=nn.init.kaiming_normal_\n",
        "        new_shape = [int(x.shape[0] / (scale ** 2))] + list(x.shape[1:])\n",
        "        subkernel = torch.zeros(new_shape)\n",
        "        subkernel = init(subkernel)\n",
        "        subkernel = subkernel.transpose(0, 1)\n",
        "        subkernel = subkernel.contiguous().view(subkernel.shape[0],\n",
        "                                                subkernel.shape[1], -1)\n",
        "        kernel = subkernel.repeat(1, 1, scale ** 2)\n",
        "        transposed_shape = [x.shape[1]] + [x.shape[0]] + list(x.shape[2:])\n",
        "        kernel = kernel.contiguous().view(transposed_shape)\n",
        "        kernel = kernel.transpose(0, 1)\n",
        "        return kernel\n",
        "\n",
        "    def __init__(self, ni:int, nf:int, scale:int=2, ks:int=3, bn:bool=True, sn:bool=False, leakyReLu:bool=False):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        assert (math.log(scale,2)).is_integer()\n",
        "\n",
        "        for i in range(int(math.log(scale,2))):\n",
        "            layers += [UpSampleBlock._conv(ni, nf*4,ks=ks, bn=bn, sn=sn, leakyReLu=leakyReLu), \n",
        "                nn.PixelShuffle(2)]\n",
        "            if bn:\n",
        "                layers += [nn.BatchNorm2d(nf)]\n",
        "\n",
        "            ni = nf\n",
        "                       \n",
        "        self.sequence = nn.Sequential(*layers)\n",
        "        self._icnr_init()\n",
        "        \n",
        "    def _icnr_init(self):\n",
        "        conv_shuffle = self.sequence[0][0].seq[0]\n",
        "        kernel = UpSampleBlock._icnr(conv_shuffle.weight)\n",
        "        conv_shuffle.weight.data.copy_(kernel)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.sequence(x)\n",
        "\n",
        "\n",
        "class UnetBlock(pl.LightningModule):\n",
        "    def __init__(self, up_in:int , x_in:int , n_out:int, bn:bool=True, sn:bool=False, leakyReLu:bool=False, \n",
        "            self_attention:bool=False, inplace_relu:bool=True):\n",
        "        super().__init__()\n",
        "        up_out = x_out = n_out//2\n",
        "        self.x_conv  = ConvBlock(x_in,  x_out,  ks=1, bn=False, actn=False, sn=sn, inplace_relu=inplace_relu)\n",
        "        self.tr_conv = UpSampleBlock(up_in, up_out, 2, bn=bn, sn=sn, leakyReLu=leakyReLu)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=inplace_relu) if leakyReLu else nn.ReLU(inplace=inplace_relu)\n",
        "        out_layers = []\n",
        "        if bn: \n",
        "            out_layers.append(nn.BatchNorm2d(n_out))\n",
        "        if self_attention:\n",
        "            out_layers.append(SelfAttention(n_out))\n",
        "        self.out = nn.Sequential(*out_layers)\n",
        "        \n",
        "        \n",
        "    def forward(self, up_p:int, x_p:int):\n",
        "        up_p = self.tr_conv(up_p)\n",
        "        x_p = self.x_conv(x_p)\n",
        "        x = torch.cat([up_p,x_p], dim=1)\n",
        "        x = self.relu(x)\n",
        "        return self.out(x)\n",
        "\n",
        "class SaveFeatures():\n",
        "    features=None\n",
        "    def __init__(self, m:pl.LightningModule): \n",
        "        self.hook = m.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output): \n",
        "        self.features = output\n",
        "    def remove(self): \n",
        "        self.hook.remove()\n",
        "\n",
        "class SelfAttention(pl.LightningModule):\n",
        "    def __init__(self, in_channel:int, gain:int=1):\n",
        "        super().__init__()\n",
        "        self.query = self._spectral_init(nn.Conv1d(in_channel, in_channel // 8, 1),gain=gain)\n",
        "        self.key = self._spectral_init(nn.Conv1d(in_channel, in_channel // 8, 1),gain=gain)\n",
        "        self.value = self._spectral_init(nn.Conv1d(in_channel, in_channel, 1), gain=gain)\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def _spectral_init(self, module:pl.LightningModule, gain:int=1):\n",
        "        nn.init.kaiming_uniform_(module.weight, gain)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "        return spectral_norm(module)\n",
        "\n",
        "    def forward(self, input:torch.Tensor):\n",
        "        shape = input.shape\n",
        "        flatten = input.view(shape[0], shape[1], -1)\n",
        "        query = self.query(flatten).permute(0, 2, 1)\n",
        "        key = self.key(flatten)\n",
        "        value = self.value(flatten)\n",
        "        query_key = torch.bmm(query, key)\n",
        "        attn = F.softmax(query_key, 1)\n",
        "        attn = torch.bmm(value, attn)\n",
        "        attn = attn.view(*shape)\n",
        "        out = self.gamma * attn + input\n",
        "        return out\n",
        "\n",
        "\n",
        "class GeneratorModule(ABC, nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def set_trainable(self, trainable:bool):\n",
        "        set_trainable(self, trainable)\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_layer_groups(self, precompute:bool=False)->[]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x_in:torch.Tensor, max_render_sz:int=400):\n",
        "        pass\n",
        "        \n",
        "    def freeze_to(self, n:int):\n",
        "        c=self.get_layer_groups()\n",
        "        for l in c:     set_trainable(l, False)\n",
        "        for l in c[n:]: set_trainable(l, True)\n",
        "\n",
        "    def get_device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "\n",
        "class AbstractUnet(GeneratorModule): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__()\n",
        "        assert (math.log(scale,2)).is_integer()\n",
        "        self.rn, self.lr_cut = self._get_pretrained_resnet_base()\n",
        "        ups = self._get_decoding_layers(nf_factor=nf_factor, scale=scale)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.up1 = ups[0]\n",
        "        self.up2 = ups[1]\n",
        "        self.up3 = ups[2]\n",
        "        self.up4 = ups[3]\n",
        "        self.up5 = ups[4]\n",
        "        self.out= nn.Sequential(ConvBlock(32*nf_factor, 3, ks=3, actn=False, bn=False, sn=True), nn.Tanh())\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        pass\n",
        "\n",
        "    #Gets around irritating inconsistent halving coming from resnet\n",
        "    def _pad(self, x:torch.Tensor, target:torch.Tensor, total_padh:int, total_padw:int)-> torch.Tensor:\n",
        "        h = x.shape[2] \n",
        "        w = x.shape[3]\n",
        "\n",
        "        target_h = target.shape[2]*2\n",
        "        target_w = target.shape[3]*2\n",
        "\n",
        "        if h<target_h or w<target_w:\n",
        "            padh = target_h-h if target_h > h else 0\n",
        "            total_padh = total_padh + padh\n",
        "            padw = target_w-w if target_w > w else 0\n",
        "            total_padw = total_padw + padw\n",
        "            return (F.pad(x, (0,padw,0,padh), \"reflect\",0), total_padh, total_padw)\n",
        "\n",
        "        return (x, total_padh, total_padw)\n",
        "\n",
        "    def _remove_padding(self, x:torch.Tensor, padh:int, padw:int)->torch.Tensor:\n",
        "        if padw == 0 and padh == 0:\n",
        "            return x \n",
        "        \n",
        "        target_h = x.shape[2]-padh\n",
        "        target_w = x.shape[3]-padw\n",
        "        return x[:,:,:target_h, :target_w]\n",
        "\n",
        "    def _encode(self, x:torch.Tensor):\n",
        "        x = self.rn[0](x)\n",
        "        x = self.rn[1](x)\n",
        "        x = self.rn[2](x)\n",
        "        enc0 = x\n",
        "        x = self.rn[3](x)\n",
        "        x = self.rn[4](x)\n",
        "        enc1 = x\n",
        "        x = self.rn[5](x)\n",
        "        enc2 = x\n",
        "        x = self.rn[6](x)\n",
        "        enc3 = x\n",
        "        x = self.rn[7](x)\n",
        "        return (x, enc0, enc1, enc2, enc3)\n",
        "\n",
        "    def _decode(self, x:torch.Tensor, enc0:torch.Tensor, enc1:torch.Tensor, enc2:torch.Tensor, enc3:torch.Tensor):\n",
        "        padh = 0\n",
        "        padw = 0\n",
        "        x = self.relu(x)\n",
        "        enc3, padh, padw = self._pad(enc3, x, padh, padw)\n",
        "        x = self.up1(x, enc3)\n",
        "        enc2, padh, padw  = self._pad(enc2, x, padh, padw)\n",
        "        x = self.up2(x, enc2)\n",
        "        enc1, padh, padw  = self._pad(enc1, x, padh, padw)\n",
        "        x = self.up3(x, enc1)\n",
        "        enc0, padh, padw  = self._pad(enc0, x, padh, padw)\n",
        "        x = self.up4(x, enc0)\n",
        "        #This is a bit too much padding being removed, but I \n",
        "        #haven't yet figured out a good way to determine what \n",
        "        #exactly should be removed.  This is consistently more \n",
        "        #than enough though.\n",
        "        x = self.up5(x)\n",
        "        x = self.out(x)\n",
        "        x = self._remove_padding(x, padh, padw)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        x, enc0, enc1, enc2, enc3 = self._encode(x)\n",
        "        x = self._decode(x, enc0, enc1, enc2, enc3)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_groups(self, precompute:bool=False)->[]:\n",
        "        lgs = list(split_by_idxs(children(self.rn), [self.lr_cut]))\n",
        "        return lgs + [children(self)[1:]]\n",
        "    \n",
        "    def close(self):\n",
        "        for sf in self.sfs: \n",
        "            sf.remove()\n",
        "\n",
        "\n",
        "class Unet34(AbstractUnet): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__(nf_factor=nf_factor, scale=scale)\n",
        "\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        f = resnet34\n",
        "        cut,lr_cut = model_meta[f]\n",
        "        cut-=layers_cut\n",
        "        layers = cut_model(f(True), cut)\n",
        "        return nn.Sequential(*layers), lr_cut\n",
        "\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        self_attention=True\n",
        "        bn=True\n",
        "        sn=True\n",
        "        leakyReLu=False\n",
        "        layers = []\n",
        "        layers.append(UnetBlock(512,256,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,128,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,512*nf_factor, sn=sn, self_attention=self_attention, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,256*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UpSampleBlock(256*nf_factor, 32*nf_factor, 2*scale, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        return layers \n",
        "\n",
        "\n",
        "class Unet101(AbstractUnet): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__(nf_factor=nf_factor, scale=scale)\n",
        "\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        f = resnet101\n",
        "        cut,lr_cut = model_meta[f]\n",
        "        cut-=layers_cut\n",
        "        layers = cut_model(f(True), cut)\n",
        "        return nn.Sequential(*layers), lr_cut\n",
        "\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        self_attention=True\n",
        "        bn=True\n",
        "        sn=True\n",
        "        leakyReLu=False\n",
        "        layers = []\n",
        "        layers.append(UnetBlock(2048,1024,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,512,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,256,512*nf_factor, sn=sn, self_attention=self_attention, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,256*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UpSampleBlock(256*nf_factor, 32*nf_factor, 2*scale, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        return layers \n",
        "\n",
        "class Unet152(AbstractUnet): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__(nf_factor=nf_factor, scale=scale)\n",
        "\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        f = resnet152\n",
        "        cut,lr_cut = model_meta[f]\n",
        "        cut-=layers_cut\n",
        "        layers = cut_model(f(True), cut)\n",
        "        return nn.Sequential(*layers), lr_cut\n",
        "\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        self_attention=True\n",
        "        bn=True\n",
        "        sn=True\n",
        "        leakyReLu=False\n",
        "        layers = []\n",
        "        layers.append(UnetBlock(2048,1024,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,512,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,256,512*nf_factor, sn=sn, self_attention=self_attention, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,256*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UpSampleBlock(256*nf_factor, 32*nf_factor, 2*scale, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        return layers \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahBsPQZVVRfF"
      },
      "source": [
        "# Experimental Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysOi9GqGVXJS",
        "cellView": "form"
      },
      "source": [
        "#@title DSNetRRDB\n",
        "#@markdown Combining DSNet and RRDB, where DSNet is the first stange and RRDB is the second stage.\n",
        "\"\"\"\n",
        "DSNet.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/DSNet.py\n",
        "\n",
        "RegionNorm.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/RegionNorm.py\n",
        "\n",
        "ValidMigration.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/ValidMigration.py\n",
        "\n",
        "Attention.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/Attention.py\n",
        "\n",
        "deform_conv.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/deform_conv.py\n",
        "\"\"\"\n",
        "#from modules.Attention import PixelContextualAttention\n",
        "#from modules.RegionNorm import RBNModule, RCNModule\n",
        "#from modules.ValidMigration import ConvOffset2D\n",
        "#from modules.deform_conv import th_batch_map_offsets, th_generate_grid\n",
        "from __future__ import absolute_import, division\n",
        "from scipy.ndimage.interpolation import map_coordinates as sp_map_coordinates\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "def th_flatten(a):\n",
        "    \"\"\"Flatten tensor\"\"\"\n",
        "    return a.contiguous().view(a.nelement())\n",
        "\n",
        "\n",
        "def th_repeat(a, repeats, axis=0):\n",
        "    \"\"\"Torch version of np.repeat for 1D\"\"\"\n",
        "    assert len(a.size()) == 1\n",
        "    return th_flatten(torch.transpose(a.repeat(repeats, 1), 0, 1))\n",
        "\n",
        "\n",
        "def np_repeat_2d(a, repeats):\n",
        "    \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n",
        "\n",
        "    assert len(a.shape) == 2\n",
        "    a = np.expand_dims(a, 0)\n",
        "    a = np.tile(a, [repeats, 1, 1])\n",
        "    return a\n",
        "\n",
        "\n",
        "def th_gather_2d(input, coords):\n",
        "    inds = coords[:, 0]*input.size(1) + coords[:, 1]\n",
        "    x = torch.index_select(th_flatten(input), 0, inds)\n",
        "    return x.view(coords.size(0))\n",
        "\n",
        "\n",
        "def th_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n",
        "    Note that coords is transposed and only 2D is supported\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (s, s)\n",
        "    coords : tf.Tensor. shape = (n_points, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    assert order == 1\n",
        "    input_size = input.size(0)\n",
        "\n",
        "    coords = torch.clamp(coords, 0, input_size - 1)\n",
        "    coords_lt = coords.floor().long()\n",
        "    coords_rb = coords.ceil().long()\n",
        "    coords_lb = torch.stack([coords_lt[:, 0], coords_rb[:, 1]], 1)\n",
        "    coords_rt = torch.stack([coords_rb[:, 0], coords_lt[:, 1]], 1)\n",
        "\n",
        "    vals_lt = th_gather_2d(input,  coords_lt.detach())\n",
        "    vals_rb = th_gather_2d(input,  coords_rb.detach())\n",
        "    vals_lb = th_gather_2d(input,  coords_lb.detach())\n",
        "    vals_rt = th_gather_2d(input,  coords_rt.detach())\n",
        "\n",
        "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
        "\n",
        "    vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n",
        "    vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n",
        "    mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_coordinates(inputs, coords):\n",
        "    \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n",
        "    # coords = coords.clip(0, inputs.shape[1] - 1)\n",
        "\n",
        "    assert (coords.shape[2] == 2)\n",
        "    height = coords[:,:,0].clip(0, inputs.shape[1] - 1)\n",
        "    width = coords[:,:,1].clip(0, inputs.shape[2] - 1)\n",
        "    np.concatenate((np.expand_dims(height, axis=2), np.expand_dims(width, axis=2)), 2)\n",
        "\n",
        "    mapped_vals = np.array([\n",
        "        sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n",
        "        for input, coord in zip(inputs, coords)\n",
        "    ])\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def th_batch_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Batch version of th_map_coordinates\n",
        "    Only supports 2D feature maps\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (b, s, s)\n",
        "    coords : tf.Tensor. shape = (b, n_points, 2)\n",
        "    Returns\n",
        "    -------\n",
        "    tf.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = input.size(0)\n",
        "    input_height = input.size(1)\n",
        "    input_width = input.size(2)\n",
        "\n",
        "    n_coords = coords.size(1)\n",
        "\n",
        "    # coords = torch.clamp(coords, 0, input_size - 1)\n",
        "\n",
        "    coords = torch.cat((torch.clamp(coords.narrow(2, 0, 1), 0, input_height - 1), torch.clamp(coords.narrow(2, 1, 1), 0, input_width - 1)), 2)\n",
        "\n",
        "    assert (coords.size(1) == n_coords)\n",
        "\n",
        "    coords_lt = coords.floor().long()\n",
        "    coords_rb = coords.ceil().long()\n",
        "    coords_lb = torch.stack([coords_lt[..., 0], coords_rb[..., 1]], 2)\n",
        "    coords_rt = torch.stack([coords_rb[..., 0], coords_lt[..., 1]], 2)\n",
        "    idx = th_repeat(torch.arange(0, batch_size), n_coords).long()\n",
        "    idx = Variable(idx, requires_grad=False)\n",
        "    if input.is_cuda:\n",
        "        idx = idx.cuda()\n",
        "\n",
        "    def _get_vals_by_coords(input, coords):\n",
        "        indices = torch.stack([\n",
        "            idx, th_flatten(coords[..., 0]), th_flatten(coords[..., 1])\n",
        "        ], 1)\n",
        "        inds = indices[:, 0]*input.size(1)*input.size(2)+ indices[:, 1]*input.size(2) + indices[:, 2]\n",
        "        vals = th_flatten(input).index_select(0, inds)\n",
        "        vals = vals.view(batch_size, n_coords)\n",
        "        return vals\n",
        "\n",
        "    vals_lt = _get_vals_by_coords(input, coords_lt.detach())\n",
        "    vals_rb = _get_vals_by_coords(input, coords_rb.detach())\n",
        "    vals_lb = _get_vals_by_coords(input, coords_lb.detach())\n",
        "    vals_rt = _get_vals_by_coords(input, coords_rt.detach())\n",
        "\n",
        "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
        "    vals_t = coords_offset_lt[..., 0]*(vals_rt - vals_lt) + vals_lt\n",
        "    vals_b = coords_offset_lt[..., 0]*(vals_rb - vals_lb) + vals_lb\n",
        "    mapped_vals = coords_offset_lt[..., 1]* (vals_b - vals_t) + vals_t\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_offsets(input, offsets):\n",
        "    \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n",
        "\n",
        "    batch_size = input.shape[0]\n",
        "    input_height = input.shape[1]\n",
        "    input_width = input.shape[2]\n",
        "\n",
        "    offsets = offsets.reshape(batch_size, -1, 2)\n",
        "    grid = np.stack(np.mgrid[:input_height, :input_width], -1).reshape(-1, 2)\n",
        "    grid = np.repeat([grid], batch_size, axis=0)\n",
        "    coords = offsets + grid\n",
        "    # coords = coords.clip(0, input_size - 1)\n",
        "\n",
        "    mapped_vals = sp_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def th_generate_grid(batch_size, input_height, input_width, dtype, cuda):\n",
        "    grid = np.meshgrid(\n",
        "        range(input_height), range(input_width), indexing='ij'\n",
        "    )\n",
        "    grid = np.stack(grid, axis=-1)\n",
        "    grid = grid.reshape(-1, 2)\n",
        "\n",
        "    grid = np_repeat_2d(grid, batch_size)\n",
        "    grid = torch.from_numpy(grid).type(dtype)\n",
        "    if cuda:\n",
        "        grid = grid.cuda()\n",
        "    return Variable(grid, requires_grad=False)\n",
        "\n",
        "\n",
        "def th_batch_map_offsets(input, offsets, grid=None, order=1):\n",
        "    \"\"\"Batch map offsets into input\n",
        "    Parameters\n",
        "    ---------\n",
        "    input : torch.Tensor. shape = (b, s, s)\n",
        "    offsets: torch.Tensor. shape = (b, s, s, 2)\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "    batch_size = input.size(0)\n",
        "    input_height = input.size(1)\n",
        "    input_width = input.size(2)\n",
        "\n",
        "    offsets = offsets.view(batch_size, -1, 2)\n",
        "    if grid is None:\n",
        "        grid = th_generate_grid(batch_size, input_height, input_width, offsets.data.type(), offsets.data.is_cuda)\n",
        "\n",
        "    coords = offsets + grid\n",
        "\n",
        "    mapped_vals = th_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "class SEModule(pl.LightningModule):\n",
        "    def __init__(self, num_channel, squeeze_ratio=1.0):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.sequeeze_mod = nn.AdaptiveAvgPool2d(1)\n",
        "        self.num_channel = num_channel\n",
        "\n",
        "        blocks = [nn.Linear(num_channel, int(num_channel * squeeze_ratio)),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(int(num_channel * squeeze_ratio), num_channel),\n",
        "                  nn.Sigmoid()]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ori = x\n",
        "        x = self.sequeeze_mod(x)\n",
        "        x = x.view(x.size(0), 1, self.num_channel)\n",
        "        x = self.blocks(x)\n",
        "        x = x.view(x.size(0), self.num_channel, 1, 1)\n",
        "        x = ori * x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ContextualAttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, patch_size=3, propagate_size=3, stride=1):\n",
        "        super(ContextualAttentionModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        ###assume the masked area has value 1\n",
        "        bz, nc, w, h = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background * masks\n",
        "        background = F.pad(background,\n",
        "                           [self.patch_size // 2, self.patch_size // 2, self.patch_size // 2, self.patch_size // 2])\n",
        "        conv_kernels_all = background.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size,\n",
        "                                                                                     self.stride).contiguous().view(bz,\n",
        "                                                                                                                    nc,\n",
        "                                                                                                                    -1,\n",
        "                                                                                                                    self.patch_size,\n",
        "                                                                                                                    self.patch_size)\n",
        "        conv_kernels_all = conv_kernels_all.transpose(2, 1)\n",
        "        output_tensor = []\n",
        "        for i in range(bz):\n",
        "            mask = masks[i:i + 1]\n",
        "            feature_map = foreground[i:i + 1].contiguous()\n",
        "            # form convolutional kernels\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels ** 2, [1, 2, 3], keepdim=True) ** 0.5\n",
        "            conv_kernels = conv_kernels / norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding=self.patch_size // 2)\n",
        "            \"\"\"\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.conv2d(conv_result, self.prop_kernels, stride=1, padding=1, groups=conv_result.size(1))\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "            self.prop_kernels.requires_grad = False\n",
        "            self.prop_kernels = self.prop_kernels.cuda()\n",
        "            conv_result = F.conv2d(conv_result, self.prop_kernels, stride=1, padding=1, groups=conv_result.size(1))\n",
        "            \n",
        "            attention_scores = F.softmax(conv_result, dim=1)\n",
        "            ##propagate the scores\n",
        "            recovered_foreground = F.conv_transpose2d(attention_scores, conv_kernels, stride=1,\n",
        "                                                      padding=self.patch_size // 2)\n",
        "            # average the recovered value, at the same time make non-masked area 0\n",
        "            recovered_foreground = (recovered_foreground * (1 - mask)) / (self.patch_size ** 2)\n",
        "            # recover the image\n",
        "            final_output = recovered_foreground + feature_map * mask\n",
        "            output_tensor.append(final_output)\n",
        "        return torch.cat(output_tensor, dim=0)\n",
        "\n",
        "\n",
        "class PixelContextualAttention(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, inchannel, patch_size_list=[1], propagate_size_list=[3], stride_list=[1]):\n",
        "        assert isinstance(patch_size_list,\n",
        "                          list), \"patch_size should be a list containing scales, or you should use Contextual Attention to initialize your module\"\n",
        "        assert len(patch_size_list) == len(propagate_size_list) and len(propagate_size_list) == len(\n",
        "            stride_list), \"the input_lists should have same lengths\"\n",
        "        super(PixelContextualAttention, self).__init__()\n",
        "        for i in range(len(patch_size_list)):\n",
        "            name = \"CA_{:d}\".format(i)\n",
        "            setattr(self, name, ContextualAttentionModule(patch_size_list[i], propagate_size_list[i], stride_list[i]))\n",
        "        self.num_of_modules = len(patch_size_list)\n",
        "        self.SqueezeExc = SEModule(inchannel * 2)\n",
        "        self.combiner = nn.Conv2d(inchannel * 2, inchannel, kernel_size=1)\n",
        "\n",
        "    def forward(self, foreground, mask):\n",
        "        outputs = [foreground]\n",
        "        for i in range(self.num_of_modules):\n",
        "            name = \"CA_{:d}\".format(i)\n",
        "            CA_module = getattr(self, name)\n",
        "            outputs.append(CA_module(foreground, mask))\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        outputs = self.SqueezeExc(outputs)\n",
        "        outputs = self.combiner(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvOffset2D(nn.Conv2d):\n",
        "    \"\"\"ConvOffset2D\n",
        "\n",
        "    Convolutional layer responsible for learning the 2D offsets and output the\n",
        "    deformed feature map using bilinear interpolation\n",
        "\n",
        "    Note that this layer does not perform convolution on the deformed feature\n",
        "    map. See get_deform_cnn in cnn.py for usage\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n",
        "        \"\"\"Init\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filters : int\n",
        "            Number of channel of the input feature map\n",
        "        init_normal_stddev : float\n",
        "            Normal kernel initialization\n",
        "        **kwargs:\n",
        "            Pass to superclass. See Con2d layer in pytorch\n",
        "        \"\"\"\n",
        "        self.filters = filters\n",
        "        self._grid_param = None\n",
        "        super(ConvOffset2D, self).__init__(self.filters, self.filters*2, 3, padding=1, bias=False, **kwargs)\n",
        "        self.weight.data.copy_(self._init_weights(self.weight, init_normal_stddev))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Return the deformed featured map\"\"\"\n",
        "        x_shape = x.size()\n",
        "        offsets = super(ConvOffset2D, self).forward(x)\n",
        "\n",
        "        # offsets: (b*c, h, w, 2)\n",
        "        offsets = self._to_bc_h_w_2(offsets, x_shape)\n",
        "\n",
        "        # x: (b*c, h, w)\n",
        "        x = self._to_bc_h_w(x, x_shape)\n",
        "\n",
        "        # X_offset: (b*c, h, w)\n",
        "        x_offset = th_batch_map_offsets(x, offsets, grid=self._get_grid(self,x))\n",
        "\n",
        "        # x_offset: (b, h, w, c)\n",
        "        x_offset = self._to_b_c_h_w(x_offset, x_shape)\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_grid(self, x):\n",
        "        batch_size, input_height, input_width = x.size(0), x.size(1), x.size(2)\n",
        "        dtype, cuda = x.data.type(), x.data.is_cuda\n",
        "        if self._grid_param == (batch_size, input_height, input_width, dtype, cuda):\n",
        "            return self._grid\n",
        "        self._grid_param = (batch_size, input_height, input_width, dtype, cuda)\n",
        "        self._grid = th_generate_grid(batch_size, input_height, input_width, dtype, cuda)\n",
        "        return self._grid\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(weights, std):\n",
        "        fan_out = weights.size(0)\n",
        "        fan_in = weights.size(1) * weights.size(2) * weights.size(3)\n",
        "        w = np.random.normal(0.0, std, (fan_out, fan_in))\n",
        "        return torch.from_numpy(w.reshape(weights.size()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w_2(x, x_shape):\n",
        "        \"\"\"(b, 2c, h, w) -> (b*c, h, w, 2)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]), 2)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w(x, x_shape):\n",
        "        \"\"\"(b, c, h, w) -> (b*c, h, w)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_b_c_h_w(x, x_shape):\n",
        "        \"\"\"(b*c, h, w) -> (b, c, h, w)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RBNModule(pl.LightningModule):\n",
        "    _version = 2\n",
        "    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n",
        "                     'running_mean', 'running_var', 'num_batches_tracked']\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, affine=True, track_running_stats=True):\n",
        "        super(RBNModule, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.momentum = momentum\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1, 1))\n",
        "            self.register_buffer('running_var', torch.ones(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            nn.init.uniform_(self.weight)\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, mask_t):\n",
        "        input_m = input * mask_t\n",
        "        if self.training:\n",
        "            mask_mean = torch.mean(mask_t, (0, 2, 3), True)\n",
        "            x_mean = torch.mean(input_m, (0, 2, 3), True) / mask_mean\n",
        "            x_var = torch.mean(((input_m - x_mean) * mask_t) ** 2, (0, 2, 3), True) / mask_mean\n",
        "\n",
        "            x_out = self.weight * (input_m - x_mean) / torch.sqrt(x_var + self.eps) + self.bias\n",
        "\n",
        "            self.running_mean.mul_(self.momentum)\n",
        "            self.running_mean.add_((1 - self.momentum) * x_mean.data)\n",
        "            self.running_var.mul_(self.momentum)\n",
        "            self.running_var.add_((1 - self.momentum) * x_var.data)\n",
        "        else:\n",
        "            x_out = self.weight * (input_m - self.running_mean) / torch.sqrt(self.running_var + self.eps) + self.bias\n",
        "        return x_out * mask_t + input * (1 - mask_t)\n",
        "\n",
        "\n",
        "class RCNModule(pl.LightningModule):\n",
        "    _version = 2\n",
        "    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n",
        "                     'running_mean', 'running_var', 'num_batches_tracked']\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, affine=True, track_running_stats=True):\n",
        "        super(RCNModule, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.momentum = momentum\n",
        "        self.mean_weight = nn.Parameter(torch.ones(3))\n",
        "        self.var_weight = nn.Parameter(torch.ones(3))\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1, 1))\n",
        "            self.register_buffer('running_var', torch.ones(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            nn.init.uniform_(self.weight)\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, mask_t):\n",
        "        input_m = input * mask_t\n",
        "\n",
        "        if self.training:\n",
        "            mask_mean_bn = torch.mean(mask_t, (0, 2, 3), True)\n",
        "            mean_bn = torch.mean(input_m, (0, 2, 3), True) / mask_mean_bn\n",
        "            var_bn = torch.mean(((input_m - mean_bn) * mask_t) ** 2, (0, 2, 3), True) / mask_mean_bn\n",
        "\n",
        "            self.running_mean.mul_(self.momentum)\n",
        "            self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
        "            self.running_var.mul_(self.momentum)\n",
        "            self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
        "        else:\n",
        "            mean_bn = torch.autograd.Variable(self.running_mean)\n",
        "            var_bn = torch.autograd.Variable(self.running_var)\n",
        "\n",
        "        mask_mean_in = torch.mean(mask_t, (2, 3), True)\n",
        "        mean_in = torch.mean(input_m, (2, 3), True) / mask_mean_in\n",
        "        var_in = torch.mean(((input_m - mean_in) * mask_t) ** 2, (2, 3), True) / mask_mean_in\n",
        "\n",
        "        mask_mean_ln = torch.mean(mask_t, (1, 2, 3), True)\n",
        "        mean_ln = torch.mean(input_m, (1, 2, 3), True) / mask_mean_ln\n",
        "        var_ln = torch.mean(((input_m - mean_ln) * mask_t) ** 2, (1, 2, 3), True) / mask_mean_ln\n",
        "\n",
        "        mean_weight = F.softmax(self.mean_weight)\n",
        "        var_weight = F.softmax(self.var_weight)\n",
        "\n",
        "        x_mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn\n",
        "        x_var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn\n",
        "\n",
        "        x_out = self.weight * (input_m - x_mean) / torch.sqrt(x_var + self.eps) + self.bias\n",
        "        return x_out * mask_t + input * (1 - mask_t)\n",
        "\n",
        "\n",
        "class DSModule(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=False, rn=True, sample='none-3', activ='relu',\n",
        "                 conv_bias=False, defor=True):\n",
        "        super().__init__()\n",
        "        if sample == 'down-5':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 5, 2, 2, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(5,2,2)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 7, 2, 3, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(7, 2, 3)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 3, 2, 1, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(3, 2, 1)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_ch+2, out_ch, 3, 1, 1, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(3,1,1)\n",
        "            if defor:\n",
        "                self.offset0 = ConvOffset2D(in_ch-out_ch+1)\n",
        "                self.offset1 = ConvOffset2D(out_ch+1)\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(out_ch)\n",
        "        if rn:\n",
        "            # Regional Composite Normalization\n",
        "            self.rn = RCNModule(out_ch)\n",
        "\n",
        "            # Regional Batch Normalization\n",
        "            # self.rn = RBNModule(out_ch)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2, inplace = True)\n",
        "\n",
        "    def forward(self, input, input_mask):\n",
        "        if hasattr(self, 'offset'):\n",
        "            input = torch.cat([input, input_mask[:,:1,:,:]], dim = 1)\n",
        "            h = self.offset(input)\n",
        "            h = input*input_mask[:,:1,:,:] + (1-input_mask[:,:1,:,:])*h\n",
        "            h = self.conv(h)\n",
        "            h_mask = self.updatemask(input_mask[:,:1,:,:])\n",
        "            h = h*h_mask\n",
        "            h = self.rn(h, h_mask)\n",
        "        elif hasattr(self, 'offset0'):\n",
        "            h1_in = torch.cat([input[:,self.in_ch-self.out_ch:,:,:], input_mask[:,1:,:,:]], dim = 1)\n",
        "            m1_in = input_mask[:,1:,:,:]\n",
        "            h0 = torch.cat([input[:,:self.in_ch-self.out_ch,:,:], input_mask[:,:1,:,:]], dim = 1)\n",
        "            h1 = self.offset1(h1_in)\n",
        "            h1 = m1_in*h1_in + (1-m1_in)*h1\n",
        "            h = self.conv(torch.cat([h0,h1], dim = 1))\n",
        "            h = self.rn(h, input_mask[:,:1,:,:])\n",
        "            h_mask = F.interpolate(input_mask[:,:1,:,:], scale_factor=2, mode='nearest')\n",
        "        else:\n",
        "            h = self.conv(torch.cat([input, input_mask[:,:,:,:]], dim = 1))\n",
        "            h_mask = self.updatemask(input_mask[:,:1,:,:])\n",
        "            h = h*h_mask\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            h = self.bn(h)\n",
        "        if hasattr(self, 'activation'):\n",
        "            h = self.activation(h)\n",
        "        return h, h_mask\n",
        "\n",
        "\n",
        "class DSNet(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, upsampling_mode='nearest'):\n",
        "        super().__init__()\n",
        "        self.freeze_enc_bn = False\n",
        "        self.upsampling_mode = upsampling_mode\n",
        "        self.layer_size = layer_size\n",
        "        self.enc_1 = DSModule(input_channels, 64, rn=False, sample='down-7', defor = False)\n",
        "        self.enc_2 = DSModule(64, 128, sample='down-5')\n",
        "        self.enc_3 = DSModule(128, 256, sample='down-5')\n",
        "        self.enc_4 = DSModule(256, 512, sample='down-3')\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            setattr(self, name, DSModule(512, 512, sample='down-3'))\n",
        "\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'dec_{:d}'.format(i + 1)\n",
        "            setattr(self, name, DSModule(512 + 512, 512, activ='leaky'))\n",
        "        self.dec_4 = DSModule(512 + 256, 256, activ='leaky')\n",
        "        self.dec_3 = DSModule(256 + 128, 128, activ='leaky')\n",
        "        self.dec_2 = DSModule(128 + 64, 64, activ='leaky')\n",
        "        self.dec_1 = DSModule(64 + input_channels, input_channels,\n",
        "                              rn=False, activ=None, defor = False)\n",
        "        self.att = PixelContextualAttention(128)\n",
        "    def forward(self, input, input_mask):\n",
        "        input = input.type(torch.cuda.FloatTensor)\n",
        "        input_mask = input_mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        input_mask = input_mask[:,0:1,:,:]\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "        h_mask_dict = {}  # for the output of enc_N\n",
        "\n",
        "        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(\n",
        "                h_dict[h_key_prev], h_mask_dict[h_key_prev])\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h_key = 'h_{:d}'.format(self.layer_size)\n",
        "        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n",
        "        h_mask = F.interpolate(h_mask, scale_factor=2, mode='nearest')\n",
        "\n",
        "        for i in range(self.layer_size, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i - 1)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "\n",
        "            h = F.interpolate(h, scale_factor=2, mode=self.upsampling_mode)\n",
        "\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n",
        "            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim=1)\n",
        "            h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n",
        "            if i == 3:\n",
        "                h = self.att(h, input_mask[:,:1,:,:])\n",
        "        #return h, h_mask\n",
        "        return h\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "block.py (9-3-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/architectures/block.py\n",
        "\"\"\"\n",
        "\n",
        "def sequential(*args):\n",
        "    # Flatten Sequential. It unwraps nn.Sequential.\n",
        "    if len(args) == 1:\n",
        "        if isinstance(args[0], OrderedDict):\n",
        "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "        return args[0]  # No sequential is needed.\n",
        "    modules = []\n",
        "    for module in args:\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            for submodule in module.children():\n",
        "                modules.append(submodule)\n",
        "        elif isinstance(module, nn.Module):\n",
        "            modules.append(module)\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\"\"\"\n",
        "RRDBNet_arch.py (12-2-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/architectures/RRDBNet_arch.py\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torchvision\n",
        "#from . import block as B\n",
        "import functools\n",
        "#from . import spectral_norm as SN\n",
        "\n",
        "\n",
        "####################\n",
        "# RRDBNet Generator (original architecture)\n",
        "####################\n",
        "\n",
        "class RRDBNet(pl.LightningModule):\n",
        "    def __init__(self, in_nc, out_nc, nf, nb, nr=3, gc=32, upscale=4, norm_type=None, \\\n",
        "            act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D', \\\n",
        "            finalact=None, gaussian_noise=False, plus=False):\n",
        "        super(RRDBNet, self).__init__()\n",
        "        n_upscale = int(math.log(upscale, 2))\n",
        "        if upscale == 3:\n",
        "            n_upscale = 1\n",
        "\n",
        "        fea_conv = conv_block(in_nc, nf, kernel_size=3, norm_type=None, act_type=None, convtype=convtype)\n",
        "        rb_blocks = [RRDB(nf, nr, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero', \\\n",
        "            norm_type=norm_type, act_type=act_type, mode='CNA', convtype=convtype, \\\n",
        "            gaussian_noise=gaussian_noise, plus=plus) for _ in range(nb)]\n",
        "        LR_conv = conv_block(nf, nf, kernel_size=3, norm_type=norm_type, act_type=None, mode=mode, convtype=convtype)\n",
        "\n",
        "        if upsample_mode == 'upconv':\n",
        "            upsample_block = upconv_block\n",
        "        elif upsample_mode == 'pixelshuffle':\n",
        "            upsample_block = pixelshuffle_block\n",
        "        else:\n",
        "            raise NotImplementedError('upsample mode [{:s}] is not found'.format(upsample_mode))\n",
        "        if upscale == 3:\n",
        "            upsampler = upsample_block(nf, nf, 3, act_type=act_type, convtype=convtype)\n",
        "        else:\n",
        "            upsampler = [upsample_block(nf, nf, act_type=act_type, convtype=convtype) for _ in range(n_upscale)]\n",
        "        HR_conv0 = conv_block(nf, nf, kernel_size=3, norm_type=None, act_type=act_type, convtype=convtype)\n",
        "        HR_conv1 = conv_block(nf, out_nc, kernel_size=3, norm_type=None, act_type=None, convtype=convtype)\n",
        "\n",
        "        # Note: this option adds new parameters to the architecture, another option is to use \"outm\" in the forward\n",
        "        outact = act(finalact) if finalact else None\n",
        "        \n",
        "        self.model = sequential(fea_conv, ShortcutBlock(sequential(*rb_blocks, LR_conv)),\\\n",
        "            *upsampler, HR_conv0, HR_conv1, outact)\n",
        "\n",
        "    def forward(self, x, outm=None):\n",
        "        x = self.model(x)\n",
        "        \n",
        "        if outm=='scaltanh': # limit output range to [-1,1] range with tanh and rescale to [0,1] Idea from: https://github.com/goldhuang/SRGAN-PyTorch/blob/master/model.py\n",
        "            return(torch.tanh(x) + 1.0) / 2.0\n",
        "        elif outm=='tanh': # limit output to [-1,1] range\n",
        "            return torch.tanh(x)\n",
        "        elif outm=='sigmoid': # limit output to [0,1] range\n",
        "            return torch.sigmoid(x)\n",
        "        elif outm=='clamp':\n",
        "            return torch.clamp(x, min=0.0, max=1.0)\n",
        "        else: #Default, no cap for the output\n",
        "            return x\n",
        "\n",
        "class RRDB(pl.LightningModule):\n",
        "    '''\n",
        "    Residual in Residual Dense Block\n",
        "    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nf, nr=3, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero', \\\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(RRDB, self).__init__()\n",
        "        # This is for backwards compatibility with existing models\n",
        "        if nr == 3:\n",
        "            self.RDB1 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "            self.RDB2 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "            self.RDB3 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "        else:\n",
        "            RDB_list = [ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type,\n",
        "                                              norm_type, act_type, mode, convtype, spectral_norm=spectral_norm,\n",
        "                                              gaussian_noise=gaussian_noise, plus=plus) for _ in range(nr)]\n",
        "            self.RDBs = nn.Sequential(*RDB_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'RDB1'):\n",
        "            out = self.RDB1(x)\n",
        "            out = self.RDB2(out)\n",
        "            out = self.RDB3(out)\n",
        "        else:\n",
        "            out = self.RDBs(x)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class ResidualDenseBlock_5C(pl.LightningModule):\n",
        "    '''\n",
        "    Residual Dense Block\n",
        "    style: 5 convs\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    Modified options that can be used:\n",
        "        - \"Partial Convolution based Padding\" arXiv:1811.11718\n",
        "        - \"Spectral normalization\" arXiv:1802.05957\n",
        "        - \"ICASSP 2020 - ESRGAN+ : Further Improving ESRGAN\" N. C. \n",
        "            {Rakotonirina} and A. {Rasoanaivo}\n",
        "    \n",
        "    Args:\n",
        "        nf (int): Channel number of intermediate features (num_feat).\n",
        "        gc (int): Channels for each growth (num_grow_ch: growth channel, \n",
        "            i.e. intermediate channels).\n",
        "        convtype (str): the type of convolution to use. Default: 'Conv2D'\n",
        "        gaussian_noise (bool): enable the ESRGAN+ gaussian noise (no new \n",
        "            trainable parameters)\n",
        "        plus (bool): enable the additional residual paths from ESRGAN+ \n",
        "            (adds trainable parameters)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nf=64, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero', \\\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(ResidualDenseBlock_5C, self).__init__()\n",
        "        \n",
        "        ## +\n",
        "        self.noise = GaussianNoise() if gaussian_noise else None\n",
        "        self.conv1x1 = conv1x1(nf, gc) if plus else None\n",
        "        ## +\n",
        "\n",
        "        self.conv1 = conv_block(nf, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv_block(nf+gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv_block(nf+2*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv_block(nf+3*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        if mode == 'CNA':\n",
        "            last_act = None\n",
        "        else:\n",
        "            last_act = act_type\n",
        "        self.conv5 = conv_block(nf+4*gc, nf, 3, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=last_act, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
        "        if self.conv1x1:\n",
        "            x2 = x2 + self.conv1x1(x) #+\n",
        "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
        "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
        "        if self.conv1x1:\n",
        "            x4 = x4 + x2 #+\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        if self.noise:\n",
        "            return self.noise(x5.mul(0.2) + x)\n",
        "        else:\n",
        "            return x5 * 0.2 + x\n",
        "\n",
        "\n",
        "####################\n",
        "# RRDBNet Generator (modified/\"new\" architecture)\n",
        "####################\n",
        "\n",
        "\n",
        "class MRRDBNet(pl.LightningModule):\n",
        "    def __init__(self, in_nc, out_nc, nf, nb, gc=32):\n",
        "        super(MRRDBNet, self).__init__()\n",
        "        RRDB_block_f = functools.partial(RRDBM, nf=nf, gc=gc)\n",
        "\n",
        "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
        "        self.RRDB_trunk = make_layer(RRDB_block_f, nb)\n",
        "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        #### upsampling\n",
        "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fea = self.conv_first(x)\n",
        "        trunk = self.trunk_conv(self.RRDB_trunk(fea))\n",
        "        fea = fea + trunk\n",
        "\n",
        "        fea = self.lrelu(self.upconv1(torch.nn.functional.interpolate(fea, scale_factor=2, mode='nearest')))\n",
        "        fea = self.lrelu(self.upconv2(torch.nn.functional.interpolate(fea, scale_factor=2, mode='nearest')))\n",
        "        out = self.conv_last(self.lrelu(self.HRconv(fea)))\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResidualDenseBlock_5CM(pl.LightningModule):\n",
        "    '''\n",
        "    Residual Dense Block\n",
        "    '''\n",
        "    def __init__(self, nf=64, gc=32, bias=True):\n",
        "        super(ResidualDenseBlock_5CM, self).__init__()\n",
        "        # gc: growth channel, i.e. intermediate channels\n",
        "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        # initialization\n",
        "        default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], scale=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "class RRDBM(pl.LightningModule):\n",
        "    '''Residual in Residual Dense Block'''\n",
        "\n",
        "    def __init__(self, nf, gc=32):\n",
        "        super(RRDBM, self).__init__()\n",
        "        self.RDB1 = ResidualDenseBlock_5CM(nf, gc)\n",
        "        self.RDB2 = ResidualDenseBlock_5CM(nf, gc)\n",
        "        self.RDB3 = ResidualDenseBlock_5CM(nf, gc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.RDB1(x)\n",
        "        out = self.RDB2(out)\n",
        "        out = self.RDB3(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "\n",
        "\n",
        "class DSNetRRDB(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, upsampling_mode='nearest',\n",
        "                in_nc=4, out_nc=3, nf=128, nb=8, gc=32, upscale=1, norm_type=None,\n",
        "                act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D',\n",
        "                finalact=None, gaussian_noise=True, plus=False, \n",
        "                nr=3):\n",
        "      super(DSNetRRDB, self).__init__()\n",
        "      self.netG1 = DSNet(layer_size=layer_size, input_channels=input_channels, upsampling_mode=upsampling_mode)\n",
        "      self.netG2 = RRDBNet(in_nc=in_nc, out_nc=out_nc, nf=nf, nb=nb, gc=gc, upscale=upscale, norm_type=norm_type,\n",
        "                act_type=act_type, mode=mode, upsample_mode=upsample_mode, convtype=convtype,\n",
        "                finalact=finalact, gaussian_noise=gaussian_noise, plus=plus, \n",
        "                nr=nr)\n",
        "    def forward(self, input, input_mask):\n",
        "      result1 = self.netG1(input, input_mask)\n",
        "\n",
        "      result1 = input*input_mask+result1*(1-input_mask)\n",
        "\n",
        "      concat = torch.cat((result1, input_mask), dim=1)\n",
        "      result2 = self.netG2(concat)\n",
        "      return result2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TLJt0017_v7k"
      },
      "source": [
        "#@title DSNetDeoldify\n",
        "#@title DSNetRRDB\n",
        "#@markdown Combining DSNet and Deoldify, where DSNet is the first stange and Deoldify is the second stage.\n",
        "\"\"\"\n",
        "DSNet.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/DSNet.py\n",
        "\n",
        "RegionNorm.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/RegionNorm.py\n",
        "\n",
        "ValidMigration.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/ValidMigration.py\n",
        "\n",
        "Attention.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/Attention.py\n",
        "\n",
        "deform_conv.py (6-3-20)\n",
        "https://github.com/wangning-001/DSNet/blob/afa174a8f8e4fbdeff086fb546c83c871e959141/modules/deform_conv.py\n",
        "\"\"\"\n",
        "#from modules.Attention import PixelContextualAttention\n",
        "#from modules.RegionNorm import RBNModule, RCNModule\n",
        "#from modules.ValidMigration import ConvOffset2D\n",
        "#from modules.deform_conv import th_batch_map_offsets, th_generate_grid\n",
        "from __future__ import absolute_import, division\n",
        "from scipy.ndimage.interpolation import map_coordinates as sp_map_coordinates\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "def th_flatten(a):\n",
        "    \"\"\"Flatten tensor\"\"\"\n",
        "    return a.contiguous().view(a.nelement())\n",
        "\n",
        "\n",
        "def th_repeat(a, repeats, axis=0):\n",
        "    \"\"\"Torch version of np.repeat for 1D\"\"\"\n",
        "    assert len(a.size()) == 1\n",
        "    return th_flatten(torch.transpose(a.repeat(repeats, 1), 0, 1))\n",
        "\n",
        "\n",
        "def np_repeat_2d(a, repeats):\n",
        "    \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n",
        "\n",
        "    assert len(a.shape) == 2\n",
        "    a = np.expand_dims(a, 0)\n",
        "    a = np.tile(a, [repeats, 1, 1])\n",
        "    return a\n",
        "\n",
        "\n",
        "def th_gather_2d(input, coords):\n",
        "    inds = coords[:, 0]*input.size(1) + coords[:, 1]\n",
        "    x = torch.index_select(th_flatten(input), 0, inds)\n",
        "    return x.view(coords.size(0))\n",
        "\n",
        "\n",
        "def th_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n",
        "    Note that coords is transposed and only 2D is supported\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (s, s)\n",
        "    coords : tf.Tensor. shape = (n_points, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    assert order == 1\n",
        "    input_size = input.size(0)\n",
        "\n",
        "    coords = torch.clamp(coords, 0, input_size - 1)\n",
        "    coords_lt = coords.floor().long()\n",
        "    coords_rb = coords.ceil().long()\n",
        "    coords_lb = torch.stack([coords_lt[:, 0], coords_rb[:, 1]], 1)\n",
        "    coords_rt = torch.stack([coords_rb[:, 0], coords_lt[:, 1]], 1)\n",
        "\n",
        "    vals_lt = th_gather_2d(input,  coords_lt.detach())\n",
        "    vals_rb = th_gather_2d(input,  coords_rb.detach())\n",
        "    vals_lb = th_gather_2d(input,  coords_lb.detach())\n",
        "    vals_rt = th_gather_2d(input,  coords_rt.detach())\n",
        "\n",
        "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
        "\n",
        "    vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n",
        "    vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n",
        "    mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_coordinates(inputs, coords):\n",
        "    \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n",
        "    # coords = coords.clip(0, inputs.shape[1] - 1)\n",
        "\n",
        "    assert (coords.shape[2] == 2)\n",
        "    height = coords[:,:,0].clip(0, inputs.shape[1] - 1)\n",
        "    width = coords[:,:,1].clip(0, inputs.shape[2] - 1)\n",
        "    np.concatenate((np.expand_dims(height, axis=2), np.expand_dims(width, axis=2)), 2)\n",
        "\n",
        "    mapped_vals = np.array([\n",
        "        sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n",
        "        for input, coord in zip(inputs, coords)\n",
        "    ])\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def th_batch_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Batch version of th_map_coordinates\n",
        "    Only supports 2D feature maps\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (b, s, s)\n",
        "    coords : tf.Tensor. shape = (b, n_points, 2)\n",
        "    Returns\n",
        "    -------\n",
        "    tf.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = input.size(0)\n",
        "    input_height = input.size(1)\n",
        "    input_width = input.size(2)\n",
        "\n",
        "    n_coords = coords.size(1)\n",
        "\n",
        "    # coords = torch.clamp(coords, 0, input_size - 1)\n",
        "\n",
        "    coords = torch.cat((torch.clamp(coords.narrow(2, 0, 1), 0, input_height - 1), torch.clamp(coords.narrow(2, 1, 1), 0, input_width - 1)), 2)\n",
        "\n",
        "    assert (coords.size(1) == n_coords)\n",
        "\n",
        "    coords_lt = coords.floor().long()\n",
        "    coords_rb = coords.ceil().long()\n",
        "    coords_lb = torch.stack([coords_lt[..., 0], coords_rb[..., 1]], 2)\n",
        "    coords_rt = torch.stack([coords_rb[..., 0], coords_lt[..., 1]], 2)\n",
        "    idx = th_repeat(torch.arange(0, batch_size), n_coords).long()\n",
        "    idx = Variable(idx, requires_grad=False)\n",
        "    if input.is_cuda:\n",
        "        idx = idx.cuda()\n",
        "\n",
        "    def _get_vals_by_coords(input, coords):\n",
        "        indices = torch.stack([\n",
        "            idx, th_flatten(coords[..., 0]), th_flatten(coords[..., 1])\n",
        "        ], 1)\n",
        "        inds = indices[:, 0]*input.size(1)*input.size(2)+ indices[:, 1]*input.size(2) + indices[:, 2]\n",
        "        vals = th_flatten(input).index_select(0, inds)\n",
        "        vals = vals.view(batch_size, n_coords)\n",
        "        return vals\n",
        "\n",
        "    vals_lt = _get_vals_by_coords(input, coords_lt.detach())\n",
        "    vals_rb = _get_vals_by_coords(input, coords_rb.detach())\n",
        "    vals_lb = _get_vals_by_coords(input, coords_lb.detach())\n",
        "    vals_rt = _get_vals_by_coords(input, coords_rt.detach())\n",
        "\n",
        "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
        "    vals_t = coords_offset_lt[..., 0]*(vals_rt - vals_lt) + vals_lt\n",
        "    vals_b = coords_offset_lt[..., 0]*(vals_rb - vals_lb) + vals_lb\n",
        "    mapped_vals = coords_offset_lt[..., 1]* (vals_b - vals_t) + vals_t\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_offsets(input, offsets):\n",
        "    \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n",
        "\n",
        "    batch_size = input.shape[0]\n",
        "    input_height = input.shape[1]\n",
        "    input_width = input.shape[2]\n",
        "\n",
        "    offsets = offsets.reshape(batch_size, -1, 2)\n",
        "    grid = np.stack(np.mgrid[:input_height, :input_width], -1).reshape(-1, 2)\n",
        "    grid = np.repeat([grid], batch_size, axis=0)\n",
        "    coords = offsets + grid\n",
        "    # coords = coords.clip(0, input_size - 1)\n",
        "\n",
        "    mapped_vals = sp_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def th_generate_grid(batch_size, input_height, input_width, dtype, cuda):\n",
        "    grid = np.meshgrid(\n",
        "        range(input_height), range(input_width), indexing='ij'\n",
        "    )\n",
        "    grid = np.stack(grid, axis=-1)\n",
        "    grid = grid.reshape(-1, 2)\n",
        "\n",
        "    grid = np_repeat_2d(grid, batch_size)\n",
        "    grid = torch.from_numpy(grid).type(dtype)\n",
        "    if cuda:\n",
        "        grid = grid.cuda()\n",
        "    return Variable(grid, requires_grad=False)\n",
        "\n",
        "\n",
        "def th_batch_map_offsets(input, offsets, grid=None, order=1):\n",
        "    \"\"\"Batch map offsets into input\n",
        "    Parameters\n",
        "    ---------\n",
        "    input : torch.Tensor. shape = (b, s, s)\n",
        "    offsets: torch.Tensor. shape = (b, s, s, 2)\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "    batch_size = input.size(0)\n",
        "    input_height = input.size(1)\n",
        "    input_width = input.size(2)\n",
        "\n",
        "    offsets = offsets.view(batch_size, -1, 2)\n",
        "    if grid is None:\n",
        "        grid = th_generate_grid(batch_size, input_height, input_width, offsets.data.type(), offsets.data.is_cuda)\n",
        "\n",
        "    coords = offsets + grid\n",
        "\n",
        "    mapped_vals = th_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "class SEModule(pl.LightningModule):\n",
        "    def __init__(self, num_channel, squeeze_ratio=1.0):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.sequeeze_mod = nn.AdaptiveAvgPool2d(1)\n",
        "        self.num_channel = num_channel\n",
        "\n",
        "        blocks = [nn.Linear(num_channel, int(num_channel * squeeze_ratio)),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(int(num_channel * squeeze_ratio), num_channel),\n",
        "                  nn.Sigmoid()]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ori = x\n",
        "        x = self.sequeeze_mod(x)\n",
        "        x = x.view(x.size(0), 1, self.num_channel)\n",
        "        x = self.blocks(x)\n",
        "        x = x.view(x.size(0), self.num_channel, 1, 1)\n",
        "        x = ori * x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ContextualAttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, patch_size=3, propagate_size=3, stride=1):\n",
        "        super(ContextualAttentionModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        ###assume the masked area has value 1\n",
        "        bz, nc, w, h = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background * masks\n",
        "        background = F.pad(background,\n",
        "                           [self.patch_size // 2, self.patch_size // 2, self.patch_size // 2, self.patch_size // 2])\n",
        "        conv_kernels_all = background.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size,\n",
        "                                                                                     self.stride).contiguous().view(bz,\n",
        "                                                                                                                    nc,\n",
        "                                                                                                                    -1,\n",
        "                                                                                                                    self.patch_size,\n",
        "                                                                                                                    self.patch_size)\n",
        "        conv_kernels_all = conv_kernels_all.transpose(2, 1)\n",
        "        output_tensor = []\n",
        "        for i in range(bz):\n",
        "            mask = masks[i:i + 1]\n",
        "            feature_map = foreground[i:i + 1].contiguous()\n",
        "            # form convolutional kernels\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels ** 2, [1, 2, 3], keepdim=True) ** 0.5\n",
        "            conv_kernels = conv_kernels / norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding=self.patch_size // 2)\n",
        "            \"\"\"\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.conv2d(conv_result, self.prop_kernels, stride=1, padding=1, groups=conv_result.size(1))\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "            self.prop_kernels.requires_grad = False\n",
        "            self.prop_kernels = self.prop_kernels.cuda()\n",
        "            conv_result = F.conv2d(conv_result, self.prop_kernels, stride=1, padding=1, groups=conv_result.size(1))\n",
        "            \n",
        "            attention_scores = F.softmax(conv_result, dim=1)\n",
        "            ##propagate the scores\n",
        "            recovered_foreground = F.conv_transpose2d(attention_scores, conv_kernels, stride=1,\n",
        "                                                      padding=self.patch_size // 2)\n",
        "            # average the recovered value, at the same time make non-masked area 0\n",
        "            recovered_foreground = (recovered_foreground * (1 - mask)) / (self.patch_size ** 2)\n",
        "            # recover the image\n",
        "            final_output = recovered_foreground + feature_map * mask\n",
        "            output_tensor.append(final_output)\n",
        "        return torch.cat(output_tensor, dim=0)\n",
        "\n",
        "\n",
        "class PixelContextualAttention(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, inchannel, patch_size_list=[1], propagate_size_list=[3], stride_list=[1]):\n",
        "        assert isinstance(patch_size_list,\n",
        "                          list), \"patch_size should be a list containing scales, or you should use Contextual Attention to initialize your module\"\n",
        "        assert len(patch_size_list) == len(propagate_size_list) and len(propagate_size_list) == len(\n",
        "            stride_list), \"the input_lists should have same lengths\"\n",
        "        super(PixelContextualAttention, self).__init__()\n",
        "        for i in range(len(patch_size_list)):\n",
        "            name = \"CA_{:d}\".format(i)\n",
        "            setattr(self, name, ContextualAttentionModule(patch_size_list[i], propagate_size_list[i], stride_list[i]))\n",
        "        self.num_of_modules = len(patch_size_list)\n",
        "        self.SqueezeExc = SEModule(inchannel * 2)\n",
        "        self.combiner = nn.Conv2d(inchannel * 2, inchannel, kernel_size=1)\n",
        "\n",
        "    def forward(self, foreground, mask):\n",
        "        outputs = [foreground]\n",
        "        for i in range(self.num_of_modules):\n",
        "            name = \"CA_{:d}\".format(i)\n",
        "            CA_module = getattr(self, name)\n",
        "            outputs.append(CA_module(foreground, mask))\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        outputs = self.SqueezeExc(outputs)\n",
        "        outputs = self.combiner(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvOffset2D(nn.Conv2d):\n",
        "    \"\"\"ConvOffset2D\n",
        "\n",
        "    Convolutional layer responsible for learning the 2D offsets and output the\n",
        "    deformed feature map using bilinear interpolation\n",
        "\n",
        "    Note that this layer does not perform convolution on the deformed feature\n",
        "    map. See get_deform_cnn in cnn.py for usage\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n",
        "        \"\"\"Init\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filters : int\n",
        "            Number of channel of the input feature map\n",
        "        init_normal_stddev : float\n",
        "            Normal kernel initialization\n",
        "        **kwargs:\n",
        "            Pass to superclass. See Con2d layer in pytorch\n",
        "        \"\"\"\n",
        "        self.filters = filters\n",
        "        self._grid_param = None\n",
        "        super(ConvOffset2D, self).__init__(self.filters, self.filters*2, 3, padding=1, bias=False, **kwargs)\n",
        "        self.weight.data.copy_(self._init_weights(self.weight, init_normal_stddev))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Return the deformed featured map\"\"\"\n",
        "        x_shape = x.size()\n",
        "        offsets = super(ConvOffset2D, self).forward(x)\n",
        "\n",
        "        # offsets: (b*c, h, w, 2)\n",
        "        offsets = self._to_bc_h_w_2(offsets, x_shape)\n",
        "\n",
        "        # x: (b*c, h, w)\n",
        "        x = self._to_bc_h_w(x, x_shape)\n",
        "\n",
        "        # X_offset: (b*c, h, w)\n",
        "        x_offset = th_batch_map_offsets(x, offsets, grid=self._get_grid(self,x))\n",
        "\n",
        "        # x_offset: (b, h, w, c)\n",
        "        x_offset = self._to_b_c_h_w(x_offset, x_shape)\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_grid(self, x):\n",
        "        batch_size, input_height, input_width = x.size(0), x.size(1), x.size(2)\n",
        "        dtype, cuda = x.data.type(), x.data.is_cuda\n",
        "        if self._grid_param == (batch_size, input_height, input_width, dtype, cuda):\n",
        "            return self._grid\n",
        "        self._grid_param = (batch_size, input_height, input_width, dtype, cuda)\n",
        "        self._grid = th_generate_grid(batch_size, input_height, input_width, dtype, cuda)\n",
        "        return self._grid\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(weights, std):\n",
        "        fan_out = weights.size(0)\n",
        "        fan_in = weights.size(1) * weights.size(2) * weights.size(3)\n",
        "        w = np.random.normal(0.0, std, (fan_out, fan_in))\n",
        "        return torch.from_numpy(w.reshape(weights.size()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w_2(x, x_shape):\n",
        "        \"\"\"(b, 2c, h, w) -> (b*c, h, w, 2)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]), 2)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w(x, x_shape):\n",
        "        \"\"\"(b, c, h, w) -> (b*c, h, w)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_b_c_h_w(x, x_shape):\n",
        "        \"\"\"(b*c, h, w) -> (b, c, h, w)\"\"\"\n",
        "        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RBNModule(pl.LightningModule):\n",
        "    _version = 2\n",
        "    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n",
        "                     'running_mean', 'running_var', 'num_batches_tracked']\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, affine=True, track_running_stats=True):\n",
        "        super(RBNModule, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.momentum = momentum\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1, 1))\n",
        "            self.register_buffer('running_var', torch.ones(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            nn.init.uniform_(self.weight)\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, mask_t):\n",
        "        input_m = input * mask_t\n",
        "        if self.training:\n",
        "            mask_mean = torch.mean(mask_t, (0, 2, 3), True)\n",
        "            x_mean = torch.mean(input_m, (0, 2, 3), True) / mask_mean\n",
        "            x_var = torch.mean(((input_m - x_mean) * mask_t) ** 2, (0, 2, 3), True) / mask_mean\n",
        "\n",
        "            x_out = self.weight * (input_m - x_mean) / torch.sqrt(x_var + self.eps) + self.bias\n",
        "\n",
        "            self.running_mean.mul_(self.momentum)\n",
        "            self.running_mean.add_((1 - self.momentum) * x_mean.data)\n",
        "            self.running_var.mul_(self.momentum)\n",
        "            self.running_var.add_((1 - self.momentum) * x_var.data)\n",
        "        else:\n",
        "            x_out = self.weight * (input_m - self.running_mean) / torch.sqrt(self.running_var + self.eps) + self.bias\n",
        "        return x_out * mask_t + input * (1 - mask_t)\n",
        "\n",
        "\n",
        "class RCNModule(pl.LightningModule):\n",
        "    _version = 2\n",
        "    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n",
        "                     'running_mean', 'running_var', 'num_batches_tracked']\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, affine=True, track_running_stats=True):\n",
        "        super(RCNModule, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.momentum = momentum\n",
        "        self.mean_weight = nn.Parameter(torch.ones(3))\n",
        "        self.var_weight = nn.Parameter(torch.ones(3))\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1, 1))\n",
        "            self.register_buffer('running_var', torch.ones(1, num_features, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            nn.init.uniform_(self.weight)\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, mask_t):\n",
        "        input_m = input * mask_t\n",
        "\n",
        "        if self.training:\n",
        "            mask_mean_bn = torch.mean(mask_t, (0, 2, 3), True)\n",
        "            mean_bn = torch.mean(input_m, (0, 2, 3), True) / mask_mean_bn\n",
        "            var_bn = torch.mean(((input_m - mean_bn) * mask_t) ** 2, (0, 2, 3), True) / mask_mean_bn\n",
        "\n",
        "            self.running_mean.mul_(self.momentum)\n",
        "            self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
        "            self.running_var.mul_(self.momentum)\n",
        "            self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
        "        else:\n",
        "            mean_bn = torch.autograd.Variable(self.running_mean)\n",
        "            var_bn = torch.autograd.Variable(self.running_var)\n",
        "\n",
        "        mask_mean_in = torch.mean(mask_t, (2, 3), True)\n",
        "        mean_in = torch.mean(input_m, (2, 3), True) / mask_mean_in\n",
        "        var_in = torch.mean(((input_m - mean_in) * mask_t) ** 2, (2, 3), True) / mask_mean_in\n",
        "\n",
        "        mask_mean_ln = torch.mean(mask_t, (1, 2, 3), True)\n",
        "        mean_ln = torch.mean(input_m, (1, 2, 3), True) / mask_mean_ln\n",
        "        var_ln = torch.mean(((input_m - mean_ln) * mask_t) ** 2, (1, 2, 3), True) / mask_mean_ln\n",
        "\n",
        "        mean_weight = F.softmax(self.mean_weight)\n",
        "        var_weight = F.softmax(self.var_weight)\n",
        "\n",
        "        x_mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn\n",
        "        x_var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn\n",
        "\n",
        "        x_out = self.weight * (input_m - x_mean) / torch.sqrt(x_var + self.eps) + self.bias\n",
        "        return x_out * mask_t + input * (1 - mask_t)\n",
        "\n",
        "\n",
        "class DSModule(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=False, rn=True, sample='none-3', activ='relu',\n",
        "                 conv_bias=False, defor=True):\n",
        "        super().__init__()\n",
        "        if sample == 'down-5':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 5, 2, 2, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(5,2,2)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 7, 2, 3, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(7, 2, 3)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = nn.Conv2d(in_ch+1, out_ch, 3, 2, 1, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(3, 2, 1)\n",
        "            if defor:\n",
        "                self.offset = ConvOffset2D(in_ch+1)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_ch+2, out_ch, 3, 1, 1, bias=conv_bias)\n",
        "            self.updatemask = nn.MaxPool2d(3,1,1)\n",
        "            if defor:\n",
        "                self.offset0 = ConvOffset2D(in_ch-out_ch+1)\n",
        "                self.offset1 = ConvOffset2D(out_ch+1)\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(out_ch)\n",
        "        if rn:\n",
        "            # Regional Composite Normalization\n",
        "            self.rn = RCNModule(out_ch)\n",
        "\n",
        "            # Regional Batch Normalization\n",
        "            # self.rn = RBNModule(out_ch)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2, inplace = True)\n",
        "\n",
        "    def forward(self, input, input_mask):\n",
        "        if hasattr(self, 'offset'):\n",
        "            input = torch.cat([input, input_mask[:,:1,:,:]], dim = 1)\n",
        "            h = self.offset(input)\n",
        "            h = input*input_mask[:,:1,:,:] + (1-input_mask[:,:1,:,:])*h\n",
        "            h = self.conv(h)\n",
        "            h_mask = self.updatemask(input_mask[:,:1,:,:])\n",
        "            h = h*h_mask\n",
        "            h = self.rn(h, h_mask)\n",
        "        elif hasattr(self, 'offset0'):\n",
        "            h1_in = torch.cat([input[:,self.in_ch-self.out_ch:,:,:], input_mask[:,1:,:,:]], dim = 1)\n",
        "            m1_in = input_mask[:,1:,:,:]\n",
        "            h0 = torch.cat([input[:,:self.in_ch-self.out_ch,:,:], input_mask[:,:1,:,:]], dim = 1)\n",
        "            h1 = self.offset1(h1_in)\n",
        "            h1 = m1_in*h1_in + (1-m1_in)*h1\n",
        "            h = self.conv(torch.cat([h0,h1], dim = 1))\n",
        "            h = self.rn(h, input_mask[:,:1,:,:])\n",
        "            h_mask = F.interpolate(input_mask[:,:1,:,:], scale_factor=2, mode='nearest')\n",
        "        else:\n",
        "            h = self.conv(torch.cat([input, input_mask[:,:,:,:]], dim = 1))\n",
        "            h_mask = self.updatemask(input_mask[:,:1,:,:])\n",
        "            h = h*h_mask\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            h = self.bn(h)\n",
        "        if hasattr(self, 'activation'):\n",
        "            h = self.activation(h)\n",
        "        return h, h_mask\n",
        "\n",
        "\n",
        "class DSNet(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, upsampling_mode='nearest'):\n",
        "        super().__init__()\n",
        "        self.freeze_enc_bn = False\n",
        "        self.upsampling_mode = upsampling_mode\n",
        "        self.layer_size = layer_size\n",
        "        self.enc_1 = DSModule(input_channels, 64, rn=False, sample='down-7', defor = False)\n",
        "        self.enc_2 = DSModule(64, 128, sample='down-5')\n",
        "        self.enc_3 = DSModule(128, 256, sample='down-5')\n",
        "        self.enc_4 = DSModule(256, 512, sample='down-3')\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            setattr(self, name, DSModule(512, 512, sample='down-3'))\n",
        "\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'dec_{:d}'.format(i + 1)\n",
        "            setattr(self, name, DSModule(512 + 512, 512, activ='leaky'))\n",
        "        self.dec_4 = DSModule(512 + 256, 256, activ='leaky')\n",
        "        self.dec_3 = DSModule(256 + 128, 128, activ='leaky')\n",
        "        self.dec_2 = DSModule(128 + 64, 64, activ='leaky')\n",
        "        self.dec_1 = DSModule(64 + input_channels, input_channels,\n",
        "                              rn=False, activ=None, defor = False)\n",
        "        self.att = PixelContextualAttention(128)\n",
        "    def forward(self, input, input_mask):\n",
        "        input = input.type(torch.cuda.FloatTensor)\n",
        "        input_mask = input_mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        input_mask = input_mask[:,0:1,:,:]\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "        h_mask_dict = {}  # for the output of enc_N\n",
        "\n",
        "        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(\n",
        "                h_dict[h_key_prev], h_mask_dict[h_key_prev])\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h_key = 'h_{:d}'.format(self.layer_size)\n",
        "        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n",
        "        h_mask = F.interpolate(h_mask, scale_factor=2, mode='nearest')\n",
        "\n",
        "        for i in range(self.layer_size, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i - 1)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "\n",
        "            h = F.interpolate(h, scale_factor=2, mode=self.upsampling_mode)\n",
        "\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n",
        "            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim=1)\n",
        "            h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n",
        "            if i == 3:\n",
        "                h = self.att(h, input_mask[:,:1,:,:])\n",
        "        #return h, h_mask\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@title [Deoldify](https://github.com/alfagao/DeOldify) (2018) (dataloader not implemented)\n",
        "\n",
        "\"\"\"\n",
        "torch_imports.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fastai/torch_imports.py\n",
        "\n",
        "conv_learner.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fastai/conv_learner.py\n",
        "\n",
        "model.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fastai/model.py\n",
        "\n",
        "modules.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fasterai/modules.py\n",
        "\n",
        "generators.py (9-3-20)\n",
        "https://github.com/alfagao/DeOldify/blob/bc9d4562bf2014f5268f5c616ae31873577d9fde/fasterai/generators.py\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.spectral_norm import spectral_norm\n",
        "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.models import vgg16_bn, vgg19_bn\n",
        "\n",
        "def vgg16(pre): return children(vgg16_bn(pre))[0]\n",
        "def vgg19(pre): return children(vgg19_bn(pre))[0]\n",
        "\n",
        "def cut_model(m, cut):\n",
        "    return list(m.children())[:cut] if cut else [m]\n",
        "\"\"\"\n",
        "model_meta = {\n",
        "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
        "    vgg16:[0,22], vgg19:[0,22],\n",
        "    resnext50:[8,6], resnext101:[8,6], resnext101_64:[8,6],\n",
        "    wrn:[8,6], inceptionresnet_2:[-2,9], inception_4:[-1,9],\n",
        "    dn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "model_meta = {\n",
        "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
        "    vgg16:[0,22], vgg19:[0,22],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class ConvBlock(pl.LightningModule):\n",
        "    def __init__(self, ni:int, no:int, ks:int=3, stride:int=1, pad:int=None, actn:bool=True, \n",
        "            bn:bool=True, bias:bool=True, sn:bool=False, leakyReLu:bool=False, self_attention:bool=False,\n",
        "            inplace_relu:bool=True):\n",
        "        super().__init__()   \n",
        "        if pad is None: pad = ks//2//stride\n",
        "\n",
        "        if sn:\n",
        "            layers = [spectral_norm(nn.Conv2d(ni, no, ks, stride, padding=pad, bias=bias))]\n",
        "        else:\n",
        "            layers = [nn.Conv2d(ni, no, ks, stride, padding=pad, bias=bias)]\n",
        "        if actn:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=inplace_relu)) if leakyReLu else layers.append(nn.ReLU(inplace=inplace_relu)) \n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(no))\n",
        "        if self_attention:\n",
        "            layers.append(SelfAttention(no, 1))\n",
        "\n",
        "        self.seq = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class UpSampleBlock(pl.LightningModule):\n",
        "    @staticmethod\n",
        "    def _conv(ni:int, nf:int, ks:int=3, bn:bool=True, sn:bool=False, leakyReLu:bool=False):\n",
        "        layers = [ConvBlock(ni, nf, ks=ks, sn=sn, bn=bn, actn=False, leakyReLu=leakyReLu)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    @staticmethod\n",
        "    def _icnr(x:torch.Tensor, scale:int=2):\n",
        "        init=nn.init.kaiming_normal_\n",
        "        new_shape = [int(x.shape[0] / (scale ** 2))] + list(x.shape[1:])\n",
        "        subkernel = torch.zeros(new_shape)\n",
        "        subkernel = init(subkernel)\n",
        "        subkernel = subkernel.transpose(0, 1)\n",
        "        subkernel = subkernel.contiguous().view(subkernel.shape[0],\n",
        "                                                subkernel.shape[1], -1)\n",
        "        kernel = subkernel.repeat(1, 1, scale ** 2)\n",
        "        transposed_shape = [x.shape[1]] + [x.shape[0]] + list(x.shape[2:])\n",
        "        kernel = kernel.contiguous().view(transposed_shape)\n",
        "        kernel = kernel.transpose(0, 1)\n",
        "        return kernel\n",
        "\n",
        "    def __init__(self, ni:int, nf:int, scale:int=2, ks:int=3, bn:bool=True, sn:bool=False, leakyReLu:bool=False):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        assert (math.log(scale,2)).is_integer()\n",
        "\n",
        "        for i in range(int(math.log(scale,2))):\n",
        "            layers += [UpSampleBlock._conv(ni, nf*4,ks=ks, bn=bn, sn=sn, leakyReLu=leakyReLu), \n",
        "                nn.PixelShuffle(2)]\n",
        "            if bn:\n",
        "                layers += [nn.BatchNorm2d(nf)]\n",
        "\n",
        "            ni = nf\n",
        "                       \n",
        "        self.sequence = nn.Sequential(*layers)\n",
        "        self._icnr_init()\n",
        "        \n",
        "    def _icnr_init(self):\n",
        "        conv_shuffle = self.sequence[0][0].seq[0]\n",
        "        kernel = UpSampleBlock._icnr(conv_shuffle.weight)\n",
        "        conv_shuffle.weight.data.copy_(kernel)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.sequence(x)\n",
        "\n",
        "\n",
        "class UnetBlock(pl.LightningModule):\n",
        "    def __init__(self, up_in:int , x_in:int , n_out:int, bn:bool=True, sn:bool=False, leakyReLu:bool=False, \n",
        "            self_attention:bool=False, inplace_relu:bool=True):\n",
        "        super().__init__()\n",
        "        up_out = x_out = n_out//2\n",
        "        self.x_conv  = ConvBlock(x_in,  x_out,  ks=1, bn=False, actn=False, sn=sn, inplace_relu=inplace_relu)\n",
        "        self.tr_conv = UpSampleBlock(up_in, up_out, 2, bn=bn, sn=sn, leakyReLu=leakyReLu)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=inplace_relu) if leakyReLu else nn.ReLU(inplace=inplace_relu)\n",
        "        out_layers = []\n",
        "        if bn: \n",
        "            out_layers.append(nn.BatchNorm2d(n_out))\n",
        "        if self_attention:\n",
        "            out_layers.append(SelfAttention(n_out))\n",
        "        self.out = nn.Sequential(*out_layers)\n",
        "        \n",
        "        \n",
        "    def forward(self, up_p:int, x_p:int):\n",
        "        up_p = self.tr_conv(up_p)\n",
        "        x_p = self.x_conv(x_p)\n",
        "        x = torch.cat([up_p,x_p], dim=1)\n",
        "        x = self.relu(x)\n",
        "        return self.out(x)\n",
        "\n",
        "class SaveFeatures():\n",
        "    features=None\n",
        "    def __init__(self, m:pl.LightningModule): \n",
        "        self.hook = m.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output): \n",
        "        self.features = output\n",
        "    def remove(self): \n",
        "        self.hook.remove()\n",
        "\n",
        "class SelfAttention(pl.LightningModule):\n",
        "    def __init__(self, in_channel:int, gain:int=1):\n",
        "        super().__init__()\n",
        "        self.query = self._spectral_init(nn.Conv1d(in_channel, in_channel // 8, 1),gain=gain)\n",
        "        self.key = self._spectral_init(nn.Conv1d(in_channel, in_channel // 8, 1),gain=gain)\n",
        "        self.value = self._spectral_init(nn.Conv1d(in_channel, in_channel, 1), gain=gain)\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def _spectral_init(self, module:pl.LightningModule, gain:int=1):\n",
        "        nn.init.kaiming_uniform_(module.weight, gain)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "        return spectral_norm(module)\n",
        "\n",
        "    def forward(self, input:torch.Tensor):\n",
        "        shape = input.shape\n",
        "        flatten = input.view(shape[0], shape[1], -1)\n",
        "        query = self.query(flatten).permute(0, 2, 1)\n",
        "        key = self.key(flatten)\n",
        "        value = self.value(flatten)\n",
        "        query_key = torch.bmm(query, key)\n",
        "        attn = F.softmax(query_key, 1)\n",
        "        attn = torch.bmm(value, attn)\n",
        "        attn = attn.view(*shape)\n",
        "        out = self.gamma * attn + input\n",
        "        return out\n",
        "\n",
        "\n",
        "class GeneratorModule(ABC, nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def set_trainable(self, trainable:bool):\n",
        "        set_trainable(self, trainable)\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_layer_groups(self, precompute:bool=False)->[]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x_in:torch.Tensor, max_render_sz:int=400):\n",
        "        pass\n",
        "        \n",
        "    def freeze_to(self, n:int):\n",
        "        c=self.get_layer_groups()\n",
        "        for l in c:     set_trainable(l, False)\n",
        "        for l in c[n:]: set_trainable(l, True)\n",
        "\n",
        "    def get_device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "\n",
        "class AbstractUnet(GeneratorModule): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__()\n",
        "        assert (math.log(scale,2)).is_integer()\n",
        "        self.rn, self.lr_cut = self._get_pretrained_resnet_base()\n",
        "        ups = self._get_decoding_layers(nf_factor=nf_factor, scale=scale)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.up1 = ups[0]\n",
        "        self.up2 = ups[1]\n",
        "        self.up3 = ups[2]\n",
        "        self.up4 = ups[3]\n",
        "        self.up5 = ups[4]\n",
        "        self.out= nn.Sequential(ConvBlock(32*nf_factor, 3, ks=3, actn=False, bn=False, sn=True), nn.Tanh())\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        pass\n",
        "\n",
        "    #Gets around irritating inconsistent halving coming from resnet\n",
        "    def _pad(self, x:torch.Tensor, target:torch.Tensor, total_padh:int, total_padw:int)-> torch.Tensor:\n",
        "        h = x.shape[2] \n",
        "        w = x.shape[3]\n",
        "\n",
        "        target_h = target.shape[2]*2\n",
        "        target_w = target.shape[3]*2\n",
        "\n",
        "        if h<target_h or w<target_w:\n",
        "            padh = target_h-h if target_h > h else 0\n",
        "            total_padh = total_padh + padh\n",
        "            padw = target_w-w if target_w > w else 0\n",
        "            total_padw = total_padw + padw\n",
        "            return (F.pad(x, (0,padw,0,padh), \"reflect\",0), total_padh, total_padw)\n",
        "\n",
        "        return (x, total_padh, total_padw)\n",
        "\n",
        "    def _remove_padding(self, x:torch.Tensor, padh:int, padw:int)->torch.Tensor:\n",
        "        if padw == 0 and padh == 0:\n",
        "            return x \n",
        "        \n",
        "        target_h = x.shape[2]-padh\n",
        "        target_w = x.shape[3]-padw\n",
        "        return x[:,:,:target_h, :target_w]\n",
        "\n",
        "    def _encode(self, x:torch.Tensor):\n",
        "        x = self.rn[0](x)\n",
        "        x = self.rn[1](x)\n",
        "        x = self.rn[2](x)\n",
        "        enc0 = x\n",
        "        x = self.rn[3](x)\n",
        "        x = self.rn[4](x)\n",
        "        enc1 = x\n",
        "        x = self.rn[5](x)\n",
        "        enc2 = x\n",
        "        x = self.rn[6](x)\n",
        "        enc3 = x\n",
        "        x = self.rn[7](x)\n",
        "        return (x, enc0, enc1, enc2, enc3)\n",
        "\n",
        "    def _decode(self, x:torch.Tensor, enc0:torch.Tensor, enc1:torch.Tensor, enc2:torch.Tensor, enc3:torch.Tensor):\n",
        "        padh = 0\n",
        "        padw = 0\n",
        "        x = self.relu(x)\n",
        "        enc3, padh, padw = self._pad(enc3, x, padh, padw)\n",
        "        x = self.up1(x, enc3)\n",
        "        enc2, padh, padw  = self._pad(enc2, x, padh, padw)\n",
        "        x = self.up2(x, enc2)\n",
        "        enc1, padh, padw  = self._pad(enc1, x, padh, padw)\n",
        "        x = self.up3(x, enc1)\n",
        "        enc0, padh, padw  = self._pad(enc0, x, padh, padw)\n",
        "        x = self.up4(x, enc0)\n",
        "        #This is a bit too much padding being removed, but I \n",
        "        #haven't yet figured out a good way to determine what \n",
        "        #exactly should be removed.  This is consistently more \n",
        "        #than enough though.\n",
        "        x = self.up5(x)\n",
        "        x = self.out(x)\n",
        "        x = self._remove_padding(x, padh, padw)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        x, enc0, enc1, enc2, enc3 = self._encode(x)\n",
        "        x = self._decode(x, enc0, enc1, enc2, enc3)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_groups(self, precompute:bool=False)->[]:\n",
        "        lgs = list(split_by_idxs(children(self.rn), [self.lr_cut]))\n",
        "        return lgs + [children(self)[1:]]\n",
        "    \n",
        "    def close(self):\n",
        "        for sf in self.sfs: \n",
        "            sf.remove()\n",
        "\n",
        "\n",
        "class Unet34(AbstractUnet): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__(nf_factor=nf_factor, scale=scale)\n",
        "\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        f = resnet34\n",
        "        cut,lr_cut = model_meta[f]\n",
        "        cut-=layers_cut\n",
        "        layers = cut_model(f(True), cut)\n",
        "        return nn.Sequential(*layers), lr_cut\n",
        "\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        self_attention=True\n",
        "        bn=True\n",
        "        sn=True\n",
        "        leakyReLu=False\n",
        "        layers = []\n",
        "        layers.append(UnetBlock(512,256,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,128,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,512*nf_factor, sn=sn, self_attention=self_attention, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,256*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UpSampleBlock(256*nf_factor, 32*nf_factor, 2*scale, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        return layers \n",
        "\n",
        "\n",
        "class Unet101(AbstractUnet): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__(nf_factor=nf_factor, scale=scale)\n",
        "\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        f = resnet101\n",
        "        cut,lr_cut = model_meta[f]\n",
        "        cut-=layers_cut\n",
        "        layers = cut_model(f(True), cut)\n",
        "        return nn.Sequential(*layers), lr_cut\n",
        "\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        self_attention=True\n",
        "        bn=True\n",
        "        sn=True\n",
        "        leakyReLu=False\n",
        "        layers = []\n",
        "        layers.append(UnetBlock(2048,1024,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,512,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,256,512*nf_factor, sn=sn, self_attention=self_attention, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,256*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UpSampleBlock(256*nf_factor, 32*nf_factor, 2*scale, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        return layers \n",
        "\n",
        "class Unet152(AbstractUnet): \n",
        "    def __init__(self, nf_factor:int=1, scale:int=1):\n",
        "        super().__init__(nf_factor=nf_factor, scale=scale)\n",
        "\n",
        "    def _get_pretrained_resnet_base(self, layers_cut:int=0):\n",
        "        f = resnet152\n",
        "        cut,lr_cut = model_meta[f]\n",
        "        cut-=layers_cut\n",
        "        layers = cut_model(f(True), cut)\n",
        "        return nn.Sequential(*layers), lr_cut\n",
        "\n",
        "    def _get_decoding_layers(self, nf_factor:int, scale:int):\n",
        "        self_attention=True\n",
        "        bn=True\n",
        "        sn=True\n",
        "        leakyReLu=False\n",
        "        layers = []\n",
        "        layers.append(UnetBlock(2048,1024,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,512,512*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,256,512*nf_factor, sn=sn, self_attention=self_attention, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UnetBlock(512*nf_factor,64,256*nf_factor, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        layers.append(UpSampleBlock(256*nf_factor, 32*nf_factor, 2*scale, sn=sn, leakyReLu=leakyReLu, bn=bn))\n",
        "        return layers \n",
        "\n",
        "\n",
        "\n",
        "class DSNetDeoldify(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, upsampling_mode='nearest'):\n",
        "      super(DSNetDeoldify, self).__init__()\n",
        "      self.netG1 = DSNet(layer_size=layer_size, input_channels=input_channels, upsampling_mode=upsampling_mode)\n",
        "      self.netG2 = Unet34()\n",
        "    def forward(self, input, input_mask):\n",
        "      result1 = self.netG1(input, input_mask)\n",
        "\n",
        "      result1 = input*input_mask+result1*(1-input_mask)\n",
        "\n",
        "      #concat = torch.cat((result1, input_mask), dim=1)\n",
        "      #result2 = self.netG2(concat)\n",
        "\n",
        "      result2 = self.netG2(result1)\n",
        "      return result2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZc7QSEN2uoR"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4fr0m2LkHhf",
        "cellView": "form"
      },
      "source": [
        "#@title delete validation, logs and checkpoints if needed\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/validation_output\n",
        "!sudo rm -rf /content/lightning_logs\n",
        "!sudo rm -rf /content/logs\n",
        "#!mkdir /content/logs/\n",
        "!find . -name \"*.ckpt\" -type f -delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1EJgIcx7FWq"
      },
      "source": [
        "Info about ``logger=None``: Logging will be done outside of lightning to have iter-based logging instead of epochs by using tensorboardX within the training loop. Be aware that loss weights and the overall training configuration is located in ``CustomTrainClass.py``. If you want to use another generator or configure parameters, edit stuff there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYW1pF5RzQc",
        "cellView": "form"
      },
      "source": [
        "#@title Training\n",
        "#@markdown Normal training assumes the usage of generator and discriminator. If you only use generator, then edit ``checkpoint.py`` and comment out the line, where the discriminator pth gets saved.\n",
        "\n",
        "#@markdown Settings depends on the dataloader. Not everything is active at the same time.\n",
        "import pytorch_lightning as pl\n",
        "%cd /content/\n",
        "\n",
        "dir_lr = '/content/lr/' #@param\n",
        "dir_hr = '/content/hr/' #@param\n",
        "val_lr =  '/content/val_lr/'#@param\n",
        "val_hr = '/content/val_hr/' #@param\n",
        "num_workers = 1 #@param\n",
        "hr_size = 256 #@param\n",
        "scale = 4 #@param\n",
        "batch_size = 1 #@param\n",
        "batch_size_DL = 20 #@param\n",
        "gpus=1 #@param\n",
        "max_epochs = 100 #@param\n",
        "progress_bar_refresh_rate = 20 #@param\n",
        "default_root_dir='/content/' #@param\n",
        "save_path='/content/' #@param\n",
        "save_step_frequency = 100 #@param\n",
        "tpu_cores = 8 #@param\n",
        "#@markdown For batch dataloader\n",
        "image_size=400 #@param\n",
        "amount_tiles=3 #@param\n",
        "#############################################\n",
        "# Dataloader\n",
        "#############################################\n",
        "# Inpainting\n",
        "# normal training\n",
        "#dm = DFNetDataModule(batch_size=batch_size, training_path = dir_hr, validation_path = val_lr)\n",
        "# tiled dataloader (batch return)\n",
        "#dm = DFNetDataModule(training_path = dir_hr, validation_path = val_lr, batch_size=batch_size, num_workers=num_workers, batch_size_DL=batch_size_DL)\n",
        "\n",
        "# Super Resolution\n",
        "# lr/hr dataloader\n",
        "dm = DFNetDataModule(batch_size=batch_size, dir_lr = dir_lr, dir_hr = dir_hr, val_lr = val_lr, val_hr = val_hr, num_workers = num_workers, hr_size = hr_size, scale = scale)\n",
        "# batch\n",
        "#dm = DFNetDataModule(batch_size=batch_size, training_path = dir_lr, val_lr = val_lr, val_hr = val_hr, num_workers=num_workers, batch_size_DL=batch_size_DL, hr_size=hr_size, scale = scale, image_size=image_size, amount_tiles=amount_tiles)\n",
        "#############################################\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Loading a Model\n",
        "#############################################\n",
        "model = CustomTrainClass()\n",
        "\n",
        "#@markdown Loading a pretrain pth\n",
        "pretrain_path = None #@param\n",
        "if pretrain_path is not None:\n",
        "  trainer.model.netG.load_state_dict(torch.load(model_path))\n",
        "\n",
        "#@markdown For resuming training\n",
        "checkpoint_path = '/content/Checkpoint_1_500.ckpt' #@param\n",
        "\n",
        "# load from checkpoint (optional) (using a model as pretrain and disregarding other parameters)\n",
        "#model = model.load_from_checkpoint(checkpoint_path) # start training from checkpoint, warning: apperantly global_step will be reset to zero and overwriting validation images, you could manually make an offset\n",
        "\n",
        "\n",
        "# continue training with checkpoint (does restore values) (optional)\n",
        "# https://github.com/PyTorchLightning/pytorch-lightning/issues/2613\n",
        "# https://pytorch-lightning.readthedocs.io/en/0.6.0/pytorch_lightning.trainer.training_io.html\n",
        "# https://github.com/PyTorchLightning/pytorch-lightning/issues/4333\n",
        "# dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'callbacks', 'optimizer_states', 'lr_schedulers', 'state_dict', 'hparams_name', 'hyper_parameters'])\n",
        "\n",
        "# To use DDP for local multi-GPU training, you need to add find_unused_parameters=True inside the DDP command\n",
        "\"\"\"\n",
        "model = model.load_from_checkpoint(checkpoint_path)\n",
        "trainer = pl.Trainer(resume_from_checkpoint=checkpoint_path, logger=None, gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=100, save_path='/content/')])\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "trainer.checkpoint_connector.restore(checkpoint, on_gpu=True)\n",
        "trainer.checkpoint_connector.restore_training_state(checkpoint)\n",
        "pl.Trainer.global_step = checkpoint['global_step']\n",
        "pl.Trainer.epoch = checkpoint['epoch']\n",
        "\"\"\"\n",
        "#############################################\n",
        "\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Training\n",
        "#############################################\n",
        "# GPU\n",
        "# Also maybe useful:\n",
        "# auto_scale_batch_size='binsearch'\n",
        "# stochastic_weight_avg=True\n",
        "\n",
        "# Warning: stochastic_weight_avg **can cause crashing after an epoch**. Test if it crashes first if you reach next epoch. Not all generators are tested.\n",
        "trainer = pl.Trainer(logger=None, gpus=gpus, max_epochs=max_epochs, progress_bar_refresh_rate=progress_bar_refresh_rate, default_root_dir=default_root_dir, callbacks=[CheckpointEveryNSteps(save_step_frequency=save_step_frequency, save_path=save_path)])\n",
        "# 2+ GPUS (locally, not inside Google Colab)\n",
        "# Recommended: Pytorch 1.8+. 1.7.1 seems to have dataloader issues and ddp only works if code is run within console.\n",
        "#trainer = pl.Trainer(logger=None, gpus=2, distributed_backend='dp', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=100, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision, 'O2' = Almost FP16, 'O3' = FP16)\n",
        "# https://nvidia.github.io/apex/amp.html?highlight=opt_level#o1-mixed-precision-recommended-for-typical-use\n",
        "#trainer = pl.Trainer(logger=None, gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(logger=None, tpu_cores=tpu_cores, max_epochs=max_epochs, progress_bar_refresh_rate=progress_bar_refresh_rate, default_root_dir=default_root_dir, callbacks=[CheckpointEveryNSteps(save_step_frequency=save_step_frequency, save_path=save_path)])\n",
        "#############################################\n",
        "\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xREJTZSbsEa8"
      },
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcHYqzCGleIX",
        "cellView": "form"
      },
      "source": [
        "#@title testing the model\n",
        "dm = DS_green_from_mask('/content/test')\n",
        "model = CustomTrainClass()\n",
        "# GPU\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.test(model, dm, ckpt_path='/content/Checkpoint_0_0.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eodC8LcPOLFe"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLNJxYMbE18T",
        "cellView": "form"
      },
      "source": [
        "#@title creating 16x16 images\n",
        "import cv2\n",
        "import numpy\n",
        "import glob\n",
        "rootdir = '/content/data' #@param {type:\"string\"}\n",
        "destination_dir = \"/content/4k/\" #@param {type:\"string\"}\n",
        "\n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "err_files=[]\n",
        "\n",
        "filepos = 0\n",
        "img_cnt = 0\n",
        "tmp_img = numpy.zeros((4096,4096, 3))\n",
        "while True:\n",
        "  for i in range(16):\n",
        "    for j in range(16):\n",
        "      image = cv2.imread(files[filepos])\n",
        "      filepos += 1\n",
        "      \n",
        "      image = cv2.resize(image, (256,256))\n",
        "      \n",
        "      tmp_img[i*256:(i+1)*256, j*256:(j+1)*256] = image\n",
        "  #cv2.imwrite(\"/content/4k/\"+str(img_cnt)+\".png\", tmp_img)\n",
        "  cv2.imwrite(destination_dir+str(img_cnt)+\".jpg\", tmp_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "  \n",
        "  img_cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9AdKcPeOMxg",
        "cellView": "form"
      },
      "source": [
        "#@title creating 16x16 images (with skip)\n",
        "import cv2\n",
        "import numpy\n",
        "import glob\n",
        "import shutil\n",
        "import tqdm\n",
        "import os\n",
        "rootdir = '/content/data' #@param {type:\"string\"}\n",
        "destination_dir = \"/content/4k/\" #@param {type:\"string\"}\n",
        "broken_dir = '/content/opencv_fail/' #@param {type:\"string\"}\n",
        " \n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "files_jpeg = glob.glob(rootdir + '/**/*.jpeg', recursive=True)\n",
        "files_webp = glob.glob(rootdir + '/**/*.webp', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "files.extend(files_jpeg)\n",
        "files.extend(files_webp)\n",
        "err_files=[]\n",
        "\n",
        "filepos = 0\n",
        "img_cnt = 0\n",
        "filename_cnt = 0\n",
        "tmp_img = numpy.zeros((4096,4096, 3))\n",
        "\n",
        "with tqdm.tqdm(files) as pbar:\n",
        "  while True:\n",
        "      image = cv2.imread(files[filepos])\n",
        "      filepos += 1\n",
        "\n",
        "      if image is not None:\n",
        "        \n",
        "        i = img_cnt % 16\n",
        "        j = img_cnt // 16\n",
        "\n",
        "        image = cv2.resize(image, (256,256))\n",
        "        tmp_img[i*256:(i+1)*256, j*256:(j+1)*256] = image\n",
        "        img_cnt += 1\n",
        "      else:\n",
        "        print(files[filepos])\n",
        "        print(f'{broken_dir}/{os.path.basename(files[filepos])}')\n",
        "        shutil.move(files[filepos], f'{broken_dir}/{os.path.basename(files[filepos])}')\n",
        "\n",
        "      if img_cnt == 256:\n",
        "        cv2.imwrite(destination_dir+str(filename_cnt)+\".jpg\", tmp_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "        filename_cnt += 1\n",
        "        img_cnt = 0\n",
        "      pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnsxPE5a2PJV",
        "cellView": "form"
      },
      "source": [
        "#@title creating 3x3 grayscale images (with skip)\n",
        "import cv2\n",
        "import numpy\n",
        "import glob\n",
        "import shutil\n",
        "import tqdm\n",
        "import os\n",
        "import random\n",
        "rootdir = '/media/veracrypt1/Font/png/' #@param {type:\"string\"}\n",
        "destination_dir = \"/media/veracrypt1/Font/hr_400/\" #@param {type:\"string\"}\n",
        "broken_dir = '/media/veracrypt1/Font/broken/' #@param {type:\"string\"}\n",
        " \n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "files_jpeg = glob.glob(rootdir + '/**/*.jpeg', recursive=True)\n",
        "files_webp = glob.glob(rootdir + '/**/*.webp', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "files.extend(files_jpeg)\n",
        "files.extend(files_webp)\n",
        "err_files=[]\n",
        "\n",
        "image_size = 400 #@param\n",
        "\n",
        "filepos = 0\n",
        "img_cnt = 0\n",
        "filename_cnt = 0\n",
        "tmp_img = numpy.zeros((image_size*3,image_size*3))\n",
        "\n",
        "interpolation_method = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4]\n",
        "\n",
        "with tqdm.tqdm(files) as pbar:\n",
        "  while True:\n",
        "      image = cv2.imread(files[filepos], cv2.IMREAD_GRAYSCALE)\n",
        "      image = cv2.resize(image, (400,400), interpolation=random.choice(interpolation_method))\n",
        "      filepos += 1\n",
        "\n",
        "      if image is not None:\n",
        "        \n",
        "        i = img_cnt % 3\n",
        "        j = img_cnt // 3\n",
        "\n",
        "        image = cv2.resize(image, (image_size,image_size))\n",
        "        tmp_img[i*image_size:(i+1)*image_size, j*image_size:(j+1)*image_size] = image\n",
        "        img_cnt += 1\n",
        "      else:\n",
        "        print(files[filepos])\n",
        "        print(f'{broken_dir}/{os.path.basename(files[filepos])}')\n",
        "        shutil.move(files[filepos], f'{broken_dir}/{os.path.basename(files[filepos])}')\n",
        "\n",
        "      if img_cnt == 9:\n",
        "        #cv2.imwrite(destination_dir+str(filename_cnt)+\".png\", tmp_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "        cv2.imwrite(destination_dir+str(filename_cnt)+\".png\", tmp_img)\n",
        "        filename_cnt += 1\n",
        "        img_cnt = 0\n",
        "      pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1_olG9wXux6",
        "cellView": "form"
      },
      "source": [
        "#@title convert to onnx\n",
        "#@markdown Make sure the input dimensions are correct. Maybe a runtime restart is needed if it complains about ``TypeError: forward() missing 1 required positional argument``. Make sure you only run the required cells.\n",
        "from torch.autograd import Variable\n",
        "model = CustomTrainClass()\n",
        "checkpoint_path = '/content/Checkpoint_0_0.ckpt' #@param\n",
        "output_path = '/content/output.onnx' #@param\n",
        "model = model.load_from_checkpoint(checkpoint_path) # start training from checkpoint, warning: apperantly global_step will be reset to zero and overwriting validation images, you could manually make an offset\n",
        "dummy_input = Variable(torch.randn(1, 1, 64, 64))\n",
        "\n",
        "model.to_onnx(output_path, input_sample=dummy_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrEWcEPto_XN",
        "cellView": "form"
      },
      "source": [
        "#@title copy pasting data to create artificatial dataset for debugging\n",
        "import shutil\n",
        "from random import random\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(5000)):\n",
        "  shutil.copy(\"/content/4k/0.jpg\", \"/content/4k/\"+str(random())+\"jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
