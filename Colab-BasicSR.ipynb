{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-BasicSR.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY_bDEi1J7bJ"
      },
      "source": [
        "# Colab-BasicSR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My897iQFpTHH"
      },
      "source": [
        "victorca25's BasicSR fork: [victorca25/BasicSR](https://github.com/victorca25/BasicSR)\n",
        "\n",
        "Original colab by [nmkd](https://github.com/n00mkrad) with modifications by [styler00dollar](https://github.com/styler00dollar)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MrEgrBSu4Qu",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "\n",
        "gpu = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "print(\"GPU: \" + gpu[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBbSxWWBOZEI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsK_NAi963qK"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l32IresQs-oW",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "\n",
        "!rm -r \"/content/BasicSR\"\n",
        "!mkdir \"/content/BasicSR\"\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import Image as ipythonimage\n",
        "import os\n",
        "import fileinput\n",
        "import sys\n",
        "\n",
        "sedloc = \"\"\n",
        "%cd /content/\n",
        "#Install apt-fast, for faster installing\n",
        "!/bin/bash -c \"$(curl -sL https://git.io/vokNn)\"\n",
        "#Get some basic dependencies\n",
        "!apt-fast install -y -q -q p7zip-full p7zip-rar\n",
        "\n",
        "dr_exists = os.path.isdir(\"BasicSR/\")\n",
        "\n",
        "!pip install opencv-python\n",
        "!pip install tensorboardX\n",
        "!pip install pyyaml\n",
        "\n",
        "# Clone BasicSR (vic's dev2 branch)\n",
        "!rm -r /content/BasicSR\n",
        "%cd /content/\n",
        "#!git clone \"https://github.com/victorca25/BasicSR.git\"\n",
        "!git clone https://github.com/styler00dollar/Colab-BasicSR BasicSR\n",
        "%cd /content/BasicSR\n",
        "#!git checkout dev2\n",
        "\n",
        "# create empty folders\n",
        "!mkdir /content/hr\n",
        "!mkdir /content/lr\n",
        "!mkdir /content/lr_val\n",
        "!mkdir /content/hr_val\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZwoYinu69nB"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRHTppHtsgkr"
      },
      "source": [
        "Pre-configured paths:\n",
        "\n",
        "LR: ```/content/lr```\n",
        "\n",
        "HR: ```/content/hr```\n",
        "\n",
        "LR_VAL: ```/content/lr_val```\n",
        "\n",
        "HR_VAL: ```/content/hr_val```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPkMNWqKgxrR"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu7HxxdGJdir"
      },
      "source": [
        "You need to upload the data and then extract it within colab. You can use Google Drive for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGU28ZJIJlEC"
      },
      "source": [
        "!cp \"/content/drive/My Drive/data.7z\" \"/content/data.7z\"\n",
        "!7z x /content/data.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPFhbN7sGD8h"
      },
      "source": [
        "# Download pretrain models if needed (Alternatively, you can load them from Google Drive)\n",
        "#!wget \"http://dl.nmkd.de/ai/esrgan/1xESRGAN.pth\" --content-disposition -P /content/BasicSR/experiments/pretrained_models/\n",
        "#!wget \"http://dl.nmkd.de/ai/esrgan/4xESRGAN.pth\" --content-disposition -P /content/BasicSR/experiments/pretrained_models/\n",
        "#!wget \"http://dl.nmkd.de/ai/esrgan/8xESRGAN.pth\" --content-disposition -P /content/BasicSR/experiments/pretrained_models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Xof6Q26BFe",
        "cellView": "form"
      },
      "source": [
        "#@title Check amount of files in paths\n",
        "\"\"\"\n",
        "%cd /content/hr\n",
        "!ls | wc -l\n",
        "%cd /content/lr\n",
        "!ls | wc -l\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l1kTpBnqimA",
        "cellView": "form"
      },
      "source": [
        "#@title example config\n",
        "%%writefile /content/BasicSR/codes/options/train/train_template.yml\n",
        "name: debug_001_template # use \"debug\" or \"debug_nochkp\" in the name to run a test session and check everything is working. Does validation and state saving every 8 iterations.\n",
        "#name: 001_template # remove \"debug\" to run the real training session.\n",
        "use_tb_logger: true\n",
        "model: AdaFill #srragan | sr | srgan | ppon | asrragan | DFNet | EdgeConnect | CSA | RN | deepfillv1 | deepfillv2 | Adaptive | Global | Pluralistic | crfill | DeepDFNet | partial | DMFN | pennet | LBAM | RFR | FRRN | PRVS | CRA | USRNet | atrous | MEDFE | AdaFill\n",
        "scale: 1\n",
        "gpu_ids: [0]\n",
        "use_amp: false\n",
        "use_swa: false\n",
        "\n",
        "# Dataset options:\n",
        "datasets:\n",
        "  train:\n",
        "    name: DIV2K\n",
        "    mode: LRHRC # LR | LRHR | LRHROTF | LRHRC | LRHRseg_bg |VLRHR | VLR | LRHRPBR\n",
        "    # dataroot_HR: '../datasets/train/hr' # Original, with a single directory\n",
        "    # dataroot_LR: '../datasets/train/lr' # Original, with a single directory\n",
        "    dataroot_HR: [\n",
        "      '/content/hr'\n",
        "      ] # high resolution / ground truth images\n",
        "    dataroot_LR: [\n",
        "      #'/content/lr' #,\n",
        "      # '../datasets/train/lr3'\n",
        "      ] # low resolution images. If there are missing LR images, they will be generated on the fly from HR\n",
        "    subset_file: null\n",
        "    use_shuffle: true\n",
        "    znorm: false # true | false # To normalize images in [-1, 1] range. Default = None (range [0,1]). Can use with activation function like tanh.\n",
        "    n_workers: 1 # 0 to disable CPU multithreading, or an integrer representing CPU threads to use for dataloading\n",
        "    batch_size: 2\n",
        "    virtual_batch_size: 1\n",
        "    HR_size: 256 # patch size. Default: 128. Needs to be coordinated with the patch size of the features network\n",
        "    image_channels: 3 # number of channels to load images in\n",
        "\n",
        "    # Color space conversion: 'color' for both LR and HR, 'color_LR' for LR independently, 'color_HR' for HR independently\n",
        "    # color: 'y' # remove for no conversion (RGB) | 'y' for Y in YCbCr | 'gray' to convert RGB to grayscale | 'RGB' to convert gray to RGB\n",
        "    # color_LR: 'y' # remove for no conversion (RGB) | 'y' for Y in YCbCr | 'gray' to convert RGB to grayscale | 'RGB' to convert gray to RGB\n",
        "    # color_HR: 'y' # remove for no conversion (RGB) | 'y' for Y in YCbCr | 'gray' to convert RGB to grayscale | 'RGB' to convert gray to RGB\n",
        "\n",
        "    # LR and HR modifiers. Random flip LR and HR or ignore provided LRs and generate new ones on the fly with defined probability:\n",
        "    # rand_flip_LR_HR: false # true # flip LR and HR during training.\n",
        "    # flip_chance: 0.05 # Example: 0.05 = 5% chance of LR and HR flipping during training.\n",
        "    # aug_downscale: 0.2 # Example: 0.6 = 60% chance of generating LR on the fly, even if LR dataset exists\n",
        "\n",
        "    # If manually configuring on the fly generation of LR: (else, it will automatically default to Matlab-like downscale algorithm (777) when/if required\n",
        "    #dataroot_kernels: '/content/kernels_4x/kernelGAN/' # location of the image kernels extracted with KernelGAN, for use with the \"realistic\" downscale type\n",
        "    lr_downscale: True # false\n",
        "    lr_downscale_types: [\"linear\", \"cubic\", \"matlab_bicubic\"] # select from [\"nearest\", \"linear\", \"cubic\", \"area\", \"lanczos4\", \"linear_exact\", \"matlab_bicubic\", \"realistic\"] #scale_algos #scaling interpolation options\n",
        "\n",
        "    # Rotations augmentations:\n",
        "    use_flip: true # flip images\n",
        "    use_rot: true # rotate images in 90 degree angles\n",
        "    hr_rrot: false # rotate images in random degress between -45 and 45\n",
        "\n",
        "    # Noise and blur augmentations:\n",
        "    lr_blur: false # true | false\n",
        "    lr_blur_types: {gaussian: 1, clean: 3} # select from: \"average\",\"box\",\"gaussian\",\"bilateral\",\"clean\" ##blur options #median and motion aren't working yet\n",
        "    noise_data: ../noise_patches/normal/ # location of the noise patches extracted from real images to use for noise injection with noise option \"patches\"\n",
        "    lr_noise: false # true | false\n",
        "    lr_noise_types: {gaussian: 1, JPEG: 1, clean: 4} # select from: \"gaussian\", \"JPEG\", \"quantize\", \"poisson\", \"dither\", \"s&p\", \"speckle\", \"patches\", \"clean\"\n",
        "    lr_noise2: false # true | false\n",
        "    lr_noise_types2: {dither: 2, clean: 2} # select from: \"gaussian\", \"JPEG\", \"quantize\", \"poisson\", \"dither\", \"s&p\", \"speckle\", \"patches\", \"clean\"\n",
        "    hr_noise: false # true | false\n",
        "    hr_noise_types:  {gaussian: 1, clean: 4} # select from: \"gaussian\", \"JPEG\", \"quantize\", \"poisson\", \"dither\", \"s&p\", \"speckle\", \"clean\"\n",
        "\n",
        "    # Color augmentations\n",
        "    # lr_fringes: true # true | false\n",
        "    # lr_fringes_chance: 0.4\n",
        "    # auto_levels: HR # \"HR\" | \"LR\" | \"Both\" #add auto levels to the images to expand dynamic range. Can use with SPL loss or (MS)-SSIM.\n",
        "    # rand_auto_levels: 0.7 # Example: 0.4 = 40% chance of adding auto levels to images on the fly\n",
        "    # lr_unsharp_mask: true # add a unsharpening mask to LR images. Can work well together with the HFEN loss function.\n",
        "    # lr_rand_unsharp: 1 # Example: 0.5 = 50% chance of adding unsharpening mask to LR images on the fly\n",
        "    # hr_unsharp_mask: true # add a unsharpening mask to HR images. Can work well together with the HFEN loss function.\n",
        "    # hr_rand_unsharp: 1 # Example: 0.5 = 50% chance of adding unsharpening mask to HR images on the fly\n",
        "\n",
        "    # Augmentations for classification or (maybe) inpainting networks:\n",
        "    # lr_cutout: false # true | false\n",
        "    # lr_erasing: false # true | false\n",
        "\n",
        "    # Options for inpainting (Options currently only for 1x inpainting in mind.)\n",
        "    center_crop: False # Will centergrop LR and HR images and keep aspect ratio\n",
        "    resize_HR_dimension: False # Downscale lr and hr to given HR value (if you don't set this to true and the image is bigger than needed, random crop will be applied. Should be used with center crop)\n",
        "\n",
        "    training_with_canny: False # ONLY needed for EdgeConnect and PRVS\n",
        "    training_with_canny_SR: False # ONLY needed for sisr\n",
        "    noise_estimation: False # ONLY enable this for USRNet\n",
        "\n",
        "  val:\n",
        "    name: val_set14_part\n",
        "    mode: LRHROTF\n",
        "    dataroot_HR: '/content/hr_val'\n",
        "    #dataroot_LR: '/content/lr_val'\n",
        "\n",
        "    znorm: false # true | false # To normalize images in [-1, 1] range. Default = None (range [0,1]). Can use with activation function like tanh.\n",
        "\n",
        "    # Color space conversion: 'color' for both LR and HR, 'color_LR' for LR independently, 'color_HR' for HR independently\n",
        "    # color: 'y' # remove for no conversion (RGB) | 'y' for Y in YCbCr | 'gray' to convert RGB to grayscale | 'RGB' to convert gray to RGB\n",
        "    # color_LR: 'y' # remove for no conversion (RGB) | 'y' for Y in YCbCr | 'gray' to convert RGB to grayscale | 'RGB' to convert gray to RGB\n",
        "    # color_HR: 'y' # remove for no conversion (RGB) | 'y' for Y in YCbCr | 'gray' to convert RGB to grayscale | 'RGB' to convert gray to RGB\n",
        "\n",
        "    # hr_crop: false #disabled\n",
        "    lr_downscale: True # Needs to be true if you don't train 1x\n",
        "\n",
        "    #dataroot_kernels: '/content/kernels_4x/kernelGAN/' # if kernels exist, downscale will happen with them, if not, then lr_downscale_types will be applied\n",
        "    lr_downscale_types: [\"linear\", \"cubic\"] # select from [\"nearest\", \"linear\", \"cubic\", \"area\", \"lanczos4\", \"linear_exact\", \"matlab_bicubic\"] #scale_algos #scaling interpolation options\n",
        "\n",
        "    training_with_canny: False # ONLY needed for EdgeConnect and PRVS\n",
        "    training_with_canny_SR: False # ONLY needed for sisr\n",
        "    noise_estimation: False # ONLY enable this for USRNet\n",
        "\n",
        "path:\n",
        "    strict: false # true | false\n",
        "    root: '/content/BasicSR/'\n",
        "    #pretrain_model_G: '../experiments/pretrained_models/RRDB_PSNR_x4.pth'\n",
        "    #pretrain_model_G: '../experiments/pretrained_models/RRDB_ESRGAN_x4.pth'\n",
        "    #pretrain_model_G: '../experiments/pretrained_models/nESRGANplus.pth'\n",
        "    #pretrain_model_G: '../experiments/pretrained_models/PPON_G.pth'\n",
        "    # pretrain_model_G: '../experiments/pretrained_models/PANx4_DF2K.pth'\n",
        "    #resume_state: '../experiments/debug_002_RRDB_ESRGAN_x4_DIV2K/training_state/16.state'\n",
        "\n",
        "# Generator options:\n",
        "network_G:\n",
        "    # ESRGAN:\n",
        "    #which_model_G: RRDB_net # RRDB_net (original ESRGAN arch) | MRRDB_net (modified/\"new\" arch) | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64 # of discrim filters in the first conv layer\n",
        "    #nb: 23\n",
        "    #in_nc: 3 # of input image channels: 3 for RGB and 1 for grayscale\n",
        "    #out_nc: 3 # of output image channels: 3 for RGB and 1 for grayscale\n",
        "    #gc: 32\n",
        "    #group: 1\n",
        "    #convtype: Conv2D # Conv2D | PartialConv2D\n",
        "    #net_act: leakyrelu # swish | leakyrelu\n",
        "    #gaussian: true # true | false\n",
        "    #plus: false # true | false\n",
        "    ##finalact: tanh # Test. Activation function to make outputs fit in [-1, 1] range. Default = None. Coordinate with znorm.\n",
        "\n",
        "    # ASRGAN:\n",
        "    #which_model_G: asr_resnet # asr_resnet | asr_cnn\n",
        "    #nf: 64\n",
        "\n",
        "    # PPON:\n",
        "    #which_model_G: ppon # | ppon\n",
        "    ##norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 24\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    ##gc: 32\n",
        "    #group: 1\n",
        "    ##convtype: Conv2D #Conv2D | PartialConv2D\n",
        "\n",
        "    # SRGAN:\n",
        "    #which_model_G: sr_resnet # RRDB_net | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 16\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "\n",
        "    # SR:\n",
        "    #which_model_G: RRDB_net # RRDB_net | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 23\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    #gc: 32\n",
        "    #group: 1\n",
        "\n",
        "    # PAN:\n",
        "    # which_model_G: pan_net\n",
        "    # in_nc: 3\n",
        "    # out_nc: 3\n",
        "    # nf: 40\n",
        "    # unf: 24\n",
        "    # nb: 16\n",
        "    # self_attention: true\n",
        "    # double_scpa: false\n",
        "\n",
        "    # edge-informed-sisr\n",
        "    #which_model_G: sisr\n",
        "    #use_spectral_norm: True\n",
        "\n",
        "    # USRNet\n",
        "    #which_model_G: USRNet\n",
        "    #in_nc=4\n",
        "    #out_nc=3\n",
        "    #nc=[64, 128, 256, 512]\n",
        "    #nb=2\n",
        "    #act_mode='R'\n",
        "    #downsample_mode='strideconv'\n",
        "    #upsample_mode='convtranspose'\n",
        "    \n",
        "    # ----Inpainting Generators----\n",
        "    # DFNet (batch_size: 2+, needs 2^x image input and validation) (2019)\n",
        "    #which_model_G: DFNet\n",
        "    #c_img: 3\n",
        "    #c_mask: 1\n",
        "    #c_alpha: 3\n",
        "    #mode: nearest\n",
        "    #norm: batch\n",
        "    #act_en: relu\n",
        "    #act_de: leaky_relu\n",
        "    #en_ksize: [7, 5, 5, 3, 3, 3, 3, 3]\n",
        "    #de_ksize: [3, 3, 3, 3, 3, 3, 3, 3]\n",
        "    #blend_layers: [0, 1, 2, 3, 4, 5]\n",
        "    #conv_type: deform # partial | normal | deform\n",
        "\n",
        "    # EdgeConnect (2019)\n",
        "    #which_model_G: EdgeConnect\n",
        "    #use_spectral_norm: True\n",
        "    #residual_blocks_edge: 8\n",
        "    #residual_blocks_inpaint: 8\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #conv_type_inpaint: 'normal' # normal | partial | deform\n",
        "\n",
        "    # CSA (2019)\n",
        "    #which_model_G: CSA\n",
        "    #c_img: 3\n",
        "    #norm: 'instance'\n",
        "    #act_en: 'leaky_relu'\n",
        "    #act_de: 'relu'\n",
        "\n",
        "    # RN (2020)\n",
        "    #which_model_G: RN\n",
        "    #input_channels: 3\n",
        "    #residual_blocks: 8\n",
        "    #threshold: 0.8\n",
        "\n",
        "    # deepfillv1 (2018)\n",
        "    #which_model_G:  deepfillv1\n",
        "\n",
        "    # deepfillv2 (2019)\n",
        "    #which_model_G: deepfillv2\n",
        "    #in_channels:  4\n",
        "    #out_channels:  3\n",
        "    #latent_channels:  64\n",
        "    #pad_type:  'zero'\n",
        "    #activation:  'lrelu' \n",
        "    #norm: 'in'\n",
        "    #conv_type: partial # partial | normal\n",
        "\n",
        "    # Adaptive (2020)\n",
        "    #which_model_G: Adaptive\n",
        "    #in_channels: 3\n",
        "    #residual_blocks: 1\n",
        "    #init_weights: True\n",
        "\n",
        "    # Global (2020)\n",
        "    #which_model_G: Global\n",
        "    #input_dim: 5\n",
        "    #ngf: 32\n",
        "    #use_cuda: True\n",
        "    #device_ids: [0]\n",
        "\n",
        "    # Pluralistic (2019)\n",
        "    #which_model_G: Pluralistic\n",
        "    #ngf_E: 32\n",
        "    #z_nc_E: 128\n",
        "    #img_f_E: 128\n",
        "    #layers_E: 5\n",
        "    #norm_E: 'none'\n",
        "    #activation_E: 'LeakyReLU'\n",
        "    #ngf_G: 32\n",
        "    #z_nc_G: 128\n",
        "    #img_f_G: 128\n",
        "    #L_G: 0\n",
        "    #output_scale_G: 1\n",
        "    #norm_G: 'instance'\n",
        "    #activation_G: 'LeakyReLU'\n",
        "\n",
        "    # crfill (2020)\n",
        "    #which_model_G: crfill\n",
        "    #cnum: 48\n",
        "\n",
        "    # DeepDFNet (experimental)\n",
        "    #which_model_G: DeepDFNet\n",
        "    #in_channels:  4\n",
        "    #out_channels:  3\n",
        "    #latent_channels:  64\n",
        "    #pad_type:  'zero'\n",
        "    #activation:  'lrelu' \n",
        "    #norm: 'in'\n",
        "\n",
        "    # partial (2018)\n",
        "    #which_model_G: partial\n",
        "\n",
        "    # DMFN (2020)\n",
        "    #which_model_G: DMFN\n",
        "    #in_nc: 4\n",
        "    #out_nc: 3\n",
        "    #nf: 64\n",
        "    #n_res: 8 \n",
        "    #norm: 'in'\n",
        "    #activation: 'relu'\n",
        "\n",
        "    # pennet (2019)\n",
        "    #which_model_G: pennet\n",
        "\n",
        "    # LBAM (2019)\n",
        "    #which_model_G: LBAM\n",
        "    #inputChannels: 4\n",
        "    #outputChannels: 3\n",
        "\n",
        "    # RFR (use_swa: false) (2020)\n",
        "    #which_model_G: RFR\n",
        "    #conv_type: deform # partial | deform\n",
        "\n",
        "    # FRRN (2019)\n",
        "    #which_model_G: FRRN\n",
        "\n",
        "    # PRVS (2019)\n",
        "    #which_model_G: PRVS\n",
        "\n",
        "    # CRA (HR_size: 512) (2020)\n",
        "    #which_model_G: CRA\n",
        "    #activation: 'elu'\n",
        "    #norm: 'none'\n",
        "\n",
        "    # atrous (2020)\n",
        "    #which_model_G: atrous\n",
        "\n",
        "    # MEDFE (batch_size: 1) (2020)\n",
        "    #which_model_G: MEDFE\n",
        "\n",
        "    # AdaFill (2021)\n",
        "    which_model_G: AdaFill\n",
        "\n",
        "# Discriminator options:\n",
        "network_D:\n",
        "    # ESRGAN (default)| PPON:\n",
        "    which_model_D: context_encoder # discriminator_vgg_128 | discriminator_vgg | discriminator_vgg_128_fea (feature extraction) | discriminator_vgg_fea (feature extraction) | patchgan | multiscale | context_encoder\n",
        "    norm_type: batch\n",
        "    act_type: leakyrelu\n",
        "    mode: CNA # CNA | NAC\n",
        "    nf: 64\n",
        "    in_nc: 3\n",
        "    nlayer: 3 # only for patchgan and multiscale\n",
        "    num_D: 3 # only for multiscale\n",
        "\n",
        "# Schedulers options:\n",
        "train:\n",
        "    lr_G: 0.0001 #0.0001 # 2e-4 # starting lr_g #Test, default: 1e-4\n",
        "    weight_decay_G: 0\n",
        "    beta1_G: 0.9\n",
        "    lr_D: 0.0001 #0.0001 # 2e-4 # starting lr_d #Test, default: 1e-4\n",
        "    weight_decay_D: 0\n",
        "    beta1_D: 0.9\n",
        "\n",
        "    # For MultiStepLR (ESRGAN, default):\n",
        "    lr_scheme: MultiStepLR\n",
        "    lr_steps: [50000, 100000, 200000, 300000] # training from scratch\n",
        "    #lr_steps: [50000, 75000, 85000, 100000] #finetuning\n",
        "    lr_gamma: 0.5 # lr change at every step (multiplied by)\n",
        "\n",
        "    # For StepLR_Restart (PPON):\n",
        "    # lr_gamma: 0.9 #lr change at every step (multiplied by)\n",
        "    # lr_scheme: StepLR_Restart # MultiStepLR | MultiStepLR_Restart | StepLR | StepLR_Restart | CosineAnnealingLR_Restart\n",
        "    # lr_step_sizes: [200, 100, 250] # Steps for each restart for \"StepLR_Restart\"\n",
        "    # restarts: [138000, 172500] # Restart iterations for \"MultiStepLR_Restart\", \"StepLR_Restart\" and \"CosineAnnealingLR_Restart\"\n",
        "    # restart_weights: [1, 0.5, 0.5] #lr_() * each weight in \"restart_weights\" for each restart in \"restarts\"\n",
        "    ##clear_state: true\n",
        "\n",
        "    # For MultiStepLR_Restart:\n",
        "    # lr_gamma: 0.9\n",
        "    # lr_scheme: MultiStepLR_Restart # MultiStepLR | MultiStepLR_Restart | StepLR | StepLR_Restart | CosineAnnealingLR_Restart\n",
        "    # lr_steps: [34500, 69000, 103500, 155250, 189750, 241500] #For \"MultiStepLR\" and \"MultiStepLR_Restart\"\n",
        "    # restarts: [138000, 172500] # Restart iterations for \"MultiStepLR_Restart\", \"StepLR_Restart\" and \"CosineAnnealingLR_Restart\"\n",
        "    # restart_weights: [0.5, 0.5] # lr_() * each weight in \"restart_weights\" for each restart in \"restarts\"\n",
        "    ##clear_state: true\n",
        "\n",
        "    # For CosineAnnealingLR_Restart (PAN)\n",
        "    # lr_G: !!float 7e-4\n",
        "    # lr_scheme: \"CosineAnnealingLR_Restart\"\n",
        "    # beta1_G: 0.9\n",
        "    # beta2_G: 0.99\n",
        "    # lr_D: 7e-4\n",
        "    # beta1_D: 0.9\n",
        "    # beta2_D: 0.99\n",
        "    # # beta1: 0.9\n",
        "    # # beta2: 0.99\n",
        "    # niter: 1000000\n",
        "    # warmup_iter: -1  # no warm up\n",
        "    # T_period: [250000, 250000, 250000, 250000]\n",
        "    # restarts: [250000, 500000, 750000]\n",
        "    # restart_weights: [1, 1, 1]\n",
        "    # eta_min: !!float 1e-7\n",
        "\n",
        "    # For SWA scheduler\n",
        "    swa_start_iter: 375000 #Just reference: 75% of 500000. Can be any value, including 0 to start right away with a pretrained model.\n",
        "    swa_lr: 1e-4 #Has to be ~order of magnitude of a stable lr for the regular scheduler\n",
        "    swa_anneal_epochs: 10\n",
        "    swa_anneal_strategy: \"cos\"\n",
        "\n",
        "    # Losses:\n",
        "    pixel_criterion: l1 # \"l1\" | \"l2\" | \"cb\" | \"elastic\" | \"relativel1\" | \"l1cosinesim\" | \"clipl1\" #pixel loss\n",
        "    pixel_weight: 1e-2 # 1e-2 | 1\n",
        "    feature_criterion: l1 # \"l1\" | \"l2\" | \"cb\" | \"elastic\" #feature loss (VGG feature network)\n",
        "    feature_weight: 1\n",
        "    # cx_weight: 0.5\n",
        "    # cx_type: contextual\n",
        "    # cx_vgg_layers: {conv_3_2: 1, conv_4_2: 1}\n",
        "    # hfen_criterion: l1 # hfen: \"l1\" | \"l2\" | \"rel_l1\" | \"rel_l2\" #helps in deblurring and finding edges, lines\n",
        "    # hfen_weight: 1e-6\n",
        "    # grad_type: grad-4d-l1 # 2d | 4d / - any of the pixel crit, ie \"grad-2d-l1\"\n",
        "    # grad_weight: 4e-1 # 4e-1\n",
        "    # tv_type: 4D # \"normal\" | \"4D\" #helps in denoising, reducing upscale artefacts\n",
        "    # tv_weight: 1e-5 # Change \"tv_weight\" so the l_g_tv is around 1e-02 - 1e-01\n",
        "    # tv_norm: 1 # 1 for l1 (default) or 2 for l2.\n",
        "    # ssim_type: ssim # \"ssim\" | \"ms-ssim\" #helps to maintain luminance, contrast and covariance between SR and HR\n",
        "    # ssim_weight: 1\n",
        "    # lpips_weight: 1 # perceptual loss\n",
        "    # lpips_type: net-lin # net-lin | net *\n",
        "    # lpips_net: squeeze # \"vgg\" | \"alex\" | \"squeeze\"\n",
        "\n",
        "    # Experimental losses\n",
        "    # spl_type: spl # \"spl\" | \"gpl\" | \"cpl\"\n",
        "    # spl_weight: 0.1 # 1e-2 # SPL loss function. note: needs to add a cap in the generator (finalcap; For [0,1] range -> \"finalcap\": \"clamp\") or the overflow loss or it can become unstable.\n",
        "    # of_type: overflow # overflow loss function to force the images back into the [0, 1] range\n",
        "    # of_weight: 0.2\n",
        "    # fft_type: fft\n",
        "    # fft_weight: 0.1\n",
        "    # color_criterion: color-l1cosinesim # l1cosinesim naturally helps color consistency, so it is the best to use here, but others can be used as well\n",
        "    # color_weight: 1 # Loss based on the UV channels of YUV color space, helps preserve color consistency\n",
        "    # avg_criterion: avg-l1\n",
        "    # avg_weight: 5 # Averaging downscale loss\n",
        "    # ms_criterion: multiscale-l1\n",
        "    # ms_weight: 1e-2\n",
        "\n",
        "    # Adversarial loss:\n",
        "    gan_type: vanilla # \"vanilla\" | \"wgan-gp\" | \"lsgan\"\n",
        "    gan_weight: 5e-3 # * test: 7e-3\n",
        "    # for wgan-gp\n",
        "    # D_update_ratio: 1\n",
        "    # D_init_iters: 0\n",
        "    # gp_weigth: 10\n",
        "    # if using the discriminator_vgg_128_fea or discriminator_vgg_fea feature maps to calculate feature loss\n",
        "    # gan_featmaps: true # true | false\n",
        "    # dis_feature_criterion: cb # \"l1\" | \"l2\" | \"cb\" | \"elastic\" #discriminator feature loss\n",
        "    # dis_feature_weight: 0.01 # 1\n",
        "\n",
        "    # For PPON\n",
        "    # train_phase: 1 # Training starting phase, can skip the first phases\n",
        "    # phase1_s: 100 # 5000 #100 #5000 #138000 #-1 to skip. Need to coordinate with the LR steps. #COBranch: lr =  2e−4, decreased by the factor of 2 for every 1000 epochs (1.38e+5 iterations) 138k\n",
        "    # phase2_s: 200 # 10000 #200 #10000 #172500 #-1 to skip. Need to coordinate with the LR steps. #SOBranch: λ = 1e+3 (?), lr = 1e−4 and halved at every 250 epochs (3.45e+4iterations) 34.5k\n",
        "    # phase3_s: 5000000 # 207000 #-1 to skip. Need to coordinate with the LR steps. #POBranch: η = 5e−3,  lr = 1e−4 and halved at every 250 epochs (3.45e+4iterations) 34.5k\n",
        "    # phase4_s: 100100\n",
        "\n",
        "    # Differentiable Augmentation for Data-Efficient GAN Training\n",
        "    # diffaug: true\n",
        "    # dapolicy: 'color,transl_zoom,flip,rotate,cutout' # smart \"all\" (translation, zoom_in and zoom_out are exclusive)\n",
        "\n",
        "    # Batch (Mixup) augmentations\n",
        "    # mixup: true\n",
        "    # mixopts: [blend, rgb, mixup, cutmix, cutmixup] # , \"cutout\", \"cutblur\"]\n",
        "    # mixprob: [1.0, 1.0, 1.0, 1.0, 1.0] #, 1.0, 1.0]\n",
        "    # mixalpha: [0.6, 1.0, 1.2, 0.7, 0.7] #, 0.001, 0.7]\n",
        "    # aux_mixprob: 1.0\n",
        "    # aux_mixalpha: 1.2\n",
        "    ## mix_p: 1.2\n",
        "\n",
        "    # Frequency Separator\n",
        "    # fs: true\n",
        "    # lpf_type: average # \"average\" | \"gaussian\"\n",
        "    # hpf_type: average # \"average\" | \"gaussian\"\n",
        "\n",
        "    # Other training options:\n",
        "    # finalcap: clamp # Test. Cap Generator outputs to fit in: [-1, 1] range (\"tanh\"), rescale tanh to [0,1] range (\"scaltanh\"), cap (\"sigmoid\") or clamp (\"clamp\") to [0,1] range. Default = None. Coordinate with znorm. Required for SPL if using image range [0,1]\n",
        "    manual_seed: 0\n",
        "    niter: 5e5\n",
        "    val_freq: 100 # 5e3\n",
        "    # overwrite_val_imgs: true\n",
        "    val_comparison: true\n",
        "    metrics: 'psnr,ssim,lpips' # select from: \"psnr,ssim,lpips\" or a combination separated by comma \",\"\n",
        "\n",
        "logger:\n",
        "    print_freq: 25 #200\n",
        "    save_checkpoint_freq: 5e3\n",
        "    overwrite_chkp: false\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG6BqJD5J41q"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWaG4UHJrTye",
        "cellView": "both"
      },
      "source": [
        "%cd '/content/BasicSR/codes'\n",
        "# for super resolution\n",
        "#!python train.py -opt \"/content/BasicSR/codes/options/train/train_template.yml\"\n",
        "# for inpainting\n",
        "!python train_inpaint.py -opt \"/content/BasicSR/codes/options/train/train_template.yml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpPQ4wJe_hcm"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/path/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y9xyxRorUim"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDbM8Oqiu-gF"
      },
      "source": [
        "# testing\n",
        "%cd /content/BasicSR/codes/\n",
        "!python test_inpaint.py --input /content/0.jpg --mask /content/mask.png --output /content/output.png --yaml \"/content/BasicSR/codes/options/train/train_template.yml\" \\\n",
        "                         --model_path \"/content/BasicSR/experiments/001_template/models/latest_G.pth\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}